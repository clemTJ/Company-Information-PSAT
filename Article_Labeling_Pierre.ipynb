{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Labeling & Lexical Fields Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cprofile for evaulation of a function's speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile,pstats, io\n",
    "def profile(fct):\n",
    "    \"\"\" a decorator for the function \n",
    "        use by writing @profile before any function that needs evaluation\"\"\"\n",
    "    def inner(*args,**kwargs):\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()\n",
    "        retval = fct(*args,**kwargs)\n",
    "        s=i0.StringIO()\n",
    "        sortBy = 'cumulative'\n",
    "        ps = pstats.Stats(pr,stream = s).sort_stats(sortBy)\n",
    "        ps.print_stats()\n",
    "        print (s.getvalue())\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Unlabelled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "raw_json_data = []\n",
    "with open('./data/20200420_20200714_business_articles.json') as f:\n",
    "    for line in f:\n",
    "        raw_json_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type <class 'list'>\n",
      "json <class 'dict'>\n",
      "keys dict_keys(['published', 'link', 'message', 'Feed', 'title', '@version', 'author', '@timestamp', 'full-text', 'type'])\n",
      "length 416307\n"
     ]
    }
   ],
   "source": [
    "print (\"data type\",type (raw_json_data))\n",
    "print (\"json\",type (raw_json_data[0]))\n",
    "print (\"keys\",raw_json_data[0].keys())\n",
    "print (\"length\", len(raw_json_data))\n",
    "#print (raw_json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Company Names & Related Names(52 companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 dict_keys(['21st century fox', 'activision blizzard', 'adobe', 'advanced micro devices', 'akamai technologies', 'alexion pharmaceuticals', 'amazon', 'american airlines group', 'amgen', 'analog devices', 'apple', 'autodesk', 'automatic data processing', 'baidu', 'bed bath & beyond', 'biogen', 'ca technologies', 'celgene', 'cerner', 'cisco', 'cognizant', 'comcast', 'discovery communications', 'dish network', 'ebay', 'electronic arts', 'equinix', 'expeditors international', 'facebook', 'alphabet', 'intel', 'liberty global', 'liberty interactive', 'linear technology', 'marriott international', 'mattle', 'mattel', 'mckesson', 'microsoft', 'netflix', 'nvidia', 'paypal', 'qualcomm', 'starbucks', 'stericycle', 'tesla motors', 'texas instruments', 'the priceline group', 'universal display'])\n",
      "['cable television', 'broadcasting', 'record label', 'movie production', 'tv production', 'rupert murdoch', 'james murdoch', 'lachlan murdoch', 'chase carey', 'fox broadcasting company', '20th century fox television', 'fox star studios', '20th century fox', 'fox entertainment group', 'fox sports networks', 'fox networks group', 'fox digital entertainment', 'sky plc', 'fox news channel', 'truex', 'national geographic channel', '20th television', 'blue sky studios', 'fox television stations', 'yes network', 'endemol shine group', 'star tv', 'hulu']\n"
     ]
    }
   ],
   "source": [
    "# fetching company names (52 companies)\n",
    "df = pd.read_excel (r'./data/relevant_words/comapny_name-related_words.xlsx', header = None)\n",
    "# Lower\n",
    "df[0] = [row[0].lower() for index, row in df.iterrows()] \n",
    "# Split company name and related names\n",
    "split = np.array([row[0].split(\";\") for index, row in df.iterrows()])\n",
    "df[\"company_name\"] = split[:,0]\n",
    "df[\"related_name\"] = split[:,1]\n",
    "df.drop(columns = [0], inplace=True)\n",
    "# build dictionary of related name of companies\n",
    "dict_companies = {}\n",
    "company_names = df[\"company_name\"].unique()\n",
    "for name in company_names:\n",
    "    df_tmp = df[df[\"company_name\"] == name]\n",
    "    dict_companies[name] = list(df_tmp[\"related_name\"])\n",
    "print (len(dict_companies.keys()), dict_companies.keys())\n",
    "print (dict_companies[\"21st century fox\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Company Names & Related Names(49 companies) unlowered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 dict_keys(['21st_Century_Fox', 'Activision_Blizzard', 'Adobe', 'Advanced_Micro_Devices', 'Akamai_Technologies', 'Alexion_Pharmaceuticals', 'Amazon', 'American_Airlines_Group', 'Amgen', 'Analog_Devices', 'Apple', 'Autodesk', 'Automatic_Data_Processing', 'Baidu', 'Bed_Bath_&_Beyond', 'Biogen', 'CA_Technologies', 'Celgene', 'Cerner', 'Cisco', 'Cognizant', 'Comcast', 'Discovery_Communications', 'Dish_Network', 'EBay', 'Electronic_Arts', 'Equinix', 'Expeditors_International', 'Facebook', 'Alphabet', 'Intel', 'Liberty_Global', 'Liberty_Interactive', 'Linear_Technology', 'Marriott_International', 'Mattle', 'Mattel', 'McKesson', 'Microsoft', 'Netflix', 'NVIDIA', 'Paypal', 'Qualcomm', 'Starbucks', 'Stericycle', 'Tesla_Motors', 'Texas_Instruments', 'The_Priceline_Group', 'Universal_Display'])\n"
     ]
    }
   ],
   "source": [
    "# fetching company names (52 companies)\n",
    "df = pd.read_excel (r'./data/relevant_words/comapny_name-related_words.xlsx', header = None)\n",
    "# Lower\n",
    "df[0] = [row[0].replace(\" \", \"_\") for index, row in df.iterrows()] \n",
    "# Split company name and related names\n",
    "split = np.array([row[0].split(\";\") for index, row in df.iterrows()])\n",
    "df[\"company_name\"] = split[:,0]\n",
    "df[\"related_name\"] = split[:,1]\n",
    "df.drop(columns = [0], inplace=True)\n",
    "# build dictionary of related name of companies\n",
    "dict_companies_unlowered = {}\n",
    "company_names = df[\"company_name\"].unique()\n",
    "for name in company_names:\n",
    "    df_tmp = df[df[\"company_name\"] == name]\n",
    "    dict_companies_unlowered[name] = list(df_tmp[\"related_name\"])\n",
    "print (len(dict_companies_unlowered.keys()), dict_companies_unlowered.keys())\n",
    "#print (dict_companies_unlowered['21st_Century_Fox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting url, title & full_text of each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list()\n",
    "plain_texts = list()\n",
    "titles = list()\n",
    "labels = list()\n",
    "\n",
    "min_article_size = 2000\n",
    "for article in raw_json_data:\n",
    "    plain_text = article.get('full-text')\n",
    "    title = article.get('title')\n",
    "    url = article.get('link')\n",
    "    if (plain_text and \"Article `download()` failed\" != plain_text[:27] and \"Please enable cookies\" != plain_text[:21] and len(plain_text)>min_article_size):\n",
    "        plain_texts.append(plain_text)\n",
    "        urls.append(url)\n",
    "        titles.append(title)\n",
    "        labels.append(list())\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataFrame with extacted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#Statistics\n",
    "# 358192 removing \"Article `download()` failed\" \n",
    "# 340987 removing \"Article `download()` failed\" and \"Please enable cookies\"\n",
    "# 215039 removing \"Article `download()` failed\" and \"Please enable cookies\" and size<min_article_size = 2000\n",
    "data = np.array([urls,titles, plain_texts, labels]).T\n",
    "columns=[\"url\", \"title\", \"plain_text\", \"label\"]\n",
    "df_articles = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>plain_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>215034</th>\n",
       "      <td>http://rssfeeds.usatoday.com/~/t/0/0/usatodayc...</td>\n",
       "      <td>Michigan partygoers test positive for COVID-19...</td>\n",
       "      <td>Michigan partygoers test positive for COVID-19...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215035</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jul/...</td>\n",
       "      <td>Coast Guard officials decline to testify on ra...</td>\n",
       "      <td>NEW LONDON, Conn. (AP) - A planned congression...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215036</th>\n",
       "      <td>https://www.denverpost.com/2020/07/08/united-a...</td>\n",
       "      <td>United Airlines will slash nearly 36,000 jobs ...</td>\n",
       "      <td>United Airlines plans to furlough as many as 3...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215037</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jul/...</td>\n",
       "      <td>The Latest: Pence says CDC will issue guidance...</td>\n",
       "      <td>WASHINGTON - Vice President Mike Pence says th...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215038</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jul/...</td>\n",
       "      <td>US rejects nearly all Chinese claims in  South...</td>\n",
       "      <td>WASHINGTON (AP) - The Trump administration esc...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  \\\n",
       "215034  http://rssfeeds.usatoday.com/~/t/0/0/usatodayc...   \n",
       "215035  https://www.washingtontimes.com/news/2020/jul/...   \n",
       "215036  https://www.denverpost.com/2020/07/08/united-a...   \n",
       "215037  https://www.washingtontimes.com/news/2020/jul/...   \n",
       "215038  https://www.washingtontimes.com/news/2020/jul/...   \n",
       "\n",
       "                                                    title  \\\n",
       "215034  Michigan partygoers test positive for COVID-19...   \n",
       "215035  Coast Guard officials decline to testify on ra...   \n",
       "215036  United Airlines will slash nearly 36,000 jobs ...   \n",
       "215037  The Latest: Pence says CDC will issue guidance...   \n",
       "215038  US rejects nearly all Chinese claims in  South...   \n",
       "\n",
       "                                               plain_text label  \n",
       "215034  Michigan partygoers test positive for COVID-19...    []  \n",
       "215035  NEW LONDON, Conn. (AP) - A planned congression...    []  \n",
       "215036  United Airlines plans to furlough as many as 3...    []  \n",
       "215037  WASHINGTON - Vice President Mike Pence says th...    []  \n",
       "215038  WASHINGTON (AP) - The Trump administration esc...    []  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/pierre/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Eliminated', 'VBN'),\n",
       " ('MasterChef', 'NNP'),\n",
       " ('contestant', 'JJ'),\n",
       " ('Harry', 'NNP'),\n",
       " ('Foster', 'NNP'),\n",
       " ('has', 'VBZ'),\n",
       " ('hit', 'VBN'),\n",
       " ('back', 'RB'),\n",
       " ('at', 'IN'),\n",
       " ('unfair', 'JJ'),\n",
       " ('criticism', 'NN'),\n",
       " ('against', 'IN'),\n",
       " ('judge', 'NN'),\n",
       " ('Melissa', 'NNP'),\n",
       " ('Leong', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = df_articles[\"plain_text\"][0]\n",
    "sent_text = nltk.sent_tokenize(paragraph) # this gives us a list of sentences\n",
    "# Now Loop over each sentence and tokenize it separately\n",
    "tokenized_paragraph = list()\n",
    "for sentence in sent_text:\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized_text)\n",
    "    tokenized_paragraph.append(tagged)\n",
    "    #print(tagged)\n",
    "tokenized_paragraph[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representing Tagged Tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning full_text of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def clean_plain_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    #[:punct:], ,[^0-9], [^a-z]\n",
    "    #text = re.sub(\"[^a-z],[^:punct:],[^0-9]\", ' ', text)\n",
    "    text = re.sub(\"[^a-z0-9]\", ' ', text)\n",
    "    #text = re.sub(r'\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliminated masterchef contestant harry foster has hit back at unfair criticism against judge melissa'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove every non-letter/number character\n",
    "#n_articles = 10000\n",
    "#df_cleaned = df_articles.head(n_articles).copy(deep= True)\n",
    "df_cleaned = df_articles.copy(deep= True)\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    row[\"plain_text\"] = row[\"plain_text\"].lower()\n",
    "    row[\"plain_text\"]= re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "    #[:punct:], ,[^0-9], [^a-z]\n",
    "    #row[\"plain_text\"] = re.sub(\"[^a-z],[^:punct:],[^0-9]\", ' ', row[\"plain_text\"])\n",
    "    row[\"plain_text\"] = re.sub(\"[^a-z0-9]\", ' ', row[\"plain_text\"])\n",
    "    #row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "df_cleaned[\"plain_text\"][0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Stop Words & Removing them from plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all stop words from plain text\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for stop_word in stop_words:\n",
    "        row[\"plain_text\"] = re.sub(' '+stop_word+' ', ' ', row[\"plain_text\"])\n",
    "    row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "df_cleaned[\"plain_text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Articles with Company Names \n",
    "### Check if Articles Talk of Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  []\n",
       "1                                  []\n",
       "2    [advanced micro devices, nvidia]\n",
       "3                                  []\n",
       "4                             [apple]\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in df_cleaned.iterrows(): # initialize labels\n",
    "    row['label'] = list()\n",
    "company_names = dict_companies.keys()   \n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for company in company_names:\n",
    "        if company in row[\"plain_text\"]:\n",
    "            row['label'].append(company)\n",
    "        else:\n",
    "            for related_name in dict_companies[company]:\n",
    "                if related_name in row[\"plain_text\"]:\n",
    "                    row['label'].append(company)\n",
    "                    break\n",
    "df_cleaned[\"label\"].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# saving data to csv\n",
    "PATH = \"./data/\"\n",
    "file = \"cleaned_articles_200k\"\n",
    "df_cleaned.to_csv(PATH + file + \".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data from csv\n",
    "PATH = \"./data/\"\n",
    "file = \"cleaned_articles_200k\"\n",
    "df_cleaned = pd.read_csv(PATH + file + \".csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>plain_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.dailymail.co.uk/tvshowbiz/article-...</td>\n",
       "      <td>MasterChef's Harry Foster hits back at claims ...</td>\n",
       "      <td>eliminated masterchef contestant harry foster ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jun/...</td>\n",
       "      <td>Protest arrests logjam tests NYC legal system,...</td>\n",
       "      <td>new york ap wave arrests new york city protest...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-83114...</td>\n",
       "      <td>Labour's Anneliese Dodds says she will REFUSE ...</td>\n",
       "      <td>a top shadow minister today said enough eviden...</td>\n",
       "      <td>[advanced micro devices, nvidia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://feeds.reuters.com/~r/Reuters/worldNews/...</td>\n",
       "      <td>Civil unrest rages in Minneapolis over raciall...</td>\n",
       "      <td>minneapolis reuters peaceful rallies gave way ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-82734...</td>\n",
       "      <td>Australia 'beats the cr*p' out of coronavirus ...</td>\n",
       "      <td>australia beating c p coronavirus six states t...</td>\n",
       "      <td>[apple]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.dailymail.co.uk/tvshowbiz/article-...   \n",
       "1  https://www.washingtontimes.com/news/2020/jun/...   \n",
       "2  https://www.dailymail.co.uk/news/article-83114...   \n",
       "3  http://feeds.reuters.com/~r/Reuters/worldNews/...   \n",
       "4  https://www.dailymail.co.uk/news/article-82734...   \n",
       "\n",
       "                                               title  \\\n",
       "0  MasterChef's Harry Foster hits back at claims ...   \n",
       "1  Protest arrests logjam tests NYC legal system,...   \n",
       "2  Labour's Anneliese Dodds says she will REFUSE ...   \n",
       "3  Civil unrest rages in Minneapolis over raciall...   \n",
       "4  Australia 'beats the cr*p' out of coronavirus ...   \n",
       "\n",
       "                                          plain_text  \\\n",
       "0  eliminated masterchef contestant harry foster ...   \n",
       "1  new york ap wave arrests new york city protest...   \n",
       "2  a top shadow minister today said enough eviden...   \n",
       "3  minneapolis reuters peaceful rallies gave way ...   \n",
       "4  australia beating c p coronavirus six states t...   \n",
       "\n",
       "                              label  \n",
       "0                                []  \n",
       "1                                []  \n",
       "2  [advanced micro devices, nvidia]  \n",
       "3                                []  \n",
       "4                           [apple]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of articles with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 121097 labeled articles in the 215039 articles of the corpus\n"
     ]
    }
   ],
   "source": [
    "labeled = 0\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    if len(row[\"label\"])>0:\n",
    "        if index ==0:\n",
    "            print (row[\"label\"],len(row[\"label\"]), row[\"label\"][1] )\n",
    "        labeled +=1\n",
    "print (\"There are %d labeled articles in the %d articles of the corpus\"%(labeled, len (df_cleaned[\"label\"])))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Number of Articles that each Company is Associated to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 0 companies with associated articles over the 49 total companies\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "dict_count = {}\n",
    "for company in company_names: dict_count[company]= 0\n",
    "    \n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for company in company_names:\n",
    "        if company in row[\"label\"]:\n",
    "            dict_count[company]+=1\n",
    "\n",
    "#dict_count          \n",
    "companies_w_articles = list()\n",
    "for company in company_names:\n",
    "    if dict_count[company]>0:\n",
    "        companies_w_articles.append(company)\n",
    "print (\"there are %d companies with associated articles over the %d total companies\"%(len(companies_w_articles),len(company_names)) )\n",
    "#dict_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'apple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0f9c475fb4da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"apple\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'apple'"
     ]
    }
   ],
   "source": [
    "dict_count[\"apple\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf.Idf to get top 20 words for each company (that have articles related to them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [23:20:51<00:00, 1715.33s/it]   \n"
     ]
    }
   ],
   "source": [
    "# Tf.Idf on Companies that have Associated Articles \n",
    "relevant_words_tfidf = {}\n",
    "for company in tqdm(companies_w_articles): # for all companies in companies_w_articles\n",
    "\n",
    "    #tfidf_vectorizer=TfidfVectorizer(stop_words = {'english'},ngram_range = (1,3), binary = True) #sublinear_tf=False\n",
    "    tfidf_vectorizer=TfidfVectorizer(stop_words = {'english'},ngram_range = (1,1))# bilinear doesn't work..\n",
    "    plain_text_list = list()\n",
    "    company_article = \"\"\n",
    "    for index, row in df_cleaned.iterrows():\n",
    "        if company in row[\"label\"]:\n",
    "            company_article = company_article+ \" \"+ row[\"plain_text\"] # add article to company BIG article\n",
    "        else:\n",
    "            plain_text_list.append(row[\"plain_text\"]) # otherwise add to corpus\n",
    "    \n",
    "    plain_text_list.insert(0,company_article) # add company article to begging of corpus\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(plain_text_list)\n",
    "\n",
    "    #Get the tf-idf scores for the words in the company article complication.\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] # discard tf.idf scores for the other texts\n",
    "\n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df = df.sort_values(by=[\"tfidf\"],ascending=False).head(40) # Take top 40 words\n",
    "    \n",
    "    relevant_words_tfidf[company] = list(zip(list(df.index),list(df[\"tfidf\"])))\n",
    "    #print (relevant_words_tfidf[company])\n",
    "    \n",
    "#100%|██████████| 52/52 [7:31:13<00:00, 520.64s/it]\n",
    "#100%|██████████| 52/52 [21:46:51<00:00, 1507.90s/it]    \n",
    "#100%|██████████| 49/49 [23:20:51<00:00, 1715.33s/it] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save dictionary\n",
    "PATH = \"./data/relevant_words/\"\n",
    "file = \"relevant_words_tfidf_binary\"\n",
    "a_file = open(PATH + file + \".json\", \"w\")\n",
    "json.dump(relevant_words_tfidf, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary \n",
    "PATH = \"./data/relevant_words/\"\n",
    "file = \"relevant_words_tfidf_200k\"\n",
    "a_file = open(PATH + file + \".json\", \"r\")\n",
    "relevant_words_tfidf = json.load(a_file)\n",
    "#relevant_words_tfidf = dict(relevant_words_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in relevant_words_tfidf.keys():\n",
    "    tmp = []\n",
    "    for element in relevant_words_tfidf[company]:\n",
    "        tmp.append((element[0],element[1]))\n",
    "    \n",
    "    relevant_words_tfidf[company] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_words_tfidf['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#companies_w_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a give model, we want to get the first 20 words related to a company of: companies_w_articles\n",
    "# And store everything into a dictionary like for tf.idf\n",
    "#dict_companies_unlowered\n",
    "def getTopWords(model, n_words, dict_companies):\n",
    "    companies = dict_companies.keys()\n",
    "    \n",
    "    #Word2Vec.most_similar(positive=[], negative=[], topn=10, restrict_vocab=None, indexer=None)\n",
    "    relevant_words = {}\n",
    "    for company in companies:\n",
    "        #print (\"company\", company)\n",
    "        if company in model.wv.vocab:\n",
    "            relevant_words[company] = model.most_similar(company,topn=n_words)\n",
    "        else:\n",
    "            for related_word in dict_companies[company]:\n",
    "                if related_word in model.wv.vocab:\n",
    "                    #print (\"related_word in model.wv.vocab\",related_word in model.wv.vocab)\n",
    "                    #print (related_word)\n",
    "                    relevant_words[company] = model.most_similar(related_word,topn=n_words)\n",
    "                    break\n",
    "    return relevant_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "#nltk.download('punkt')\n",
    "import warnings \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import collections\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "  \n",
    "# iterate through each article in the file \n",
    "data = [word_tokenize(plain_text) for plain_text in df_cleaned[\"plain_text\"]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PATH = \"./data/\"\n",
    "file = \"list_tokenized_pt\"\n",
    "with open(PATH +file , 'wb') as fp:\n",
    "    pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/\"\n",
    "file = \"list_tokenized_pt\"\n",
    "with open (PATH +file, 'rb') as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Matrix factorization to get top 20 words of a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "# HAL (Hyper Analogue Language)\n",
    "# CBOW\n",
    "\n",
    "# Create CBOW model \n",
    "model_cbow = Word2Vec(data, min_count = 1, size = 100, window = 5)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PATH = \"./data/models/\"\n",
    "file = \"CBOW_model_buff\"\n",
    "word_vectors = model_cbow.wv\n",
    "word_vectors.save(PATH + file+\".kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/models/\"\n",
    "file = \"CBOW_model_200k\"\n",
    "word_vectors = KeyedVectors.load(PATH + file+\".kv\", mmap='r')\n",
    "model_cbow = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('googles', 0.7780945301055908), ('apple', 0.7284873723983765), ('googl', 0.704851508140564), ('alphabet', 0.6509729623794556), ('spotify', 0.6320379972457886), ('facebook', 0.6315559148788452), ('microsoft', 0.6176584362983704), ('alphabets', 0.6062008142471313), ('apps', 0.605197548866272), ('stadia', 0.5996901988983154)]\n",
      "\n",
      "[('king', 0.8107793927192688), ('godfather', 0.5998413562774658), ('thatcher', 0.5920987129211426), ('mitford', 0.5835937261581421), ('altimus', 0.5723137259483337), ('chemouny', 0.5638600587844849), ('atwood', 0.5631056427955627), ('macbeth', 0.5596096515655518), ('enid', 0.5574297308921814), ('antoinette', 0.5557938814163208)]\n",
      "\n",
      "-0.056162722\n"
     ]
    }
   ],
   "source": [
    "print (model_cbow.most_similar('google'))\n",
    "vec = model_cbow['king'] - model_cbow['man'] + model_cbow['woman']\n",
    "print ()\n",
    "print (model_cbow.most_similar([vec]))\n",
    "print()\n",
    "print(model_cbow.similarity('apple', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local context window methods to get top 20 words on a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip- gram\n",
    "\n",
    "# Create Skip Gram model \n",
    "model_sg = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 4) \n",
    "# Start 12:49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sg\n",
    "PATH = \"./data/models/\"\n",
    "file = \"skip-gram_model\"\n",
    "word_vectors = model_sg.wv\n",
    "word_vectors.save(PATH + file+\".kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sg\n",
    "PATH = \"./data/models/\"\n",
    "file = \"skip-gram_model\"\n",
    "word_vectors = KeyedVectors.load(PATH + file+\".kv\", mmap='r')\n",
    "model_sg = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255358"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_sg.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aapl', 0.7750053405761719), ('iphone', 0.7719913125038147), ('google', 0.7477635145187378), ('iphones', 0.6974998712539673), ('android', 0.6899538636207581), ('spotify', 0.6810340285301208), ('9to5mac', 0.6795322895050049), ('watchos', 0.679144024848938), ('alphabet', 0.6761608123779297), ('carkey', 0.6720519661903381)]\n",
      "\n",
      "[('king', 0.8746283054351807), ('coretta', 0.6387332677841187), ('suffragettes', 0.6350522041320801), ('zog', 0.5962809324264526), ('stenhammar', 0.5960381031036377), ('foiling', 0.5941222906112671), ('hietpas', 0.5872020721435547), ('ducruet', 0.5851268768310547), ('luther', 0.5800729990005493), ('khesar', 0.5800399780273438)]\n",
      "\n",
      "0.21863858\n"
     ]
    }
   ],
   "source": [
    "print (model_sg.most_similar('apple'))\n",
    "vec = model_sg['king'] - model_sg['man'] +model_sg['woman']\n",
    "print ()\n",
    "print (model_sg.most_similar([vec]))\n",
    "print()\n",
    "print(model_sg.similarity('apple', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe to get top 20 words of a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe is a global log-bilinear regression model\n",
    "#from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "#glove_input_file = 'glove.txt'\n",
    "#word2vec_output_file = 'word2vec.txt'\n",
    "#glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load('glove-wiki-gigaword-300')\n",
    "#https://github.com/stanfordnlp/GloVe"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save glove\n",
    "PATH = \"./data/models/\"\n",
    "file = \"glove_model\"\n",
    "word_vectors = glove_model.wv\n",
    "word_vectors.save(PATH + file+\".kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove\n",
    "PATH = \"./data/models/\"\n",
    "file = \"glove_model\"\n",
    "word_vectors = KeyedVectors.load(PATH + file+\".kv\", mmap='r')\n",
    "glove_model = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('iphone', 0.5987042188644409), ('macintosh', 0.5836331248283386), ('ipod', 0.5761123895645142), ('microsoft', 0.5663833022117615), ('ipad', 0.5628098249435425), ('intel', 0.5457563400268555), ('ibm', 0.5286195278167725), ('google', 0.5282472372055054), ('imac', 0.5072520971298218), ('software', 0.4962984323501587)]\n",
      "\n",
      "[('king', 0.8065859079360962), ('queen', 0.689616322517395), ('monarch', 0.5575490593910217), ('throne', 0.5565374493598938), ('princess', 0.5518684387207031), ('mother', 0.5142154693603516), ('daughter', 0.5133156776428223), ('kingdom', 0.5025345087051392), ('prince', 0.5017740726470947), ('elizabeth', 0.49080315232276917)]\n",
      "\n",
      "0.090478964\n"
     ]
    }
   ],
   "source": [
    "print (glove_model.most_similar('apple'))\n",
    "vec = glove_model['king'] - glove_model['man'] +glove_model['woman']\n",
    "print ()\n",
    "print (glove_model.most_similar([vec]))\n",
    "print()\n",
    "print(glove_model.similarity('apple', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google sg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PATH = \"./data/models/\"\n",
    "file = \"freebase-vectors-skipgram1000.bin\"\n",
    "model_google_sg = gensim.models.KeyedVectors.load_word2vec_format(PATH+file, binary=True)\n",
    "# Start 10:03"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gensim.downloader as api\n",
    "model_google_sg = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save google sg\n",
    "PATH = \"./data/models/\"\n",
    "file = \"model_google_sg\"\n",
    "word_vectors = model_google_sg.wv\n",
    "word_vectors.save(PATH + file+\".kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KeyedVectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dc3c59899a2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/models/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_google_sg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".kv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel_google_sg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KeyedVectors' is not defined"
     ]
    }
   ],
   "source": [
    "# load google sg\n",
    "PATH = \"./data/models/\"\n",
    "file = \"model_google_sg\"\n",
    "word_vectors = KeyedVectors.load(PATH + file+\".kv\", mmap='r')\n",
    "model_google_sg = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_google_sg.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple_AAPL', 0.7456986308097839), ('Apple_Nasdaq_AAPL', 0.7300410270690918), ('Apple_NASDAQ_AAPL', 0.7175089120864868), ('Apple_Computer', 0.7145973443984985), ('iPhone', 0.6924266815185547), ('Apple_NSDQ_AAPL', 0.6868604421615601), ('Steve_Jobs', 0.6758421659469604), ('iPad', 0.6580768823623657), ('Apple_nasdaq_AAPL', 0.6444970369338989), ('AAPL_PriceWatch_Alert', 0.6439753174781799)]\n",
      "\n",
      "[('king', 0.8449392318725586), ('queen', 0.7300517559051514), ('monarch', 0.6454660892486572), ('princess', 0.6156250834465027), ('crown_prince', 0.5818676352500916), ('prince', 0.577711820602417), ('kings', 0.5613664388656616), ('sultan', 0.5376776456832886), ('Queen_Consort', 0.5344247817993164), ('queens', 0.5289887189865112)]\n",
      "\n",
      "0.11685416\n"
     ]
    }
   ],
   "source": [
    "print (model_google_sg.most_similar('apple'))\n",
    "vec = model_google_sg['king'] - model_google_sg['man'] +model_google_sg['woman']\n",
    "print ()\n",
    "print (model_google_sg.most_similar([vec]))\n",
    "print()\n",
    "print(model_google_sg.similarity('apple', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext & LTSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all the models into one & score each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_words_tfidf 52 40\n",
      "relevant_words_glove 42 20\n",
      "relevant_words_cbow 42 20\n",
      "relevant_words_sg 42 20\n",
      "relevant_words_google_sg 48 20\n",
      "relevant_words_tfidf [('apple', 0.19221091642898255), ('quarter', 0.1813757896319136), ('analyst', 0.15118983680084835), ('officer', 0.13039576289958923), ('think', 0.12978398724001325), ('chief', 0.11686635544725837), ('said', 0.11546809934419339), ('million', 0.10470667399352565), ('year', 0.10416846454512013), ('executive', 0.10082235178310875), ('thank', 0.09817567118056518), ('question', 0.09223465868111332), ('business', 0.08953489880929696), ('first', 0.08804188508963462), ('going', 0.08717832601671512), ('us', 0.0832110708925914), ('operator', 0.0831805862027488), ('new', 0.08301255662265287), ('would', 0.08202024828466181), ('financial', 0.07905948282017072), ('like', 0.07856134695086009), ('president', 0.07620409424496975), ('well', 0.07347791690670509), ('covid', 0.07175791753631373), ('call', 0.07142795430315475), ('company', 0.07105299660625786), ('one', 0.07070389586874286), ('customers', 0.06964788053166336), ('see', 0.0685976459883667), ('also', 0.06854789015504241), ('people', 0.06850500586183966), ('good', 0.06826272239409702), ('market', 0.06756314929921786), ('time', 0.06636995729168095), ('really', 0.06632632566987039), ('thanks', 0.0650085462292002), ('cash', 0.06475359072492178), ('19', 0.06300962430267361), ('growth', 0.06293439642351165), ('kind', 0.06107871771847573)]\n",
      "relevant_words_glove [('iphone', 0.5987042188644409), ('macintosh', 0.5836331248283386), ('ipod', 0.5761123895645142), ('microsoft', 0.5663833022117615), ('ipad', 0.5628098249435425), ('intel', 0.5457563400268555), ('ibm', 0.5286195278167725), ('google', 0.5282472372055054), ('imac', 0.5072520971298218), ('software', 0.4962984323501587), ('motorola', 0.47161784768104553), ('computer', 0.4711154103279114), ('apples', 0.46574175357818604), ('itunes', 0.4646926522254944), ('pc', 0.4600933790206909), ('iphones', 0.452551007270813), ('mac', 0.4503524601459503), ('ipods', 0.44740575551986694), ('cherry', 0.4464744031429291), ('computers', 0.44292744994163513)]\n",
      "relevant_words_cbow [('google', 0.7284873723983765), ('iphone', 0.6975679397583008), ('spotify', 0.666739821434021), ('googles', 0.6604712009429932), ('earpods', 0.6569204926490784), ('android', 0.6541101932525635), ('apples', 0.6331382393836975), ('homekit', 0.6308133602142334), ('aapl', 0.6295210719108582), ('ios', 0.6281814575195312), ('microsoft', 0.6176786422729492), ('iphones', 0.6138747334480286), ('macos', 0.6059454679489136), ('alphabet', 0.5999994277954102), ('airpods', 0.5960399508476257), ('apps', 0.595978856086731), ('wearables', 0.5949651002883911), ('nvidia', 0.5927913188934326), ('googl', 0.5908902883529663), ('sonos', 0.5848830938339233)]\n",
      "relevant_words_sg [('aapl', 0.7750053405761719), ('iphone', 0.7719913125038147), ('google', 0.7477635145187378), ('iphones', 0.6974998712539673), ('android', 0.6899538636207581), ('spotify', 0.6810340285301208), ('9to5mac', 0.6795322895050049), ('watchos', 0.679144024848938), ('alphabet', 0.6761608123779297), ('carkey', 0.6720519661903381), ('googles', 0.6628540754318237), ('jailbreak', 0.660087525844574), ('app', 0.6537714004516602), ('airpods', 0.6532244682312012), ('homekit', 0.6506407856941223), ('tmsc', 0.65050208568573), ('betwildwood', 0.6501995921134949), ('macos', 0.6495474576950073), ('ios', 0.6470357179641724), ('osx', 0.6468653678894043)]\n",
      "relevant_words_google_sg [('Apple_AAPL', 0.7456986308097839), ('Apple_Nasdaq_AAPL', 0.7300410270690918), ('Apple_NASDAQ_AAPL', 0.7175089120864868), ('Apple_Computer', 0.7145973443984985), ('iPhone', 0.6924266815185547), ('Apple_NSDQ_AAPL', 0.6868604421615601), ('Steve_Jobs', 0.6758421659469604), ('iPad', 0.6580768823623657), ('Apple_nasdaq_AAPL', 0.6444970369338989), ('AAPL_PriceWatch_Alert', 0.6439753174781799), ('Apple_iPad', 0.622774600982666), ('iPhones', 0.6192502975463867), ('Nexus_One', 0.6192277669906616), ('Appleâ_€_™', 0.617669403553009), ('Apple_AAPL_iPhone', 0.6159960031509399), ('Apple_AAPL_Fortune', 0.6144310832023621), ('Apple_Inc_AAPL.O', 0.6141717433929443), ('Mac_cloner_Psystar', 0.6066085696220398), ('Apple_APPL', 0.605398952960968), ('iPod', 0.6045313477516174)]\n"
     ]
    }
   ],
   "source": [
    "# retrieve all similarity words\n",
    "#tf.idf\n",
    "n_relevant_words = 20\n",
    "\n",
    "print (\"relevant_words_tfidf\", len(relevant_words_tfidf), len(relevant_words_tfidf[list(relevant_words_tfidf.keys())[0]]))\n",
    "#GloVe\n",
    "relevant_words_glove = getTopWords(glove_model,n_relevant_words,dict_companies)\n",
    "print (\"relevant_words_glove\",len(relevant_words_glove), len(relevant_words_glove[list(relevant_words_glove.keys())[0]]))\n",
    "#CBOW\n",
    "relevant_words_cbow = getTopWords(model_cbow,n_relevant_words,dict_companies)\n",
    "print (\"relevant_words_cbow\",len(relevant_words_cbow), len(relevant_words_cbow[list(relevant_words_cbow.keys())[0]]))\n",
    "#SG\n",
    "relevant_words_sg = getTopWords(model_sg,n_relevant_words,dict_companies)\n",
    "print (\"relevant_words_sg\",len(relevant_words_sg), len(relevant_words_sg[list(relevant_words_sg.keys())[0]]))\n",
    "# google SG\n",
    "relevant_words_google_sg = getTopWords(model_google_sg,n_relevant_words,dict_companies_unlowered)\n",
    "print (\"relevant_words_google_sg\",len(relevant_words_google_sg), len(relevant_words_google_sg[list(relevant_words_google_sg.keys())[0]]))\n",
    "\n",
    "#print(relevant_words_glove.keys())\n",
    "#print(relevant_words_sg.keys())\n",
    "#print(relevant_words_cbow.keys())\n",
    "\n",
    "print (\"relevant_words_tfidf\",relevant_words_tfidf[\"apple\"])\n",
    "print(\"relevant_words_glove\",relevant_words_glove[\"apple\"])\n",
    "print (\"relevant_words_cbow\",relevant_words_cbow[\"apple\"])\n",
    "print (\"relevant_words_sg\",relevant_words_sg[\"apple\"])\n",
    "print (\"relevant_words_google_sg\",relevant_words_google_sg[\"Apple\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in sg but not in glove\n",
      "stericycle\n",
      "words in glove but not in sg\n",
      "dish network\n"
     ]
    }
   ],
   "source": [
    "print (\"Words in sg but not in glove:\")\n",
    "for relevant_word_sg in relevant_words_sg.keys():\n",
    "    if relevant_word_sg not in relevant_words_glove.keys():\n",
    "        print (relevant_word_sg)\n",
    "print (\"Words in glove but not in sg:\")\n",
    "for relevant_word_glove in relevant_words_glove.keys():\n",
    "    if relevant_word_glove not in relevant_words_sg.keys():\n",
    "        print (relevant_word_glove)\n",
    "print(\"Words in \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_words_sg [('apples', 0.720359742641449), ('pear', 0.6450697183609009), ('fruit', 0.641014575958252), ('berry', 0.6302294135093689), ('pears', 0.6133961081504822), ('strawberry', 0.6058261394500732), ('peach', 0.6025872230529785), ('potato', 0.5960935354232788), ('grape', 0.5935864448547363), ('blueberry', 0.5866668224334717), ('cherries', 0.5784382224082947), ('mango', 0.5751855373382568), ('apricot', 0.5727777481079102), ('melon', 0.5719985365867615), ('almond', 0.5704830288887024), ('Granny_Smiths', 0.5695333480834961), ('grapes', 0.5692256093025208), ('peaches', 0.5659247040748596), ('pumpkin', 0.5651882886886597), ('apricots', 0.5645568370819092)]\n"
     ]
    }
   ],
   "source": [
    "print (\"relevant_words_sg\",relevant_words_google_sg[\"apple\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word(dictionary,tup):\n",
    "    for key in dictionary.keys():\n",
    "        if tup[0]==key:\n",
    "            #dictionary[key] = max(tup[1],dictionary[key])\n",
    "            dictionary[key] +=tup[1]\n",
    "            \n",
    "            return\n",
    "    #l.append(tup)\n",
    "    dictionary[tup[0]] = tup[1]\n",
    "\n",
    "def switch_tup(l, tup1, tup2):\n",
    "    tmp = l[tup1]\n",
    "    l[tup1] = l[tup2]\n",
    "    l[tup2] = tmp\n",
    "    #print (\"switch\")\n",
    "def bubble_sort_list_tup(l):\n",
    "    for i in range (len(l)):\n",
    "        for j in range (0,len(l)-i-1):\n",
    "            if l[j][1]<l[j+1][1]:\n",
    "                switch_tup(l, j, j+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Use the words’ similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iphone': 2.0682634711265564, 'google': 2.0044981241226196, 'iphones': 1.7639256119728088, 'aapl': 1.40452641248703, 'spotify': 1.3477738499641418, 'android': 1.3440640568733215, 'googles': 1.323325276374817, 'homekit': 1.2814541459083557, 'alphabet': 1.2761602401733398, 'ios': 1.2752171754837036, 'macos': 1.255492925643921, 'airpods': 1.249264419078827, 'microsoft': 1.1840619444847107, 'apples': 1.0988799929618835, '9to5mac': 0.6795322895050049, 'watchos': 0.679144024848938, 'carkey': 0.6720519661903381, 'jailbreak': 0.660087525844574, 'earpods': 0.6569204926490784, 'app': 0.6537714004516602, 'tmsc': 0.65050208568573, 'betwildwood': 0.6501995921134949, 'osx': 0.6468653678894043, 'apps': 0.595978856086731, 'wearables': 0.5949651002883911, 'nvidia': 0.5927913188934326, 'googl': 0.5908902883529663, 'sonos': 0.5848830938339233, 'macintosh': 0.5836331248283386, 'ipod': 0.5761123895645142, 'ipad': 0.5628098249435425, 'intel': 0.5457563400268555, 'ibm': 0.5286195278167725, 'imac': 0.5072520971298218, 'software': 0.4962984323501587, 'motorola': 0.47161784768104553, 'computer': 0.4711154103279114, 'itunes': 0.4646926522254944, 'pc': 0.4600933790206909, 'mac': 0.4503524601459503, 'ipods': 0.44740575551986694, 'cherry': 0.4464744031429291, 'computers': 0.44292744994163513, 'apple': 0.19221091642898255, 'quarter': 0.1813757896319136, 'analyst': 0.15118983680084835, 'officer': 0.13039576289958923, 'think': 0.12978398724001325, 'chief': 0.11686635544725837, 'said': 0.11546809934419339, 'million': 0.10470667399352565, 'year': 0.10416846454512013, 'executive': 0.10082235178310875, 'thank': 0.09817567118056518, 'question': 0.09223465868111332, 'business': 0.08953489880929696, 'first': 0.08804188508963462, 'going': 0.08717832601671512, 'us': 0.0832110708925914, 'operator': 0.0831805862027488, 'new': 0.08301255662265287, 'would': 0.08202024828466181, 'financial': 0.07905948282017072, 'like': 0.07856134695086009, 'president': 0.07620409424496975, 'well': 0.07347791690670509, 'covid': 0.07175791753631373, 'call': 0.07142795430315475, 'company': 0.07105299660625786, 'one': 0.07070389586874286, 'customers': 0.06964788053166336, 'see': 0.0685976459883667, 'also': 0.06854789015504241, 'people': 0.06850500586183966, 'good': 0.06826272239409702, 'market': 0.06756314929921786, 'time': 0.06636995729168095, 'really': 0.06632632566987039, 'thanks': 0.0650085462292002, 'cash': 0.06475359072492178, '19': 0.06300962430267361, 'growth': 0.06293439642351165, 'kind': 0.06107871771847573}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat all the words using their similarity scores\n",
    "# remove two same words and keep highest score OR add both scores\n",
    "# sort the words\n",
    "#one_word_companies = relevant_words_glove.keys()\n",
    "company_names= list(dict_companies.keys())\n",
    "related_words_concat_1 = {}\n",
    "for company in company_names: related_words_concat_1[company]= {}\n",
    "# Creat a unique list of words\n",
    "for company in related_words_concat_1.keys():\n",
    "    \n",
    "    if company in relevant_words_glove.keys():\n",
    "        for word in relevant_words_glove[company]:\n",
    "            add_word(related_words_concat_1[company], word)\n",
    "    \n",
    "    if company in relevant_words_sg.keys():\n",
    "        for word in relevant_words_sg[company]:\n",
    "            add_word(related_words_concat_1[company], word)\n",
    "    \n",
    "    if company in relevant_words_cbow.keys():\n",
    "        for word in relevant_words_cbow[company]:\n",
    "            add_word(related_words_concat_1[company], word)\n",
    "    \n",
    "    if company in relevant_words_tfidf.keys():   \n",
    "        for word in relevant_words_tfidf[company]:\n",
    "            add_word(related_words_concat_1[company], word) # convert to tuple\n",
    "    #Sort the list of words\n",
    "    related_words_concat_1[company] = {k: v for k, v in sorted(related_words_concat_1[company].items(), key=lambda item: -item[1])}\n",
    "        \n",
    "print (related_words_concat_1[\"apple\"])\n",
    "len(related_words_concat_1.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Score wrt. the number of lists they belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iphone': 3, 'google': 3, 'iphones': 3, 'microsoft': 2, 'apples': 2, 'aapl': 2, 'android': 2, 'spotify': 2, 'alphabet': 2, 'googles': 2, 'airpods': 2, 'homekit': 2, 'macos': 2, 'ios': 2, 'macintosh': 1, 'ipod': 1, 'ipad': 1, 'intel': 1, 'ibm': 1, 'imac': 1, 'software': 1, 'motorola': 1, 'computer': 1, 'itunes': 1, 'pc': 1, 'mac': 1, 'ipods': 1, 'cherry': 1, 'computers': 1, '9to5mac': 1, 'watchos': 1, 'carkey': 1, 'jailbreak': 1, 'app': 1, 'tmsc': 1, 'betwildwood': 1, 'osx': 1, 'earpods': 1, 'apps': 1, 'wearables': 1, 'nvidia': 1, 'googl': 1, 'sonos': 1, 'apple': 1, 'quarter': 1, 'analyst': 1, 'officer': 1, 'think': 1, 'chief': 1, 'said': 1, 'million': 1, 'year': 1, 'executive': 1, 'thank': 1, 'question': 1, 'business': 1, 'first': 1, 'going': 1, 'us': 1, 'operator': 1, 'new': 1, 'would': 1, 'financial': 1, 'like': 1, 'president': 1, 'well': 1, 'covid': 1, 'call': 1, 'company': 1, 'one': 1, 'customers': 1, 'see': 1, 'also': 1, 'people': 1, 'good': 1, 'market': 1, 'time': 1, 'really': 1, 'thanks': 1, 'cash': 1, '19': 1, 'growth': 1, 'kind': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets score the words compared to the number of times they appear in a list\n",
    "\n",
    "company_names= list(dict_companies.keys())\n",
    "related_words_concat_2 = {}\n",
    "for company in company_names: related_words_concat_2[company]= 0\n",
    "\n",
    "for company in related_words_concat_2.keys():\n",
    "    #creat list of all the words\n",
    "    frequency_map = {}\n",
    "    list_words = list()\n",
    "    if company in relevant_words_glove.keys():\n",
    "        list_words = list_words + relevant_words_glove[company]\n",
    "    if company in relevant_words_sg.keys():\n",
    "        list_words = list_words +relevant_words_sg[company] \n",
    "    if company in relevant_words_cbow.keys():\n",
    "        list_words = list_words + relevant_words_cbow[company] \n",
    "    if company in relevant_words_tfidf.keys():\n",
    "        list_words = list_words + relevant_words_tfidf[company]\n",
    "\n",
    "    for word in list_words:\n",
    "        if word[0] in frequency_map.keys():\n",
    "            frequency_map[word[0]] +=1\n",
    "        else:\n",
    "            frequency_map[word[0]] =1\n",
    "    frequency_map = {k: v for k, v in sorted(frequency_map.items(), key=lambda item: -item[1])}\n",
    "    related_words_concat_2[company] = frequency_map   \n",
    "\n",
    "print (related_words_concat_2[\"apple\"])\n",
    "len(related_words_concat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Compute precision score for each word (using the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [29:17<00:00, 35.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# Ranking the words using the \"precison\" score (with the corpus)\n",
    "#df_cleaned.tail(2)\n",
    "#data\n",
    "company_names= list(dict_companies.keys())\n",
    "related_words_concat_3 = {}\n",
    "related_words_concat_3_count = {}\n",
    "for company in company_names: related_words_concat_3[company]= 0\n",
    "\n",
    "for company in tqdm(related_words_concat_3.keys()):\n",
    "    frequency_map = {}\n",
    "    count_map = {}\n",
    "    n_artcles = 0\n",
    "    list_words = list()\n",
    "    if company in relevant_words_glove.keys():\n",
    "        list_words = list_words + [word[0] for word in relevant_words_glove[company]]\n",
    "    if company in relevant_words_sg.keys():\n",
    "        list_words = list_words +[word[0] for word in relevant_words_sg[company]] \n",
    "    if company in relevant_words_cbow.keys():\n",
    "        list_words = list_words + [word[0] for word in relevant_words_cbow[company]] \n",
    "    if company in relevant_words_tfidf.keys():\n",
    "        list_words = list_words + [word[0] for word in relevant_words_tfidf[company]]\n",
    "    set_words = set(list_words)\n",
    "                      \n",
    "    for index, row in df_cleaned.iterrows():\n",
    "        if company in row['label']:   # if related to company\n",
    "            n_artcles +=1\n",
    "            for word in set_words:\n",
    "                if word in data[index]:\n",
    "                    if word in frequency_map.keys():\n",
    "                        frequency_map[word] +=1\n",
    "                    else:\n",
    "                        frequency_map[word] =1\n",
    "    sorted_frequency_map = {k: v/n_artcles for k, v in sorted(frequency_map.items(), key=lambda item: -item[1])}\n",
    "    sorted_count_map = {k: v for k, v in sorted(frequency_map.items(), key=lambda item: -item[1])}\n",
    "    related_words_concat_3[company] = sorted_frequency_map\n",
    "    related_words_concat_3_count[company] = sorted_count_map\n",
    "\n",
    "#100%|██████████| 23/23 [13:43<00:00, 35.82s/it] \n",
    "#100%|██████████| 43/43 [22:57<00:00, 32.02s/it]\n",
    "#100%|██████████| 49/49 [29:17<00:00, 35.86s/it]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save dictionary\n",
    "PATH = \"./data/\"\n",
    "file = \"related_words_precison\"\n",
    "a_file = open(PATH + file + \"score\"+\".json\", \"w\")\n",
    "b_file = open(PATH + file + \"count\"+\".json\", \"w\")\n",
    "json.dump(related_words_concat_3 , a_file)\n",
    "json.dump(related_words_concat_3_count, b_file)\n",
    "a_file.close()\n",
    "b_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary \n",
    "PATH = \"./data/\"\n",
    "file = \"related_words_precison\"\n",
    "a_file = open(PATH + file +\"score\"+ \".json\", \"r\")\n",
    "b_file = open(PATH + file +\"count\"+ \".json\", \"r\")\n",
    "related_words_concat_3 = json.load(a_file)\n",
    "related_words_concat_3_count = json.load(b_file)\n",
    "#relevant_words_tfidf = dict(relevant_words_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'said': 0.8869693750288697, 'also': 0.7427132892974271, 'new': 0.727701048547277, 'would': 0.6748117695967482, 'one': 0.6467735230264677, 'first': 0.6188276594761882, 'year': 0.6062173772460622, 'people': 0.6017829922860178, '19': 0.5762390872557623, 'time': 0.5720818513557209, 'covid': 0.5450136264954502, 'like': 0.49577347683495776, 'well': 0.4456556884844566, 'million': 0.41932652778419327, 'company': 0.40722435216407227, 'going': 0.3755369763037554, 'president': 0.3690701649036907, 'see': 0.36514388655365143, 'chief': 0.3454663032934547, 'market': 0.34398817497343986, 'business': 0.32874497667328745, 'apple': 0.32135433507321354, 'us': 0.3015381772830154, 'think': 0.28846597995288464, 'good': 0.2808905723128089, 'financial': 0.2792276779527923, 'executive': 0.2790891034227909, 'quarter': 0.25848768996258487, 'really': 0.23696244630236962, 'officer': 0.23673148875236733, 'call': 0.23405238117234053, 'customers': 0.18952376553189523, 'growth': 0.18841516929188415, 'cash': 0.18795325419187953, 'kind': 0.1539101113215391, 'question': 0.15349438773153495, 'analyst': 0.144302277241443, 'google': 0.12522518361125226, 'app': 0.12245369301122454, 'thanks': 0.1166797542611668, 'aapl': 0.10531664280105317, 'operator': 0.10323802485103238, 'thank': 0.09533927664095339, 'apps': 0.07898748210078987, 'iphone': 0.07875652455078756, 'software': 0.07843318398078433, 'apples': 0.0622199639706222, 'alphabet': 0.05524504596055245, 'microsoft': 0.05519885445055199, 'iphones': 0.0453600628204536, 'googl': 0.034135525890341356, 'computer': 0.03353503626033535, 'android': 0.03016305603030163, 'ios': 0.019631391750196313, 'googles': 0.018799944570188, 'intel': 0.01593607095015936, 'spotify': 0.014088410550140883, 'ipad': 0.013811261490138112, 'computers': 0.011917409580119174, 'mac': 0.009561642570095617, 'wearables': 0.009053535960090535, 'cherry': 0.008499237840084993, 'pc': 0.008222088780082222, 'airpods': 0.006974918010069749, 'ibm': 0.006466811400064668, 'nvidia': 0.005866321770058663, 'itunes': 0.0046191510000461915, 'macos': 0.0013857453000138574, 'tmsc': 0.0008314471800083145, 'motorola': 0.0007852556700078526, 'sonos': 0.0006928726500069287, 'ipod': 0.0006928726500069287, 'watchos': 0.000554298120005543, 'imac': 0.0005081066100050811, 'homekit': 0.00046191510000461914, '9to5mac': 0.00046191510000461914, 'earpods': 0.00041572359000415724, 'carkey': 0.0003233405700032334, 'macintosh': 0.00023095755000230957, 'jailbreak': 0.00018476604000184767, 'osx': 0.00018476604000184767, 'ipods': 9.238302000092384e-05, 'betwildwood': 9.238302000092384e-05}\n",
      "{'said': 19202, 'also': 16079, 'new': 15754, 'would': 14609, 'one': 14002, 'first': 13397, 'year': 13124, 'people': 13028, '19': 12475, 'time': 12385, 'covid': 11799, 'like': 10733, 'well': 9648, 'million': 9078, 'company': 8816, 'going': 8130, 'president': 7990, 'see': 7905, 'chief': 7479, 'market': 7447, 'business': 7117, 'apple': 6957, 'us': 6528, 'think': 6245, 'good': 6081, 'financial': 6045, 'executive': 6042, 'quarter': 5596, 'really': 5130, 'officer': 5125, 'call': 5067, 'customers': 4103, 'growth': 4079, 'cash': 4069, 'kind': 3332, 'question': 3323, 'analyst': 3124, 'google': 2711, 'app': 2651, 'thanks': 2526, 'aapl': 2280, 'operator': 2235, 'thank': 2064, 'apps': 1710, 'iphone': 1705, 'software': 1698, 'apples': 1347, 'alphabet': 1196, 'microsoft': 1195, 'iphones': 982, 'googl': 739, 'computer': 726, 'android': 653, 'ios': 425, 'googles': 407, 'intel': 345, 'spotify': 305, 'ipad': 299, 'computers': 258, 'mac': 207, 'wearables': 196, 'cherry': 184, 'pc': 178, 'airpods': 151, 'ibm': 140, 'nvidia': 127, 'itunes': 100, 'macos': 30, 'tmsc': 18, 'motorola': 17, 'sonos': 15, 'ipod': 15, 'watchos': 12, 'imac': 11, 'homekit': 10, '9to5mac': 10, 'earpods': 9, 'carkey': 7, 'macintosh': 5, 'jailbreak': 4, 'osx': 4, 'ipods': 2, 'betwildwood': 2}\n"
     ]
    }
   ],
   "source": [
    "print (related_words_concat_3[\"apple\"])\n",
    "print(related_words_concat_3_count[\"apple\"])\n",
    "#rwc3_frac = dict(related_words_concat_3)\n",
    "#rwc3_frac[\"apple\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_plain_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    #[:punct:], ,[^0-9], [^a-z]\n",
    "    #text = re.sub(\"[^a-z],[^:punct:],[^0-9]\", ' ', text)\n",
    "    text = re.sub(\"[^a-z0-9]\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_company(plain_text, word_list):\n",
    "    n_words = len(word_list)\n",
    "    words_in_text = 0\n",
    "    #print (word_list)\n",
    "    for token in plain_text.split(\" \"):\n",
    "        for word in word_list:\n",
    "             if word == token:\n",
    "                words_in_text +=1\n",
    "                #print(word)\n",
    "    return words_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def label_text(plain_text,related_words,n_sig_words=10, min_score = 0.01):\n",
    "    label_dict = {}\n",
    "    #print (sig_words_list)\n",
    "    for company in related_words.keys():\n",
    "        #print(\"Company\", company)\n",
    "        sig_words_list = list(related_words[company].keys())[:n_sig_words] + [company]\n",
    "        score = score_company(plain_text, sig_words_list)\n",
    "        #print (score)\n",
    "        if score>min_score:\n",
    "            label_dict[company]= score\n",
    "    # Soft_max\n",
    "    sum_exp = sum([np.exp(v) for v in label_dict.values()])\n",
    "    label_dict = {k: np.exp(v)/sum_exp for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    # Exponomial\n",
    "    #max_val = max(label_dict.values())\n",
    "    #label_dict = {k: v/max_val for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 0.9982787595210628,\n",
       " 'alphabet': 0.0009103123974033909,\n",
       " 'amazon': 0.00033488521604819544,\n",
       " 'intel': 0.00033488521604819544,\n",
       " 'microsoft': 0.00012319738613638786,\n",
       " '21st century fox': 1.667295314677839e-05,\n",
       " 'cisco': 3.053757890451772e-07,\n",
       " 'comcast': 3.053757890451772e-07,\n",
       " 'starbucks': 3.053757890451772e-07,\n",
       " 'autodesk': 1.1234147462122804e-07,\n",
       " 'advanced micro devices': 4.1328118904033145e-08,\n",
       " 'ebay': 4.1328118904033145e-08,\n",
       " 'netflix': 4.1328118904033145e-08,\n",
       " 'nvidia': 4.1328118904033145e-08,\n",
       " 'universal display': 4.1328118904033145e-08,\n",
       " 'equinix': 1.5203765287082636e-08,\n",
       " 'adobe': 5.593152677513733e-09,\n",
       " 'ca technologies': 5.593152677513733e-09,\n",
       " 'facebook': 5.593152677513733e-09,\n",
       " 'mckesson': 5.593152677513733e-09,\n",
       " 'qualcomm': 5.593152677513733e-09,\n",
       " 'tesla motors': 5.593152677513733e-09,\n",
       " 'liberty global': 2.0576058813903088e-09,\n",
       " 'marriott international': 7.569509017969398e-10,\n",
       " 'mattel': 2.784666747472775e-10,\n",
       " 'bed bath & beyond': 1.0244216469089824e-10,\n",
       " 'discovery communications': 1.0244216469089824e-10,\n",
       " 'the priceline group': 1.0244216469089824e-10,\n",
       " 'activision blizzard': 3.76863662988805e-11}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_text = \"\"\"\n",
    "The New York Times said on Monday that it was exiting its partnership with Apple News, as news organizations struggle to compete with large tech companies for readers’ attention and dollars.\n",
    "\n",
    "Starting on Monday, Times articles were no longer appearing alongside those from other publications in the curated Apple News feed available on Apple devices.\n",
    "\n",
    "The Times is one of the first media organizations to pull out of Apple News. The Times, which has made adding new subscribers a key business goal, said Apple had given it little in the way of direct relationships with readers and little control over the business. It said it hoped to instead drive readers directly to its own website and mobile app so that it could “fund quality journalism.”\n",
    "\n",
    "“Core to a healthy model between The Times and the platforms is a direct path for sending those readers back into our environments, where we control the presentation of our report, the relationships with our readers and the nature of our business rules,” Meredith Kopit Levien, chief operating officer, wrote in a memo to employees. “Our relationship with Apple News does not fit within these parameters.”\n",
    "\n",
    "An Apple spokesman said that The Times “only offered Apple News a few stories a day,” and that the company would continue to provide readers with trusted information from thousands of publishers.\n",
    "\n",
    "“We are also committed to supporting quality journalism through the proven business models of advertising, subscriptions and commerce,” he said.\"\n",
    "\"\"\"\n",
    "plain_text = clean_plain_text(plain_text)\n",
    "related_words = related_words_concat_1\n",
    "n_sig_words= 100\n",
    "min_score = 10 # nbr of sig words in text\n",
    "#print (plain_text)\n",
    "label_dict = label_text(plain_text,related_words, n_sig_words, min_score)\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------- Annexe Testing -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python program to generate word vectors using Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "#nltk.download('punkt')\n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-ed853e0b9298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# iterate through each article in the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclean_articles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# tokenize the article into words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_articles' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply 2 Word2Vec models to articles   \n",
    "\n",
    "data = [] \n",
    "  \n",
    "# iterate through each article in the file \n",
    "for i in clean_articles: \n",
    "    temp = [] \n",
    "    # tokenize the article into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "\n",
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'australia' \" + \n",
    "               \"and 'melbourne' - CBOW : \", \n",
    "    model1.similarity('melbourne', 'australia')) \n",
    "\n",
    "print(model1.wv.most_similar('melbourne'))\n",
    "    \n",
    "\n",
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 4) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'australia' \" +\n",
    "          \"and 'melbourne' - Skip Gram : \", \n",
    "    model2.similarity('melbourne', 'australia')) \n",
    "print(model2.wv.most_similar('melbourne'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-855081c8fe11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# enumerate data it is trained on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "# FOR GENSIN USING CBOW Manipulations\n",
    "\n",
    "# enumerate data it is trained on\n",
    "for i, word in enumerate(model1.wv.vocab):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "# Show frequencies\n",
    "#print(\"Original List : \",data)\n",
    "data_flat = []\n",
    "for line in data:\n",
    "    for word in line:\n",
    "        data_flat.append(word)\n",
    "\n",
    "\n",
    "ctr = collections.Counter(data_flat)\n",
    "#print(\"Frequency of the elements in the List : \",ctr)\n",
    "ctr[\"the\"] # count of word \"the\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.itf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X = vectorizer.fit_transform(clean_articles)\n",
    "\n",
    "#print(vectorizer.get_feature_names()[:10])\n",
    "#print(X.shape)\n",
    "#print(vectorizer.get_stop_words())\n",
    "#print(vectorizer.get_params(deep=True))\n",
    "\n",
    "n_articles, n_distinct_words = X.shape\n",
    "print(n_articles, n_distinct_words)\n",
    "\n",
    "collect_word_importance = []\n",
    "#place tf-idf values in a pandas data frame \n",
    "for tf_idf_vector_id in range(n_articles):\n",
    "    \n",
    "    tf_idf_vector=X[tf_idf_vector_id]\n",
    "    #print (tf_idf_vector.todense().sum())\n",
    "    #print (tf_idf_vector.T.todense())\n",
    "    df = pd.DataFrame(tf_idf_vector.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df_word_importance = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    word_importance_list = np.array(df_word_importance.index)\n",
    "    collect_word_importance.append(word_importance_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each line corresponds to the highest scored words in the article of same index.\n",
    "collect_word_importance = np.array(collect_word_importance)\n",
    "collect_word_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfTransformer\n",
    "#TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]\n",
    "\n",
    "\n",
    "corpus = ['this is the first document',\n",
    "           'this document is the second document',\n",
    "          'and this is the third one',\n",
    "           'is this the first document']\n",
    "vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n",
    "               'and', 'one']\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()\n",
    "pipe['tfid'].idf_\n",
    "pipe.transform(corpus).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('count', CountVectorizer()),\n",
    "                  ('tfid', TfidfTransformer())]).fit(clean_articles)\n",
    "pipe['count'].transform(clean_articles).toarray().shape\n",
    "print (pipe['tfid'].idf_)\n",
    "Tfidf_res = pipe.transform(clean_articles)\n",
    "Tfidf_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tutorial\n",
    "\n",
    "#Dataset and Imports\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    " \n",
    "# this is a very toy example, do not try this at home unless you want to understand the usage differences \n",
    "docs=[\"the house had a tiny little mouse\", \n",
    "\"the cat saw the mouse\", \n",
    "\"the mouse ran away from the house\", \n",
    "\"the cat finally ate the mouse\", \n",
    "\"the end of the mouse story\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer\n",
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer() \n",
    "# this steps generates word counts for the words in your docs \n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector.shape\n",
    "# 5 texts, 9 distinct words -> gives the count for each word in each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the IDF values\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the TFIDF score for your documents\n",
    "# count matrix \n",
    "count_vector=cv.transform(docs) #<==> word_count_vector\n",
    "\n",
    "# tf-idf scores \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names() \n",
    " \n",
    "#get tfidf vector for FFFFFFFFFirst document \n",
    "first_document_vector=tf_idf_vector[0] \n",
    " \n",
    "#print the scores (Tf-idf scores of first document)\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidfvectorizer Usage - Compute all at Once\n",
    "\n",
    " \n",
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    "#fitted_vectorizer=tfidf_vectorizer.fit(docs)               # This method would work too\n",
    "#tfidf_vectorizer_vectors=fitted_vectorizer.transform(docs)  \n",
    " \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
