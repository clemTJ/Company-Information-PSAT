{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Labeling & Lexical Fields Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Unlabelled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "raw_json_data = []\n",
    "with open('./data/20200420_20200714_business_articles.json') as f:\n",
    "    for line in f:\n",
    "        raw_json_data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type <class 'list'>\n",
      "json <class 'dict'>\n",
      "keys dict_keys(['published', 'link', 'message', 'Feed', 'title', '@version', 'author', '@timestamp', 'full-text', 'type'])\n",
      "length 416307\n"
     ]
    }
   ],
   "source": [
    "print (\"data type\",type (raw_json_data))\n",
    "print (\"json\",type (raw_json_data[0]))\n",
    "print (\"keys\",raw_json_data[0].keys())\n",
    "print (\"length\", len(raw_json_data))\n",
    "#print (raw_json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Company Names & Related Names(52 companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['21st century fox', 'activision blizzard', 'adobe ', 'advanced micro devices', 'akamai technologies', 'akamai tecnologies', 'alexion pharmaceuticals', 'amazon', 'american airlines group', 'amgen', 'analog devices', 'apple', 'autodesk', 'automatic data processing', 'baidu', 'bed bath & beyond', 'biogen', 'ca technologies', 'celgene', 'cerner', 'cisco ', 'cognizant', 'comcast', 'discovery communications', 'dish network', 'ebay', 'electronic arts', 'equinix', 'expeditors international', 'facebook', 'alphabet', 'intel', 'liberty global', 'liberty interactive', 'linear technology', 'marriott international', 'mattle', 'mattel', 'mckesson ', 'mckesson', 'microsoft', 'netflix', 'nvidia', 'paypal', 'qualcomm', 'starbucks', 'stericycle', 'tesla motors', 'texas instruments', 'the priceline group', 'universal display ', 'universal display'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetching company names (52 companies)\n",
    "df = pd.read_excel (r'./data/comapny_name-related_words.xlsx', header = None)\n",
    "# Lower\n",
    "df[0] = [row[0].lower() for index, row in df.iterrows()] \n",
    "# Split company name and related names\n",
    "split = np.array([row[0].split(\";\") for index, row in df.iterrows()])\n",
    "df[\"company_name\"] = split[:,0]\n",
    "df[\"related_name\"] = split[:,1]\n",
    "df.drop(columns = [0], inplace=True)\n",
    "# build dictionary of related name of companies\n",
    "dict_companies = {}\n",
    "company_names = df[\"company_name\"].unique()\n",
    "for name in company_names:\n",
    "    df_tmp = df[df[\"company_name\"] == name]\n",
    "    dict_companies[name] = list(df_tmp[\"related_name\"])\n",
    "dict_companies.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting url, title & full_text of each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list()\n",
    "plain_texts = list()\n",
    "titles = list()\n",
    "labels = list()\n",
    "\n",
    "min_article_size = 2000\n",
    "for article in raw_json_data:\n",
    "    plain_text = article.get('full-text')\n",
    "    title = article.get('title')\n",
    "    url = article.get('link')\n",
    "    if (plain_text and \"Article `download()` failed\" != plain_text[:27] and \"Please enable cookies\" != plain_text[:21] and len(plain_text)>min_article_size):\n",
    "        plain_texts.append(plain_text)\n",
    "        urls.append(url)\n",
    "        titles.append(title)\n",
    "        labels.append(list())\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataFrame with extacted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#Statistics\n",
    "# 358192 removing \"Article `download()` failed\" \n",
    "# 340987 removing \"Article `download()` failed\" and \"Please enable cookies\"\n",
    "# 215039 removing \"Article `download()` failed\" and \"Please enable cookies\" and size<min_article_size = 2000\n",
    "data = np.array([urls,titles, plain_texts, labels]).T\n",
    "columns=[\"url\", \"title\", \"plain_text\", \"label\"]\n",
    "df_articles = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>plain_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.dailymail.co.uk/tvshowbiz/article-...</td>\n",
       "      <td>MasterChef's Harry Foster hits back at claims ...</td>\n",
       "      <td>Eliminated MasterChef contestant Harry Foster ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jun/...</td>\n",
       "      <td>Protest arrests logjam tests NYC legal system,...</td>\n",
       "      <td>NEW YORK (AP) - A wave of arrests in the New Y...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-83114...</td>\n",
       "      <td>Labour's Anneliese Dodds says she will REFUSE ...</td>\n",
       "      <td>A top shadow minister today said there was not...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://feeds.reuters.com/~r/Reuters/worldNews/...</td>\n",
       "      <td>Civil unrest rages in Minneapolis over raciall...</td>\n",
       "      <td>MINNEAPOLIS (Reuters) - Peaceful rallies gave ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-82734...</td>\n",
       "      <td>Australia 'beats the cr*p' out of coronavirus ...</td>\n",
       "      <td>Australia is 'beating the c**p' out of coronav...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.dailymail.co.uk/tvshowbiz/article-...   \n",
       "1  https://www.washingtontimes.com/news/2020/jun/...   \n",
       "2  https://www.dailymail.co.uk/news/article-83114...   \n",
       "3  http://feeds.reuters.com/~r/Reuters/worldNews/...   \n",
       "4  https://www.dailymail.co.uk/news/article-82734...   \n",
       "\n",
       "                                               title  \\\n",
       "0  MasterChef's Harry Foster hits back at claims ...   \n",
       "1  Protest arrests logjam tests NYC legal system,...   \n",
       "2  Labour's Anneliese Dodds says she will REFUSE ...   \n",
       "3  Civil unrest rages in Minneapolis over raciall...   \n",
       "4  Australia 'beats the cr*p' out of coronavirus ...   \n",
       "\n",
       "                                          plain_text label  \n",
       "0  Eliminated MasterChef contestant Harry Foster ...    []  \n",
       "1  NEW YORK (AP) - A wave of arrests in the New Y...    []  \n",
       "2  A top shadow minister today said there was not...    []  \n",
       "3  MINNEAPOLIS (Reuters) - Peaceful rallies gave ...    []  \n",
       "4  Australia is 'beating the c**p' out of coronav...    []  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning full_text of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliminated masterchef contestant harry foster has hit back at unfair criticism against judge melissa leong  the show s first female judge  40  has faced a barrage of trolling  with haters taking aim at everything from her behaviour on set to her fashion sense  despite being eliminated on tuesday night s episode  harry had nothing but good things to say about the melbourne based food writer   this could not be further from the truth   eliminated masterchef australia contestant harry foster  pictured  has hit back at unfair criticism against judge melissa leong  she s a queen  i love her   harry told huffpost australia   she is energetic  passionate and really just vibrant   when asked about accusations melissa was rude and biased on the show  he said   this could not be further from the truth   all three judges have received an overwhelmingly positive response from fans  but melissa has copped a backlash from a vocal minority   she s a queen   the show s first female judge  40  has faced a barrage of trolling  with haters taking aim at everything from her behaviour on set to her fashion sense while many have praised her for her fashion sense and positivity  others claim she waits for feedback from jock zonfrillo and andy allen before repeating it as her own   any chance melissa leong has an original idea on masterchef  or just continue to wait for others to tell her what to think about the dish   one viewer tweeted  another added   new judge melissa speaks like an affected melbourne millennial  and as an affected melbourne millennial myself  i am in a good position to note how grating that is   mixed  all three masterchef judges have received an overwhelmingly positive response from fans  but melissa has copped a backlash from a vocal minority kind words   she is energetic  passionate and really just vibrant   said harry  left   who was eliminated on tuesday night s episode  i love masterchef bringing back the old contestants but loathe the new judges  especially melissa leong  i ve only been watching for ten minutes and she s already irritating   a third fan wrote on twitter  others claimed she was biased towards certain contestants  such as hayden quinn and khanh ong  masterchef continues thursday at 7 30pm on channel 10 '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove every non-letter/number character\n",
    "#df_cleaned = df_articles.copy(deep= True)\n",
    "df_cleaned = df_articles.head(5000).copy(deep= True)\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    row[\"plain_text\"] = row[\"plain_text\"].lower()\n",
    "    row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "    #[:punct:], ,[^0-9], [^a-z]\n",
    "    #row[\"plain_text\"] = re.sub(\"[^a-z],[^:punct:],[^0-9]\", ' ', row[\"plain_text\"])\n",
    "    row[\"plain_text\"] = re.sub(\"[^a-z0-9]\", ' ', row[\"plain_text\"])\n",
    "    #row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "df_cleaned[\"plain_text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Stop Words & Removing them from plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliminated masterchef contestant harry foster hit back unfair criticism judge melissa leong show first female judge 40 faced barrage trolling haters taking aim everything behaviour set fashion sense despite eliminated tuesday night episode harry nothing good things say melbourne based food writer could truth eliminated masterchef australia contestant harry foster pictured hit back unfair criticism judge melissa leong queen love harry told huffpost australia energetic passionate really vibrant asked accusations melissa rude biased show said could truth three judges received overwhelmingly positive response fans melissa copped backlash vocal minority queen show first female judge 40 faced barrage trolling haters taking aim everything behaviour set fashion sense many praised fashion sense positivity others claim waits feedback jock zonfrillo andy allen repeating chance melissa leong original idea masterchef continue wait others tell think dish one viewer tweeted another added new judge melissa speaks like affected melbourne millennial affected melbourne millennial good position note grating mixed three masterchef judges received overwhelmingly positive response fans melissa copped backlash vocal minority kind words energetic passionate really vibrant said harry left eliminated tuesday night episode love masterchef bringing back old contestants loathe new judges especially melissa leong watching ten minutes already irritating third fan wrote twitter others claimed biased towards certain contestants hayden quinn khanh ong masterchef continues thursday 7 30pm channel 10 '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all stop words from plain text\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for stop_word in stop_words:\n",
    "        row[\"plain_text\"] = re.sub(' '+stop_word+' ', ' ', row[\"plain_text\"])\n",
    "    row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "df_cleaned[\"plain_text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Articles with Company Names \n",
    "### Check if Articles Talk of Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4990                                 []\n",
       "4991    [facebook, alphabet, microsoft]\n",
       "4992                                 []\n",
       "4993                                 []\n",
       "4994                        [microsoft]\n",
       "4995                                 []\n",
       "4996                                 []\n",
       "4997                        [microsoft]\n",
       "4998                                 []\n",
       "4999                                 []\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in df_cleaned.iterrows(): # initialize labels\n",
    "    row['label'] = []\n",
    "    \n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for company in company_names:\n",
    "        if company in row[\"plain_text\"]:\n",
    "            row['label'].append(company)\n",
    "        else:\n",
    "            for related_name in dict_companies[company]:\n",
    "                if related_name in row[\"plain_text\"]:\n",
    "                    row['label'].append(company)\n",
    "                    break\n",
    "df_cleaned[\"label\"].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Number of Articles that each Company is Associated to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 46 companies with associated articles over the 52 total companies\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "dict_count = {}\n",
    "for company in company_names: dict_count[company]= 0\n",
    "    \n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for company in company_names:\n",
    "        if company in row[\"label\"]:\n",
    "            dict_count[company]+=1\n",
    "dict_count          \n",
    "\n",
    "companies_w_articles = list()\n",
    "for company in company_names:\n",
    "    if dict_count[company]>0:\n",
    "        companies_w_articles.append(company)\n",
    "print (\"there are %d companies with associated articles over the %d total companies\"%(len(companies_w_articles),len(company_names)) )\n",
    "#dict_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.idf on Companies that have Associated Articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'21st century fox': ['films genres disney',\n",
       "  'happen the world',\n",
       "  'dangers loom top',\n",
       "  'company ashen faced',\n",
       "  'company ashen',\n",
       "  'star trek perhaps',\n",
       "  'classic green ottawa',\n",
       "  'green source mlb',\n",
       "  'green source',\n",
       "  'green ottawa reuters',\n",
       "  'green ottawa',\n",
       "  'classic green source',\n",
       "  'llc toronto reuters',\n",
       "  'llc toronto',\n",
       "  'llc report state',\n",
       "  'royal tea palace',\n",
       "  'royal tea ottawa',\n",
       "  'llc report',\n",
       "  'llc pop culture',\n",
       "  'llc pop'],\n",
       " 'activision blizzard': ['beyond new',\n",
       "  'morning open montgomery',\n",
       "  'filming video',\n",
       "  'llc activision',\n",
       "  'llc activision blizzard',\n",
       "  'year beyond new',\n",
       "  'week going',\n",
       "  'week going business',\n",
       "  'resumed filming video',\n",
       "  'growth charts abiomed',\n",
       "  'morning open japanese',\n",
       "  'kellytyko japanese',\n",
       "  'open japanese',\n",
       "  'open japanese electronics',\n",
       "  'call transcripts johnny',\n",
       "  'next week going',\n",
       "  'times llc activision',\n",
       "  'transcripts johnny',\n",
       "  'transcripts johnny jj',\n",
       "  'open montgomery'],\n",
       " 'adobe ': ['technologist left',\n",
       "  'growing technologist left',\n",
       "  'technologist left workers',\n",
       "  'labor',\n",
       "  'partner',\n",
       "  'partner hotels retailers',\n",
       "  'partnerships',\n",
       "  'partnerships like',\n",
       "  'partnerships like shopify',\n",
       "  'party',\n",
       "  'party sales',\n",
       "  'party sales strong',\n",
       "  'pay',\n",
       "  'pay 600',\n",
       "  'pay 600 monthly',\n",
       "  'pay bills',\n",
       "  'pay bills benz',\n",
       "  'paying',\n",
       "  'paying work',\n",
       "  'paying work know'],\n",
       " 'advanced micro devices': ['away meatpackers weatherman',\n",
       "  'pint reuters',\n",
       "  'yield pint reuters',\n",
       "  'business kansas city',\n",
       "  'paragraph annaly',\n",
       "  'paragraph annaly capital',\n",
       "  'time fact check',\n",
       "  'paragraph berlin',\n",
       "  'paragraph berlin reuters',\n",
       "  'business kansas',\n",
       "  'pint reuters economy',\n",
       "  'home washington ap',\n",
       "  'toennies paragraph berlin',\n",
       "  'home washington',\n",
       "  'time fact',\n",
       "  'parsley john sanfilippo',\n",
       "  'parsley john',\n",
       "  'today preferred apartment',\n",
       "  'today preferred',\n",
       "  'story students'],\n",
       " 'akamai technologies': ['10',\n",
       "  'parties looking sell',\n",
       "  'payment',\n",
       "  'party sales strong',\n",
       "  'party sales',\n",
       "  'party',\n",
       "  'partnerships like shopify',\n",
       "  'partnerships like',\n",
       "  'partnerships',\n",
       "  'parties looking',\n",
       "  'outpaced overall',\n",
       "  'parties',\n",
       "  'overall business even',\n",
       "  'overall business',\n",
       "  'overall',\n",
       "  'outset current crisis',\n",
       "  'outset current',\n",
       "  'outset',\n",
       "  'payment technology',\n",
       "  'payment technology growing'],\n",
       " 'akamai tecnologies': ['partners what',\n",
       "  'due breather shopify',\n",
       "  'breather shopify stock',\n",
       "  'paul rockwell top',\n",
       "  'rockwell file photo',\n",
       "  'comment washington reuters',\n",
       "  'comment washington',\n",
       "  'comment shopify nyse',\n",
       "  'comment shopify',\n",
       "  'technologist new york',\n",
       "  'declined comment shopify',\n",
       "  'technologist new',\n",
       "  'declined comment washington',\n",
       "  'ibd leaderboard shopify',\n",
       "  'paul rockwell file',\n",
       "  'ibd leaderboard washington',\n",
       "  'leaderboard washington reuters',\n",
       "  'leaderboard washington',\n",
       "  'leaderboard shopify stock',\n",
       "  'leaderboard shopify'],\n",
       " 'alexion pharmaceuticals': ['haskett adds',\n",
       "  'haskett reuters hedge',\n",
       "  'haskett adds intesa',\n",
       "  'haskett reuters',\n",
       "  'bengaluru adds',\n",
       "  'gordon haskett reuters',\n",
       "  'gordon haskett adds',\n",
       "  'bengaluru adds intesa',\n",
       "  'paramasivam bengaluru adds',\n",
       "  'zealands ultrafast fibre',\n",
       "  'reported 2000',\n",
       "  'resources',\n",
       "  'resources exor',\n",
       "  'resources exor updates',\n",
       "  'resources limited',\n",
       "  'resources limited confirmed',\n",
       "  'elliott management publicly',\n",
       "  'reported 2000 gmt',\n",
       "  'reinsurer parnerre',\n",
       "  'reported'],\n",
       " 'amazon': ['official travelers bangkok',\n",
       "  'us sao paulo',\n",
       "  'americans reuters',\n",
       "  'test flight rebuilding',\n",
       "  'americans london reuters',\n",
       "  'americans london',\n",
       "  'record one roku',\n",
       "  'record one',\n",
       "  'time designer',\n",
       "  'time designer emily',\n",
       "  'time elon',\n",
       "  'time elon musk',\n",
       "  'new program shopify',\n",
       "  'us sao',\n",
       "  'films genres disney',\n",
       "  'new markets in',\n",
       "  'new lows washington',\n",
       "  'new lows reuters',\n",
       "  'new lows psycho',\n",
       "  'new lows laid'],\n",
       " 'american airlines group': ['transcripts workday',\n",
       "  'transcripts adds intesa',\n",
       "  'transcripts antero',\n",
       "  'said new york',\n",
       "  'said new',\n",
       "  'counting houston',\n",
       "  'counting houston reuters',\n",
       "  'transcripts american financial',\n",
       "  'transcripts american airlines',\n",
       "  'transcripts american',\n",
       "  'transcripts after finalizing',\n",
       "  'test positive reuters',\n",
       "  'test positive washington',\n",
       "  'transcripts after',\n",
       "  'transcripts adds',\n",
       "  'july vancouver',\n",
       "  'permission star bulk',\n",
       "  '175 38 broadcom',\n",
       "  '175 38 reuters',\n",
       "  'permission star'],\n",
       " 'amgen': ['use new york',\n",
       "  'biotechnology reuters president',\n",
       "  'trading reuters',\n",
       "  'intelligence biotechnology reuters',\n",
       "  'clinical use reuters',\n",
       "  'use reuters president',\n",
       "  'use reuters',\n",
       "  'gordon haskett new',\n",
       "  'use new',\n",
       "  'clinical use new',\n",
       "  'gordon haskett reuters',\n",
       "  'trading in april',\n",
       "  'trading in',\n",
       "  'hours trading reuters',\n",
       "  'hours trading in',\n",
       "  'biotechnology reuters',\n",
       "  'biotechnology reuters amgen',\n",
       "  'trading reuters amgen',\n",
       "  'haskett reuters hedge',\n",
       "  'haskett reuters'],\n",
       " 'apple': ['leave president trump',\n",
       "  'open internet television',\n",
       "  'com geneva reuters',\n",
       "  'com geneva',\n",
       "  'com fans make',\n",
       "  'com fans',\n",
       "  'com democratic presidential',\n",
       "  'com democratic',\n",
       "  'untouched 10',\n",
       "  'public health annaly',\n",
       "  'tests mongodb',\n",
       "  'wobbly market gildan',\n",
       "  'tests mongodb inc',\n",
       "  'untouched 10 covid',\n",
       "  'concerns los',\n",
       "  'concerns los angeles',\n",
       "  'kind right berkeley',\n",
       "  'arenas washington reuters',\n",
       "  'arenas washington',\n",
       "  'arenas reuters national'],\n",
       " 'autodesk': ['week it',\n",
       "  'optimistic reuters yahaira',\n",
       "  'oil reserve nba',\n",
       "  'oil reserve new',\n",
       "  'hobbies in march',\n",
       "  'optimistic london',\n",
       "  'optimistic london sydney',\n",
       "  'hobbies in',\n",
       "  'optimistic reuters',\n",
       "  'harmful industry washington',\n",
       "  'xfinity series high',\n",
       "  'harmful industry file',\n",
       "  'joeygarrison personal training',\n",
       "  'joeygarrison personal',\n",
       "  'next year london',\n",
       "  'water network evans',\n",
       "  'next year defence',\n",
       "  'hotel ballys pompeo',\n",
       "  'outbreak going',\n",
       "  'outbreak going business'],\n",
       " 'automatic data processing': ['blurred sensient',\n",
       "  'call transcripts forward',\n",
       "  'record houston',\n",
       "  'record houston reuters',\n",
       "  'call transcripts opinion',\n",
       "  'call transcripts mongodb',\n",
       "  'call transcripts majesco',\n",
       "  'call transcripts magnolia',\n",
       "  'call transcripts john',\n",
       "  'call transcripts janus',\n",
       "  'record live',\n",
       "  'record live blog',\n",
       "  'call transcripts houston',\n",
       "  'call transcripts gildan',\n",
       "  'call transcripts file',\n",
       "  'call transcripts paris',\n",
       "  'call transcripts ferro',\n",
       "  'call transcripts federal',\n",
       "  'call transcripts even',\n",
       "  'call transcripts ensign'],\n",
       " 'baidu': ['birdsong one china',\n",
       "  'call transcripts photo',\n",
       "  'threats qutoutiao',\n",
       "  'threats qutoutiao inc',\n",
       "  'lists one china',\n",
       "  'lists one',\n",
       "  'transcripts photo',\n",
       "  'transcripts photo courtesy',\n",
       "  'lovely birdsong one',\n",
       "  'term threats qutoutiao',\n",
       "  'stock lists one',\n",
       "  'birdsong one',\n",
       "  'performance based assets',\n",
       "  'period last year',\n",
       "  'perhaps maryland',\n",
       "  'perhaps maryland state',\n",
       "  'perfect description',\n",
       "  'perfect',\n",
       "  'percentage revenues stood',\n",
       "  'period last'],\n",
       " 'bed bath & beyond': ['ayanti bera updates',\n",
       "  'bera updates intesa',\n",
       "  'bera updates',\n",
       "  '16',\n",
       "  'offer spanish',\n",
       "  'nivedita ayanti bera',\n",
       "  'non',\n",
       "  'non food',\n",
       "  'non food retailer',\n",
       "  'occidental',\n",
       "  'occidental petroleum',\n",
       "  'occidental petroleum reviewing',\n",
       "  'offer',\n",
       "  'offer rival',\n",
       "  'offer rival ubi',\n",
       "  'offer spanish telecoms',\n",
       "  'nivedita',\n",
       "  'oil',\n",
       "  'oil gas',\n",
       "  'oil gas producer'],\n",
       " 'ca technologies': ['hobbies amazon nasdaq',\n",
       "  'transcripts alphabet nasdaq',\n",
       "  'technologist washington',\n",
       "  'technologist washington ap',\n",
       "  'personal vendetta one',\n",
       "  'mean growth total',\n",
       "  'last decade international',\n",
       "  'justify spend new',\n",
       "  'transcripts alphabet',\n",
       "  'lightly dividend',\n",
       "  'early afternoon as',\n",
       "  'lightly dividend stocks',\n",
       "  'olsavsky said shopify',\n",
       "  'olsavsky said reuters',\n",
       "  'hobbies amazon',\n",
       "  'stock lists one',\n",
       "  'stock lists computing',\n",
       "  'roy bengaluru the',\n",
       "  'decade international business',\n",
       "  'chinese year another'],\n",
       " 'celgene': ['trading reuters',\n",
       "  'hours trading reuters',\n",
       "  'trading reuters amgen',\n",
       "  'trading it scary',\n",
       "  'trading it',\n",
       "  'haskett reuters hedge',\n",
       "  'haskett reuters amgen',\n",
       "  'haskett reuters',\n",
       "  'hours trading it',\n",
       "  'gordon haskett reuters',\n",
       "  'little pandemic',\n",
       "  'like mckesson may',\n",
       "  'little pandemic looks',\n",
       "  'long term',\n",
       "  'long term growth',\n",
       "  'long term investors',\n",
       "  'little',\n",
       "  'long yield',\n",
       "  'likely thrive despite',\n",
       "  'likely thrive'],\n",
       " 'cerner': ['said new york',\n",
       "  'bach said eight',\n",
       "  'bach said new',\n",
       "  'said new',\n",
       "  'said eight million',\n",
       "  'said eight',\n",
       "  '000',\n",
       "  'stay home end',\n",
       "  'intensive care die',\n",
       "  'interactions',\n",
       "  'interactions subject',\n",
       "  'interactions subject modelling',\n",
       "  'stay',\n",
       "  'stay home',\n",
       "  'intensive',\n",
       "  'isolation',\n",
       "  'isolation policies',\n",
       "  'isolation policies low',\n",
       "  'status worst hit',\n",
       "  'status worst'],\n",
       " 'cisco ': ['steveagardner johan',\n",
       "  'pandemic laid',\n",
       "  'coverings the latest',\n",
       "  'creating change laid',\n",
       "  'creating change madrid',\n",
       "  'creating change sports',\n",
       "  'pandemic richard sherman',\n",
       "  'pandemic richard',\n",
       "  'pandemic laid workers',\n",
       "  'creditform george',\n",
       "  'past new',\n",
       "  'creditform george floyd',\n",
       "  'creditform want',\n",
       "  'creditform want money',\n",
       "  'fine san francisco',\n",
       "  'fine san',\n",
       "  'industry an',\n",
       "  'industry an indiana',\n",
       "  'coverings the',\n",
       "  'past new york'],\n",
       " 'cognizant': ['said mongodb inc',\n",
       "  'prosecutors said mongodb',\n",
       "  'transcripts boston',\n",
       "  'transcripts boston reuters',\n",
       "  'said boston',\n",
       "  'said boston reuters',\n",
       "  'prosecutors said boston',\n",
       "  'call transcripts boston',\n",
       "  'said mongodb',\n",
       "  'plans update',\n",
       "  'plans think actually',\n",
       "  'plans go',\n",
       "  'plans given everything',\n",
       "  'plans given',\n",
       "  'plans address constraints',\n",
       "  'plans address',\n",
       "  'platform adopted bottom',\n",
       "  'planning work various',\n",
       "  'planning work',\n",
       "  'planning pulling forward'],\n",
       " 'comcast': ['service nbc',\n",
       "  'founder reuters asapp',\n",
       "  'llc madrid london',\n",
       "  'llc report',\n",
       "  'fried rice work',\n",
       "  'fried rice toronto',\n",
       "  'llc report state',\n",
       "  'llc toronto',\n",
       "  'llc toronto reuters',\n",
       "  'co founder reuters',\n",
       "  'co founder ashen',\n",
       "  'twitter work home',\n",
       "  'bts ee madrid',\n",
       "  'bts ee milan',\n",
       "  'desired the typical',\n",
       "  'desired the',\n",
       "  'lot desired the',\n",
       "  'twitter work',\n",
       "  'founder reuters',\n",
       "  'health market ottawa'],\n",
       " 'discovery communications': ['llc disney owned',\n",
       "  'films genres disney',\n",
       "  'genres keanu',\n",
       "  'genres keanu reeves',\n",
       "  'times llc disney',\n",
       "  '21 release keanu',\n",
       "  'films genres keanu',\n",
       "  'genres disney owned',\n",
       "  'llc disney',\n",
       "  'release keanu',\n",
       "  'release keanu reeves',\n",
       "  'genres disney',\n",
       "  'company',\n",
       "  'company 2014',\n",
       "  'hariton said structural',\n",
       "  'result',\n",
       "  'represented 2019 pay',\n",
       "  'represented 2019',\n",
       "  'represented',\n",
       "  'sign'],\n",
       " 'dish network': ['engineers phds reuters',\n",
       "  'week reuters asapp',\n",
       "  'founder reuters asapp',\n",
       "  'founder reuters',\n",
       "  'dangers loom in',\n",
       "  'week reuters',\n",
       "  'phds reuters',\n",
       "  'phds reuters asapp',\n",
       "  'next week reuters',\n",
       "  'loom in',\n",
       "  'loom in episode',\n",
       "  'co founder reuters',\n",
       "  'pretty impressive',\n",
       "  'pretty good',\n",
       "  'pretty cool company',\n",
       "  'pretty cool',\n",
       "  'pretty good intellectual',\n",
       "  '00',\n",
       "  'pretty broad',\n",
       "  'pretty impressive gross'],\n",
       " 'ebay': ['haskett reuters hedge',\n",
       "  'insider timely getting',\n",
       "  'llc reuters',\n",
       "  'jenniferjolly unemployed coronavirus',\n",
       "  'jenniferjolly unemployed',\n",
       "  'jenniferjolly seoul reuters',\n",
       "  'jenniferjolly seoul',\n",
       "  'chizu nomiyama repeats',\n",
       "  'chizu nomiyama washington',\n",
       "  'pandemic washington',\n",
       "  'pandemic washington ap',\n",
       "  'crisis canadian commerce',\n",
       "  'crisis canadian',\n",
       "  'spring amazon',\n",
       "  'spring amazon nasdaq',\n",
       "  'spring seoul',\n",
       "  'spring seoul reuters',\n",
       "  'momentum repeats',\n",
       "  'momentum repeats earlier',\n",
       "  'program left workers'],\n",
       " 'electronic arts': ['contract 2020 madrid',\n",
       "  'today auto racing',\n",
       "  'rousey wrestlemania by',\n",
       "  'rousey wrestlemania new',\n",
       "  'reprint permission check',\n",
       "  'curtis blaydes analysis',\n",
       "  'curtis blaydes by',\n",
       "  'today auto',\n",
       "  'name opposition minneapolis',\n",
       "  'county register remember',\n",
       "  'llc check',\n",
       "  'llc check photos',\n",
       "  'law nhl',\n",
       "  'world in',\n",
       "  'world in ca',\n",
       "  'llc four',\n",
       "  'way by mma',\n",
       "  'way by',\n",
       "  'llc four death',\n",
       "  'llc madrid'],\n",
       " 'equinix': ['chinese year oakland',\n",
       "  'society washington reuters',\n",
       "  'tdavisdmr their stores',\n",
       "  'tdavisdmr equity bancshares',\n",
       "  'years dating new',\n",
       "  'twitter tdavisdmr equity',\n",
       "  'tesla inc cape',\n",
       "  'times llc louisville',\n",
       "  'times llc the',\n",
       "  'inc cape canaveral',\n",
       "  'permission washington reuters',\n",
       "  'permission washington',\n",
       "  'times llc washington',\n",
       "  'inc cape',\n",
       "  'plans cape',\n",
       "  'take cape',\n",
       "  'take cape canaveral',\n",
       "  'plans cape canaveral',\n",
       "  'said new',\n",
       "  'tdavisdmr their'],\n",
       " 'facebook': ['mikesisak washington',\n",
       "  'press reuters 500',\n",
       "  'media unacceptable coleen',\n",
       "  'operations stand',\n",
       "  'operations stand comedian',\n",
       "  'press minneapolis',\n",
       "  'press minneapolis mayor',\n",
       "  'one surfaces an',\n",
       "  'press police',\n",
       "  'press police across',\n",
       "  'press police dash',\n",
       "  'press protest',\n",
       "  'press protest chaos',\n",
       "  'officials said the',\n",
       "  'press reuters',\n",
       "  'times llc file',\n",
       "  'press where',\n",
       "  'times llc fire',\n",
       "  'press staggering',\n",
       "  'press staggering 20'],\n",
       " 'alphabet': ['week chamblon switzerland',\n",
       "  'fine washington reuters',\n",
       "  'meetings some crossfit',\n",
       "  'meetings some',\n",
       "  'visit profile timotheus',\n",
       "  'fine washington',\n",
       "  'would monetized joe',\n",
       "  'would monetized washington',\n",
       "  'permission washington reuters',\n",
       "  'meetings story corrects',\n",
       "  'permission washington',\n",
       "  'permission the video',\n",
       "  'permission the',\n",
       "  'results london reuters',\n",
       "  'results london',\n",
       "  'permission sen ted',\n",
       "  'meetings story',\n",
       "  'company april',\n",
       "  'permission peter lynch',\n",
       "  '3400 baht bangkok'],\n",
       " 'intel': ['haunss bogota',\n",
       "  'years hong kong',\n",
       "  'come advertisement father',\n",
       "  'co founder broadcom',\n",
       "  'race happy birthday',\n",
       "  'race happy',\n",
       "  'race file photo',\n",
       "  'race file',\n",
       "  'doctor little',\n",
       "  'chinese market the',\n",
       "  'doctor little known',\n",
       "  'group kabul jalalabad',\n",
       "  'progress said reuters',\n",
       "  'progress said hartford',\n",
       "  'month los angeles',\n",
       "  'month los',\n",
       "  'group kabul',\n",
       "  'whatsoever new york',\n",
       "  'come bolton',\n",
       "  'come bolton says'],\n",
       " 'liberty global': ['transcripts madrid',\n",
       "  'call transcripts madrid',\n",
       "  'bts ee madrid',\n",
       "  'ee madrid london',\n",
       "  'ee madrid',\n",
       "  'transcripts madrid london',\n",
       "  '00',\n",
       "  'process well',\n",
       "  'process pushing decision',\n",
       "  'process pushing',\n",
       "  'process needed optimistic',\n",
       "  'processes',\n",
       "  'process needed',\n",
       "  'process jason meggs',\n",
       "  'process jason',\n",
       "  'process holding alistair',\n",
       "  'process holding',\n",
       "  'process well established',\n",
       "  'processes forward think',\n",
       "  'processes forward'],\n",
       " 'liberty interactive': ['transcripts reuters warren',\n",
       "  'american director reuters',\n",
       "  'transcripts caleres',\n",
       "  'transcripts as hertz',\n",
       "  'transcripts as',\n",
       "  'transcripts artisan partners',\n",
       "  'transcripts artisan',\n",
       "  'transcripts antero midstream',\n",
       "  'transcripts antero',\n",
       "  'transcripts american financial',\n",
       "  'transcripts american',\n",
       "  'becomes involved file',\n",
       "  'becomes involved workday',\n",
       "  'transcripts amarin corporation',\n",
       "  'transcripts amarin',\n",
       "  'transcripts altra industrial',\n",
       "  'transcripts altra',\n",
       "  'dsilva live',\n",
       "  'dsilva live blog',\n",
       "  'dsilva reuters'],\n",
       " 'marriott international': ['said salyersville',\n",
       "  'tomorrow salyersville',\n",
       "  'said salyersville kentucky',\n",
       "  'rowe said salyersville',\n",
       "  'rowe said new',\n",
       "  'amid coronavirus what',\n",
       "  'llc rebuilding america',\n",
       "  'llc rebuilding',\n",
       "  'amid coronavirus new',\n",
       "  'back tomorrow salyersville',\n",
       "  'rowe said bangkok',\n",
       "  'tomorrow salyersville kentucky',\n",
       "  'times llc rebuilding',\n",
       "  'coronavirus what',\n",
       "  'said new',\n",
       "  'coronavirus what happened',\n",
       "  'coronavirus new normal',\n",
       "  'said new normal',\n",
       "  'said bangkok ap',\n",
       "  'said bangkok'],\n",
       " 'mattel': ['together big launch',\n",
       "  'coming together big',\n",
       "  'together big',\n",
       "  '000',\n",
       "  'prices',\n",
       "  'price similarly',\n",
       "  'price similarly lego',\n",
       "  'price tag',\n",
       "  'price tag 19',\n",
       "  'prices year',\n",
       "  'price 50 finally',\n",
       "  'prices year making',\n",
       "  'products',\n",
       "  'products outdoor',\n",
       "  'products outdoor toys',\n",
       "  'programme',\n",
       "  'price 50 working',\n",
       "  'price 50',\n",
       "  'programme elicited mixed',\n",
       "  'price 25 recent'],\n",
       " 'mckesson ': ['category it scary',\n",
       "  'euros paris brussels',\n",
       "  'transcripts paris',\n",
       "  '8886 euros paris',\n",
       "  '8886 euros the',\n",
       "  'promising pipeline the',\n",
       "  'transcripts paris brussels',\n",
       "  'euros paris',\n",
       "  'euros the',\n",
       "  'pipeline the',\n",
       "  'euros the covid',\n",
       "  'risk how lockdown',\n",
       "  'bengaluru adds',\n",
       "  'risk how',\n",
       "  'candidate account karachi',\n",
       "  'use hairstylist revealed',\n",
       "  'transcripts osmotica pharmaceuticals',\n",
       "  'transcripts osmotica',\n",
       "  'list reuters hedge',\n",
       "  'list reuters'],\n",
       " 'mckesson': ['000',\n",
       "  'payout ratio healthy',\n",
       "  'poised',\n",
       "  'pipeline empliciti used',\n",
       "  'pipeline empliciti',\n",
       "  'pipeline company biggest',\n",
       "  'pipeline company',\n",
       "  'pipeline',\n",
       "  'pharmaceutical vendor department',\n",
       "  'pharmaceutical vendor',\n",
       "  'pharmaceutical division overall',\n",
       "  'pharmaceutical division',\n",
       "  'pharmaceutical company profit',\n",
       "  'pharmaceutical company',\n",
       "  'pharmaceutical',\n",
       "  'per share bristol',\n",
       "  'per share',\n",
       "  'poised keep',\n",
       "  'poised keep climbing',\n",
       "  'portfolio'],\n",
       " 'microsoft': ['trek pop',\n",
       "  'gov black',\n",
       "  'charged reuters',\n",
       "  'edu want classical',\n",
       "  'edu want',\n",
       "  'edu dems 3t',\n",
       "  'edu dems',\n",
       "  'put bind mgm',\n",
       "  'put bind reuters',\n",
       "  'put bind sign',\n",
       "  'last xpress',\n",
       "  'last xpress enterprises',\n",
       "  'charged minneapolis reuters',\n",
       "  'charged minneapolis',\n",
       "  'referrals inevitable advertisement',\n",
       "  'adding want peter',\n",
       "  'killed what happened',\n",
       "  'charged reuters 500',\n",
       "  'investigation ongoing reuters',\n",
       "  '15 canceled there'],\n",
       " 'netflix': ['pullback american',\n",
       "  'transcripts washington reuters',\n",
       "  'star trek washington',\n",
       "  '3400 baht bangkok',\n",
       "  '3400 baht london',\n",
       "  'child nbc',\n",
       "  'child nbc renews',\n",
       "  'child tencent',\n",
       "  'child tencent otc',\n",
       "  'transcripts reuters',\n",
       "  'transcripts reuters 500',\n",
       "  'transcripts washington',\n",
       "  'understandingtheoutbreak amazon expects',\n",
       "  'understandingtheoutbreak amazon',\n",
       "  'mikesnider in episode',\n",
       "  'alive release the',\n",
       "  'mikesnider in',\n",
       "  'late may staying',\n",
       "  'late may reuters',\n",
       "  'next week while'],\n",
       " 'nvidia': ['llc harrisburg',\n",
       "  'transcripts hayneville ala',\n",
       "  'happen preferred apartment',\n",
       "  'happen reuters economy',\n",
       "  'reprint permission advertisement',\n",
       "  'business george',\n",
       "  'business george floyd',\n",
       "  'business kansas',\n",
       "  'business kansas city',\n",
       "  'going happen reuters',\n",
       "  'transcripts hayneville',\n",
       "  'aspects business george',\n",
       "  'treasures senate reuters',\n",
       "  'going happen preferred',\n",
       "  'good us veteran',\n",
       "  'home washington ap',\n",
       "  'senate reuters',\n",
       "  'senate reuters economy',\n",
       "  'amsterdam home washington',\n",
       "  'home washington'],\n",
       " 'paypal': ['tesla inc cape',\n",
       "  'explosion in yet',\n",
       "  'explosion nasa chief',\n",
       "  'tesla inc story',\n",
       "  'pros in',\n",
       "  'jeffersongraham twitter analysis',\n",
       "  'twitter linkedin cape',\n",
       "  'jeffersongraham twitter work',\n",
       "  'gravesham kent reuters',\n",
       "  'county register cape',\n",
       "  'twitter analysis opinion',\n",
       "  'twitter analysis',\n",
       "  'meetings story corrects',\n",
       "  'meetings story',\n",
       "  'inc story corrects',\n",
       "  'inc story',\n",
       "  'meetings fremont california',\n",
       "  'meetings fremont',\n",
       "  'texas washington reuters',\n",
       "  'texas washington'],\n",
       " 'qualcomm': ['matter reuters',\n",
       "  'familiar matter chinese',\n",
       "  'familiar matter reuters',\n",
       "  'matter chinese',\n",
       "  'matter chinese tech',\n",
       "  'matter reuters united',\n",
       "  'mobile chipmaker',\n",
       "  'much 75 billion',\n",
       "  'much 75',\n",
       "  'mu qualcomm nasdaq',\n",
       "  'mu qualcomm',\n",
       "  'mu',\n",
       "  'mobile socs system',\n",
       "  'mobile socs select',\n",
       "  'mobile socs',\n",
       "  'mobile chipmaker generates',\n",
       "  '12',\n",
       "  'mobile',\n",
       "  'nand chips',\n",
       "  'million anti competitive'],\n",
       " 'starbucks': ['kristakafer tech',\n",
       "  'press ariana grande',\n",
       "  'setback hong',\n",
       "  'declined comment george',\n",
       "  'setback hong kong',\n",
       "  'profile melania',\n",
       "  'profile melania trump',\n",
       "  'say goodbye nutritionist',\n",
       "  'point safe life',\n",
       "  'next week michael',\n",
       "  'gauthier las',\n",
       "  'gauthier las vegas',\n",
       "  'change time going',\n",
       "  'change time life',\n",
       "  'coral murphy the',\n",
       "  'change time the',\n",
       "  'change time things',\n",
       "  'gauthier paris',\n",
       "  'gauthier paris reuters',\n",
       "  'purchasing stock london'],\n",
       " 'stericycle': ['transcripts magnolia oil',\n",
       "  'transcripts vishay intertechnology',\n",
       "  '02 reuters boeing',\n",
       "  '02 reuters',\n",
       "  '02 repeats story',\n",
       "  '02 repeats',\n",
       "  '02 amarin',\n",
       "  'market crash tilray',\n",
       "  'week october file',\n",
       "  'week october canadian',\n",
       "  'york county new',\n",
       "  'york county textron',\n",
       "  'transcripts xpress enterprises',\n",
       "  'transcripts xpress',\n",
       "  'haunss new',\n",
       "  'haunss new york',\n",
       "  'transcripts workday inc',\n",
       "  'haskett reuters hedge',\n",
       "  'haskett reuters',\n",
       "  'haskett artisan partners'],\n",
       " 'tesla motors': ['low paris reuters',\n",
       "  'coronavirus outbreak nikola',\n",
       "  'transcripts prague warsaw',\n",
       "  'transcripts prague',\n",
       "  'transcripts how network',\n",
       "  'transcripts how',\n",
       "  'transcripts cape canaveral',\n",
       "  'transcripts cape',\n",
       "  'transcripts berlin frankfurt',\n",
       "  'transcripts berlin',\n",
       "  'transcripts bangkok ap',\n",
       "  'transcripts bangkok',\n",
       "  'call transcripts bangkok',\n",
       "  'call transcripts berlin',\n",
       "  'call transcripts cape',\n",
       "  'call transcripts how',\n",
       "  'call transcripts prague',\n",
       "  'call transcripts reuters',\n",
       "  'call transcripts story',\n",
       "  'call transcripts xpress'],\n",
       " 'texas instruments': ['questions eight',\n",
       "  'ask questions eight',\n",
       "  'ask questions what',\n",
       "  'questions eight million',\n",
       "  'questions what',\n",
       "  'questions what get',\n",
       "  'disease 10 per',\n",
       "  'see uk death',\n",
       "  'see uk',\n",
       "  'see effect numbers',\n",
       "  'see effect',\n",
       "  'discussing',\n",
       "  'discussing going',\n",
       "  'discussing going back',\n",
       "  'disease 10',\n",
       "  'disease copd',\n",
       "  'differently mitigation',\n",
       "  'disease copd six',\n",
       "  'disease diabetes',\n",
       "  'disease diabetes could'],\n",
       " 'universal display ': ['hoped recent',\n",
       "  'texas tv special',\n",
       "  'llc boris',\n",
       "  'child dead by',\n",
       "  'pores prince harry',\n",
       "  'pores prince',\n",
       "  'special status reuters',\n",
       "  'transcripts secretary',\n",
       "  'transcripts secretary state',\n",
       "  'transcripts we',\n",
       "  'transcripts we chef',\n",
       "  'year mongodb inc',\n",
       "  'year mongodb',\n",
       "  'hoped recent editorials',\n",
       "  'texas tv',\n",
       "  'stats corpses analysis',\n",
       "  'texas reuters cheniere',\n",
       "  'texas reuters',\n",
       "  'permission year ago',\n",
       "  'permission year'],\n",
       " 'universal display': ['10',\n",
       "  'progress',\n",
       "  'production progress',\n",
       "  'production',\n",
       "  'process sorts semiconductors',\n",
       "  'process sorts',\n",
       "  'process continue come',\n",
       "  'process continue',\n",
       "  'process',\n",
       "  'prices sit 28',\n",
       "  'prices sit',\n",
       "  'prices regret decision',\n",
       "  'prices regret',\n",
       "  'prices',\n",
       "  'price point oled',\n",
       "  'price point',\n",
       "  'price',\n",
       "  'pressures important smartphone',\n",
       "  'pressures important',\n",
       "  'pressures']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dict_relavant_words = {}\n",
    "for company in companies_w_articles: # for all companies in companies_w_articles\n",
    "\n",
    "    tfidf_vectorizer=TfidfVectorizer(stop_words = {'english'},ngram_range = (1,3), binary = True)\n",
    "    plain_text_list = list()\n",
    "    company_article = \"\"\n",
    "    for index, row in df_cleaned.iterrows():\n",
    "        if company in row[\"label\"]:\n",
    "            company_article = company_article+ \" \"+ row[\"plain_text\"]\n",
    "            plain_text_list.append(row[\"plain_text\"])\n",
    "    \n",
    "    plain_text_list.insert(0,company_article)\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(plain_text_list)\n",
    "\n",
    "    #Get the tf-idf scores for the words in the company article complication.\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    "\n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df = df.sort_values(by=[\"tfidf\"],ascending=False).head(20)\n",
    "    dict_relavant_words[company] = list(df.index)\n",
    "dict_relavant_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python program to generate word vectors using Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "#nltk.download('punkt')\n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'australia' and 'melbourne' - CBOW :  0.66155946\n",
      "[('portland', 0.8813670873641968), ('perez', 0.8812724351882935), ('rigel', 0.8715772032737732), ('heights', 0.8652781844139099), ('jaylen', 0.8628085255622864), ('pool', 0.8600395917892456), ('santa', 0.8597794771194458), ('cincinnati', 0.8594452738761902), ('hollywood', 0.8580102920532227), ('charleston', 0.8569580316543579)]\n",
      "Cosine similarity between 'australia' and 'melbourne' - Skip Gram :  0.5816691\n",
      "[('hollywood', 0.8261724710464478), ('capitan', 0.8180453181266785), ('dga', 0.8162583112716675), ('monica', 0.8151893615722656), ('gods', 0.809572696685791), ('augusta', 0.805156946182251), ('sands', 0.8041570782661438), ('citywest', 0.8036929368972778), ('lutheran', 0.8020343780517578), ('hangar', 0.800697922706604)]\n"
     ]
    }
   ],
   "source": [
    "# Apply 2 Word2Vec models to articles   \n",
    "\n",
    "data = [] \n",
    "  \n",
    "# iterate through each article in the file \n",
    "for i in clean_articles: \n",
    "    temp = [] \n",
    "    # tokenize the article into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "\n",
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'australia' \" + \n",
    "               \"and 'melbourne' - CBOW : \", \n",
    "    model1.similarity('melbourne', 'australia')) \n",
    "\n",
    "print(model1.wv.most_similar('melbourne'))\n",
    "    \n",
    "\n",
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 4) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'australia' \" +\n",
    "          \"and 'melbourne' - Skip Gram : \", \n",
    "    model2.similarity('melbourne', 'australia')) \n",
    "print(model2.wv.most_similar('melbourne'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliminated\n",
      "masterchef\n",
      "contestant\n",
      "harry\n",
      "foster\n"
     ]
    }
   ],
   "source": [
    "# FOR GENSIN USING CBOW Manipulations\n",
    "\n",
    "# enumerate data it is trained on\n",
    "for i, word in enumerate(model1.wv.vocab):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84611"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "# Show frequencies\n",
    "#print(\"Original List : \",data)\n",
    "data_flat = []\n",
    "for line in data:\n",
    "    for word in line:\n",
    "        data_flat.append(word)\n",
    "\n",
    "\n",
    "ctr = collections.Counter(data_flat)\n",
    "#print(\"Frequency of the elements in the List : \",ctr)\n",
    "ctr[\"the\"] # count of word \"the\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.itf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 29466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-125a418885c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print (tf_idf_vector.todense().sum())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#print (tf_idf_vector.T.todense())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tfidf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mdf_word_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tfidf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mword_importance_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_word_importance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m         return [t for t, i in sorted(self.vocabulary_.items(),\n\u001b[0;32m-> 1298\u001b[0;31m                                      key=itemgetter(1))]\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         return [t for t, i in sorted(self.vocabulary_.items(),\n\u001b[0m\u001b[1;32m   1298\u001b[0m                                      key=itemgetter(1))]\n\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Vectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X = vectorizer.fit_transform(clean_articles)\n",
    "\n",
    "#print(vectorizer.get_feature_names()[:10])\n",
    "#print(X.shape)\n",
    "#print(vectorizer.get_stop_words())\n",
    "#print(vectorizer.get_params(deep=True))\n",
    "\n",
    "n_articles, n_distinct_words = X.shape\n",
    "print(n_articles, n_distinct_words)\n",
    "\n",
    "collect_word_importance = []\n",
    "#place tf-idf values in a pandas data frame \n",
    "for tf_idf_vector_id in range(n_articles):\n",
    "    \n",
    "    tf_idf_vector=X[tf_idf_vector_id]\n",
    "    #print (tf_idf_vector.todense().sum())\n",
    "    #print (tf_idf_vector.T.todense())\n",
    "    df = pd.DataFrame(tf_idf_vector.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df_word_importance = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    word_importance_list = np.array(df_word_importance.index)\n",
    "    collect_word_importance.append(word_importance_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['melissa', 'masterchef', 'leong', ..., 'findlay', 'findings',\n",
       "        'zuocheng'],\n",
       "       ['the', 'burglary', 'bail', ..., 'firmware', 'firms', 'zuocheng'],\n",
       "       ['the', 'to', 'children', ..., 'flaring', 'flareups', 'zuocheng'],\n",
       "       ...,\n",
       "       ['the', 'to', 'in', ..., 'firestorm', 'fires', 'zuocheng'],\n",
       "       ['the', 'to', 'in', ..., 'firestorm', 'fires', 'zuocheng'],\n",
       "       ['the', 'to', 'in', ..., 'firestorm', 'fires', 'zuocheng']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each line corresponds to the highest scored words in the article of same index.\n",
    "collect_word_importance = np.array(collect_word_importance)\n",
    "collect_word_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfTransformer\n",
    "#TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]\n",
    "\n",
    "\n",
    "corpus = ['this is the first document',\n",
    "           'this document is the second document',\n",
    "          'and this is the third one',\n",
    "           'is this the first document']\n",
    "vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n",
    "               'and', 'one']\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()\n",
    "pipe['tfid'].idf_\n",
    "pipe.transform(corpus).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.21510797 7.90825515 7.90825515 ... 7.90825515 6.99196442 7.21510797]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 29466)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('count', CountVectorizer()),\n",
    "                  ('tfid', TfidfTransformer())]).fit(clean_articles)\n",
    "pipe['count'].transform(clean_articles).toarray().shape\n",
    "print (pipe['tfid'].idf_)\n",
    "Tfidf_res = pipe.transform(clean_articles)\n",
    "Tfidf_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x29466 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 666150 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tfidf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tutorial\n",
    "\n",
    "#Dataset and Imports\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    " \n",
    "# this is a very toy example, do not try this at home unless you want to understand the usage differences \n",
    "docs=[\"the house had a tiny little mouse\", \n",
    "\"the cat saw the mouse\", \n",
    "\"the mouse ran away from the house\", \n",
    "\"the cat finally ate the mouse\", \n",
    "\"the end of the mouse story\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize CountVectorizer\n",
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer() \n",
    "# this steps generates word counts for the words in your docs \n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector.shape\n",
    "# 5 texts, 9 distinct words -> gives the count for each word in each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x16 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute the IDF values\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         idf_weights\n",
       "mouse       1.000000\n",
       "the         1.000000\n",
       "cat         1.693147\n",
       "house       1.693147\n",
       "ate         2.098612\n",
       "away        2.098612\n",
       "end         2.098612\n",
       "finally     2.098612\n",
       "from        2.098612\n",
       "had         2.098612\n",
       "little      2.098612\n",
       "of          2.098612\n",
       "ran         2.098612\n",
       "saw         2.098612\n",
       "story       2.098612\n",
       "tiny        2.098612"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the TFIDF score for your documents\n",
    "# count matrix \n",
    "count_vector=cv.transform(docs) #<==> word_count_vector\n",
    "\n",
    "# tf-idf scores \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x16 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.398203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfidf\n",
       "had      0.493562\n",
       "little   0.493562\n",
       "tiny     0.493562\n",
       "house    0.398203\n",
       "mouse    0.235185\n",
       "the      0.235185\n",
       "ate      0.000000\n",
       "away     0.000000\n",
       "cat      0.000000\n",
       "end      0.000000\n",
       "finally  0.000000\n",
       "from     0.000000\n",
       "of       0.000000\n",
       "ran      0.000000\n",
       "saw      0.000000\n",
       "story    0.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names() \n",
    " \n",
    "#get tfidf vector for FFFFFFFFFirst document \n",
    "first_document_vector=tf_idf_vector[0] \n",
    " \n",
    "#print the scores (Tf-idf scores of first document)\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidfvectorizer Usage - Compute all at Once\n",
    "\n",
    " \n",
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    "#fitted_vectorizer=tfidf_vectorizer.fit(docs)               # This method would work too\n",
    "#tfidf_vectorizer_vectors=fitted_vectorizer.transform(docs)  \n",
    " \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.398203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfidf\n",
       "had      0.493562\n",
       "little   0.493562\n",
       "tiny     0.493562\n",
       "house    0.398203\n",
       "mouse    0.235185\n",
       "the      0.235185\n",
       "ate      0.000000\n",
       "away     0.000000\n",
       "cat      0.000000\n",
       "end      0.000000\n",
       "finally  0.000000\n",
       "from     0.000000\n",
       "of       0.000000\n",
       "ran      0.000000\n",
       "saw      0.000000\n",
       "story    0.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
