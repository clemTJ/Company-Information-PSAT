{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Labeling & Lexical Fields Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Unlabelled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "raw_json_data = []\n",
    "with open('./data/20200420_20200714_business_articles.json') as f:\n",
    "    for line in f:\n",
    "        raw_json_data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type <class 'list'>\n",
      "json <class 'dict'>\n",
      "keys dict_keys(['published', 'link', 'message', 'Feed', 'title', '@version', 'author', '@timestamp', 'full-text', 'type'])\n",
      "length 416307\n"
     ]
    }
   ],
   "source": [
    "print (\"data type\",type (raw_json_data))\n",
    "print (\"json\",type (raw_json_data[0]))\n",
    "print (\"keys\",raw_json_data[0].keys())\n",
    "print (\"length\", len(raw_json_data))\n",
    "#print (raw_json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Company Names & Related Names(52 companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['21st century fox', 'activision blizzard', 'adobe ', 'advanced micro devices', 'akamai technologies', 'akamai tecnologies', 'alexion pharmaceuticals', 'amazon', 'american airlines group', 'amgen', 'analog devices', 'apple', 'autodesk', 'automatic data processing', 'baidu', 'bed bath & beyond', 'biogen', 'ca technologies', 'celgene', 'cerner', 'cisco ', 'cognizant', 'comcast', 'discovery communications', 'dish network', 'ebay', 'electronic arts', 'equinix', 'expeditors international', 'facebook', 'alphabet', 'intel', 'liberty global', 'liberty interactive', 'linear technology', 'marriott international', 'mattle', 'mattel', 'mckesson ', 'mckesson', 'microsoft', 'netflix', 'nvidia', 'paypal', 'qualcomm', 'starbucks', 'stericycle', 'tesla motors', 'texas instruments', 'the priceline group', 'universal display ', 'universal display'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetching company names (52 companies)\n",
    "df = pd.read_excel (r'./data/comapny_name-related_words.xlsx', header = None)\n",
    "# Lower\n",
    "df[0] = [row[0].lower() for index, row in df.iterrows()] \n",
    "# Split company name and related names\n",
    "split = np.array([row[0].split(\";\") for index, row in df.iterrows()])\n",
    "df[\"company_name\"] = split[:,0]\n",
    "df[\"related_name\"] = split[:,1]\n",
    "df.drop(columns = [0], inplace=True)\n",
    "# build dictionary of related name of companies\n",
    "dict_companies = {}\n",
    "company_names = df[\"company_name\"].unique()\n",
    "for name in company_names:\n",
    "    df_tmp = df[df[\"company_name\"] == name]\n",
    "    dict_companies[name] = list(df_tmp[\"related_name\"])\n",
    "dict_companies.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting url, title & full_text of each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list()\n",
    "plain_texts = list()\n",
    "titles = list()\n",
    "labels = list()\n",
    "\n",
    "min_article_size = 2000\n",
    "for article in raw_json_data:\n",
    "    plain_text = article.get('full-text')\n",
    "    title = article.get('title')\n",
    "    url = article.get('link')\n",
    "    if (plain_text and \"Article `download()` failed\" != plain_text[:27] and \"Please enable cookies\" != plain_text[:21] and len(plain_text)>min_article_size):\n",
    "        plain_texts.append(plain_text)\n",
    "        urls.append(url)\n",
    "        titles.append(title)\n",
    "        labels.append(list())\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataFrame with extacted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#Statistics\n",
    "# 358192 removing \"Article `download()` failed\" \n",
    "# 340987 removing \"Article `download()` failed\" and \"Please enable cookies\"\n",
    "# 215039 removing \"Article `download()` failed\" and \"Please enable cookies\" and size<min_article_size = 2000\n",
    "data = np.array([urls,titles, plain_texts, labels]).T\n",
    "columns=[\"url\", \"title\", \"plain_text\", \"label\"]\n",
    "df_articles = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>plain_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.dailymail.co.uk/tvshowbiz/article-...</td>\n",
       "      <td>MasterChef's Harry Foster hits back at claims ...</td>\n",
       "      <td>Eliminated MasterChef contestant Harry Foster ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jun/...</td>\n",
       "      <td>Protest arrests logjam tests NYC legal system,...</td>\n",
       "      <td>NEW YORK (AP) - A wave of arrests in the New Y...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-83114...</td>\n",
       "      <td>Labour's Anneliese Dodds says she will REFUSE ...</td>\n",
       "      <td>A top shadow minister today said there was not...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://feeds.reuters.com/~r/Reuters/worldNews/...</td>\n",
       "      <td>Civil unrest rages in Minneapolis over raciall...</td>\n",
       "      <td>MINNEAPOLIS (Reuters) - Peaceful rallies gave ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-82734...</td>\n",
       "      <td>Australia 'beats the cr*p' out of coronavirus ...</td>\n",
       "      <td>Australia is 'beating the c**p' out of coronav...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.dailymail.co.uk/tvshowbiz/article-...   \n",
       "1  https://www.washingtontimes.com/news/2020/jun/...   \n",
       "2  https://www.dailymail.co.uk/news/article-83114...   \n",
       "3  http://feeds.reuters.com/~r/Reuters/worldNews/...   \n",
       "4  https://www.dailymail.co.uk/news/article-82734...   \n",
       "\n",
       "                                               title  \\\n",
       "0  MasterChef's Harry Foster hits back at claims ...   \n",
       "1  Protest arrests logjam tests NYC legal system,...   \n",
       "2  Labour's Anneliese Dodds says she will REFUSE ...   \n",
       "3  Civil unrest rages in Minneapolis over raciall...   \n",
       "4  Australia 'beats the cr*p' out of coronavirus ...   \n",
       "\n",
       "                                          plain_text label  \n",
       "0  Eliminated MasterChef contestant Harry Foster ...    []  \n",
       "1  NEW YORK (AP) - A wave of arrests in the New Y...    []  \n",
       "2  A top shadow minister today said there was not...    []  \n",
       "3  MINNEAPOLIS (Reuters) - Peaceful rallies gave ...    []  \n",
       "4  Australia is 'beating the c**p' out of coronav...    []  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning full_text of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliminated masterchef contestant harry foster has hit back at unfair criticism against judge melissa leong  the show s first female judge  40  has faced a barrage of trolling  with haters taking aim at everything from her behaviour on set to her fashion sense  despite being eliminated on tuesday night s episode  harry had nothing but good things to say about the melbourne based food writer   this could not be further from the truth   eliminated masterchef australia contestant harry foster  pictured  has hit back at unfair criticism against judge melissa leong  she s a queen  i love her   harry told huffpost australia   she is energetic  passionate and really just vibrant   when asked about accusations melissa was rude and biased on the show  he said   this could not be further from the truth   all three judges have received an overwhelmingly positive response from fans  but melissa has copped a backlash from a vocal minority   she s a queen   the show s first female judge  40  has faced a barrage of trolling  with haters taking aim at everything from her behaviour on set to her fashion sense while many have praised her for her fashion sense and positivity  others claim she waits for feedback from jock zonfrillo and andy allen before repeating it as her own   any chance melissa leong has an original idea on masterchef  or just continue to wait for others to tell her what to think about the dish   one viewer tweeted  another added   new judge melissa speaks like an affected melbourne millennial  and as an affected melbourne millennial myself  i am in a good position to note how grating that is   mixed  all three masterchef judges have received an overwhelmingly positive response from fans  but melissa has copped a backlash from a vocal minority kind words   she is energetic  passionate and really just vibrant   said harry  left   who was eliminated on tuesday night s episode  i love masterchef bringing back the old contestants but loathe the new judges  especially melissa leong  i ve only been watching for ten minutes and she s already irritating   a third fan wrote on twitter  others claimed she was biased towards certain contestants  such as hayden quinn and khanh ong  masterchef continues thursday at 7 30pm on channel 10 '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove every non-letter/number character\n",
    "#df_cleaned = df_articles.copy(deep= True)\n",
    "df_cleaned = df_articles.head(5000).copy(deep= True)\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    row[\"plain_text\"] = row[\"plain_text\"].lower()\n",
    "    row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "    #[:punct:], ,[^0-9], [^a-z]\n",
    "    #row[\"plain_text\"] = re.sub(\"[^a-z],[^:punct:],[^0-9]\", ' ', row[\"plain_text\"])\n",
    "    row[\"plain_text\"] = re.sub(\"[^a-z0-9]\", ' ', row[\"plain_text\"])\n",
    "    #row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "df_cleaned[\"plain_text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Stop Words & Removing them from plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliminated masterchef contestant harry foster hit back unfair criticism judge melissa leong show first female judge 40 faced barrage trolling haters taking aim everything behaviour set fashion sense despite eliminated tuesday night episode harry nothing good things say melbourne based food writer could truth eliminated masterchef australia contestant harry foster pictured hit back unfair criticism judge melissa leong queen love harry told huffpost australia energetic passionate really vibrant asked accusations melissa rude biased show said could truth three judges received overwhelmingly positive response fans melissa copped backlash vocal minority queen show first female judge 40 faced barrage trolling haters taking aim everything behaviour set fashion sense many praised fashion sense positivity others claim waits feedback jock zonfrillo andy allen repeating chance melissa leong original idea masterchef continue wait others tell think dish one viewer tweeted another added new judge melissa speaks like affected melbourne millennial affected melbourne millennial good position note grating mixed three masterchef judges received overwhelmingly positive response fans melissa copped backlash vocal minority kind words energetic passionate really vibrant said harry left eliminated tuesday night episode love masterchef bringing back old contestants loathe new judges especially melissa leong watching ten minutes already irritating third fan wrote twitter others claimed biased towards certain contestants hayden quinn khanh ong masterchef continues thursday 7 30pm channel 10 '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all stop words from plain text\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for stop_word in stop_words:\n",
    "        row[\"plain_text\"] = re.sub(' '+stop_word+' ', ' ', row[\"plain_text\"])\n",
    "    row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "df_cleaned[\"plain_text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Articles with Company Names \n",
    "### Check if Articles Talk of Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  []\n",
       "1                                  []\n",
       "2    [advanced micro devices, nvidia]\n",
       "3                                  []\n",
       "4                             [apple]\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in df_cleaned.iterrows(): # initialize labels\n",
    "    row['label'] = []\n",
    "    \n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for company in company_names:\n",
    "        if company in row[\"plain_text\"]:\n",
    "            row['label'].append(company)\n",
    "        else:\n",
    "            for related_name in dict_companies[company]:\n",
    "                if related_name in row[\"plain_text\"]:\n",
    "                    row['label'].append(company)\n",
    "                    break\n",
    "df_cleaned[\"label\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of articles with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2805 labeled articles in the 5000 articles of the corpus\n"
     ]
    }
   ],
   "source": [
    "labeled = 0\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    if row[\"label\"]:\n",
    "        labeled +=1\n",
    "print (\"There are %d labeled articles in the %d articles of the corpus\"%(labeled, len (df_cleaned[\"label\"])))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Number of Articles that each Company is Associated to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 46 companies with associated articles over the 52 total companies\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'21st century fox': 40,\n",
       " 'activision blizzard': 11,\n",
       " 'adobe ': 2,\n",
       " 'advanced micro devices': 68,\n",
       " 'akamai technologies': 1,\n",
       " 'akamai tecnologies': 25,\n",
       " 'alexion pharmaceuticals': 10,\n",
       " 'amazon': 283,\n",
       " 'american airlines group': 220,\n",
       " 'amgen': 26,\n",
       " 'analog devices': 0,\n",
       " 'apple': 503,\n",
       " 'autodesk': 783,\n",
       " 'automatic data processing': 124,\n",
       " 'baidu': 5,\n",
       " 'bed bath & beyond': 4,\n",
       " 'biogen': 0,\n",
       " 'ca technologies': 38,\n",
       " 'celgene': 13,\n",
       " 'cerner': 4,\n",
       " 'cisco ': 145,\n",
       " 'cognizant': 11,\n",
       " 'comcast': 60,\n",
       " 'discovery communications': 5,\n",
       " 'dish network': 8,\n",
       " 'ebay': 31,\n",
       " 'electronic arts': 30,\n",
       " 'equinix': 66,\n",
       " 'expeditors international': 0,\n",
       " 'facebook': 709,\n",
       " 'alphabet': 255,\n",
       " 'intel': 305,\n",
       " 'liberty global': 8,\n",
       " 'liberty interactive': 104,\n",
       " 'linear technology': 0,\n",
       " 'marriott international': 14,\n",
       " 'mattle': 0,\n",
       " 'mattel': 2,\n",
       " 'mckesson ': 38,\n",
       " 'mckesson': 1,\n",
       " 'microsoft': 728,\n",
       " 'netflix': 132,\n",
       " 'nvidia': 16,\n",
       " 'paypal': 75,\n",
       " 'qualcomm': 4,\n",
       " 'starbucks': 83,\n",
       " 'stericycle': 166,\n",
       " 'tesla motors': 143,\n",
       " 'texas instruments': 3,\n",
       " 'the priceline group': 0,\n",
       " 'universal display ': 37,\n",
       " 'universal display': 1}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init\n",
    "dict_count = {}\n",
    "for company in company_names: dict_count[company]= 0\n",
    "    \n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for company in company_names:\n",
    "        if company in row[\"label\"]:\n",
    "            dict_count[company]+=1\n",
    "dict_count          \n",
    "\n",
    "companies_w_articles = list()\n",
    "for company in company_names:\n",
    "    if dict_count[company]>0:\n",
    "        companies_w_articles.append(company)\n",
    "print (\"there are %d companies with associated articles over the %d total companies\"%(len(companies_w_articles),len(company_names)) )\n",
    "#dict_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.idf on Companies that have Associated Articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'21st century fox': ['said',\n",
       "  'police',\n",
       "  'sports',\n",
       "  'like',\n",
       "  'think',\n",
       "  'million',\n",
       "  'netflix',\n",
       "  'trump',\n",
       "  'people',\n",
       "  'mr',\n",
       "  'theaters',\n",
       "  'companies',\n",
       "  'going',\n",
       "  'tv',\n",
       "  'would',\n",
       "  'twitter',\n",
       "  'humphrey',\n",
       "  'hill',\n",
       "  'one',\n",
       "  'world'],\n",
       " 'activision blizzard': ['mall',\n",
       "  'impella',\n",
       "  'penney',\n",
       "  'center',\n",
       "  'patients',\n",
       "  'st',\n",
       "  'road',\n",
       "  'patient',\n",
       "  'stores',\n",
       "  'square',\n",
       "  'heart',\n",
       "  'year',\n",
       "  'closing',\n",
       "  'shopping',\n",
       "  'abiomed',\n",
       "  'covid',\n",
       "  'chief',\n",
       "  'paddy',\n",
       "  'drive',\n",
       "  'store'],\n",
       " 'adobe ': ['shopify',\n",
       "  'fastly',\n",
       "  'job',\n",
       "  'jobs',\n",
       "  'walmart',\n",
       "  'commerce',\n",
       "  'like',\n",
       "  'said',\n",
       "  'back',\n",
       "  'work',\n",
       "  'take',\n",
       "  'pandemic',\n",
       "  'unemployment',\n",
       "  'may',\n",
       "  'businesses',\n",
       "  'new',\n",
       "  'far',\n",
       "  'traffic',\n",
       "  'many',\n",
       "  'abbott'],\n",
       " 'advanced micro devices': ['quarter',\n",
       "  'officer',\n",
       "  'think',\n",
       "  'said',\n",
       "  'chief',\n",
       "  'analyst',\n",
       "  'business',\n",
       "  'year',\n",
       "  'million',\n",
       "  'executive',\n",
       "  'market',\n",
       "  'credit',\n",
       "  'first',\n",
       "  'question',\n",
       "  'covid',\n",
       "  'also',\n",
       "  'black',\n",
       "  'portfolio',\n",
       "  'time',\n",
       "  'well'],\n",
       " 'akamai technologies': ['shopify',\n",
       "  'fastly',\n",
       "  'walmart',\n",
       "  'commerce',\n",
       "  'like',\n",
       "  'customers',\n",
       "  'traffic',\n",
       "  'new',\n",
       "  '2020',\n",
       "  'could',\n",
       "  'deal',\n",
       "  'business',\n",
       "  'small',\n",
       "  'digital',\n",
       "  'nyse',\n",
       "  'revenue',\n",
       "  'via',\n",
       "  'q1',\n",
       "  'world',\n",
       "  'market'],\n",
       " 'akamai tecnologies': ['campaign',\n",
       "  'said',\n",
       "  'google',\n",
       "  'shopify',\n",
       "  'presidential',\n",
       "  'intelligence',\n",
       "  'digital',\n",
       "  'trump',\n",
       "  'targeted',\n",
       "  'group',\n",
       "  'inc',\n",
       "  'become',\n",
       "  'walmart',\n",
       "  'attempts',\n",
       "  'stock',\n",
       "  'hackers',\n",
       "  'reuters',\n",
       "  'accounts',\n",
       "  'companies',\n",
       "  'candidate'],\n",
       " 'alexion pharmaceuticals': ['said',\n",
       "  'elliott',\n",
       "  'alexion',\n",
       "  'company',\n",
       "  'share',\n",
       "  'price',\n",
       "  'companys',\n",
       "  'letter',\n",
       "  'one',\n",
       "  'investors',\n",
       "  'billion',\n",
       "  'alexions',\n",
       "  'board',\n",
       "  'drug',\n",
       "  'sale',\n",
       "  'deal',\n",
       "  'buy',\n",
       "  'industry',\n",
       "  'hedge',\n",
       "  'ltd'],\n",
       " 'amazon': ['said',\n",
       "  'quarter',\n",
       "  'new',\n",
       "  'trump',\n",
       "  'coronavirus',\n",
       "  'amazon',\n",
       "  'year',\n",
       "  'million',\n",
       "  'company',\n",
       "  'companies',\n",
       "  'people',\n",
       "  'twitter',\n",
       "  'billion',\n",
       "  'first',\n",
       "  'workers',\n",
       "  'would',\n",
       "  'think',\n",
       "  'president',\n",
       "  'one',\n",
       "  'like'],\n",
       " 'american airlines group': ['think',\n",
       "  'quarter',\n",
       "  'analyst',\n",
       "  'year',\n",
       "  'million',\n",
       "  'executive',\n",
       "  'chief',\n",
       "  'officer',\n",
       "  'president',\n",
       "  'us',\n",
       "  'going',\n",
       "  'thank',\n",
       "  'question',\n",
       "  'first',\n",
       "  'business',\n",
       "  'really',\n",
       "  'operator',\n",
       "  'well',\n",
       "  'see',\n",
       "  'customers'],\n",
       " 'amgen': ['china',\n",
       "  'biden',\n",
       "  'antibody',\n",
       "  'said',\n",
       "  'elliott',\n",
       "  'trump',\n",
       "  'antibodies',\n",
       "  'alexion',\n",
       "  'sullivan',\n",
       "  'plasma',\n",
       "  'company',\n",
       "  'patients',\n",
       "  'president',\n",
       "  '19',\n",
       "  'covid',\n",
       "  'share',\n",
       "  'billion',\n",
       "  'use',\n",
       "  'treatments',\n",
       "  'amgen'],\n",
       " 'apple': ['said',\n",
       "  'quarter',\n",
       "  'think',\n",
       "  'analyst',\n",
       "  'year',\n",
       "  'officer',\n",
       "  'chief',\n",
       "  'first',\n",
       "  'million',\n",
       "  'new',\n",
       "  'would',\n",
       "  'president',\n",
       "  'business',\n",
       "  'people',\n",
       "  'wells',\n",
       "  'going',\n",
       "  'one',\n",
       "  'trump',\n",
       "  'executive',\n",
       "  'us'],\n",
       " 'autodesk': ['said',\n",
       "  'police',\n",
       "  'quarter',\n",
       "  'think',\n",
       "  'officer',\n",
       "  'year',\n",
       "  'analyst',\n",
       "  'million',\n",
       "  'people',\n",
       "  'may',\n",
       "  'chief',\n",
       "  'would',\n",
       "  'first',\n",
       "  'president',\n",
       "  'new',\n",
       "  'business',\n",
       "  'one',\n",
       "  'executive',\n",
       "  'also',\n",
       "  'state'],\n",
       " 'automatic data processing': ['quarter',\n",
       "  'think',\n",
       "  'analyst',\n",
       "  'million',\n",
       "  'year',\n",
       "  'officer',\n",
       "  'chief',\n",
       "  'president',\n",
       "  'going',\n",
       "  'first',\n",
       "  'us',\n",
       "  'thank',\n",
       "  'question',\n",
       "  'call',\n",
       "  'business',\n",
       "  'well',\n",
       "  'executive',\n",
       "  'customers',\n",
       "  'financial',\n",
       "  'operator'],\n",
       " 'baidu': ['bird',\n",
       "  'state',\n",
       "  'courtesy',\n",
       "  'photo',\n",
       "  'getty',\n",
       "  'images',\n",
       "  'user',\n",
       "  'year',\n",
       "  'thank',\n",
       "  'content',\n",
       "  'think',\n",
       "  'terms',\n",
       "  'also',\n",
       "  'tencent',\n",
       "  'revenue',\n",
       "  'amazon',\n",
       "  'stock',\n",
       "  'second',\n",
       "  'market',\n",
       "  'official'],\n",
       " 'bed bath & beyond': ['said',\n",
       "  'coronavirus',\n",
       "  'billion',\n",
       "  'retailer',\n",
       "  'market',\n",
       "  'bid',\n",
       "  'debt',\n",
       "  'south',\n",
       "  'spanish',\n",
       "  'private',\n",
       "  'rival',\n",
       "  'masmovil',\n",
       "  'chief',\n",
       "  'company',\n",
       "  'frances',\n",
       "  'bids',\n",
       "  'selling',\n",
       "  'co',\n",
       "  'stake',\n",
       "  'revenue'],\n",
       " 'ca technologies': ['amazon',\n",
       "  'said',\n",
       "  'billion',\n",
       "  'app',\n",
       "  'quarter',\n",
       "  'revenue',\n",
       "  'company',\n",
       "  'year',\n",
       "  'polo',\n",
       "  'marco',\n",
       "  'growth',\n",
       "  'new',\n",
       "  'olsavsky',\n",
       "  'customers',\n",
       "  'time',\n",
       "  'stock',\n",
       "  'first',\n",
       "  'million',\n",
       "  'analyst',\n",
       "  'olivier'],\n",
       " 'celgene': ['elliott',\n",
       "  'said',\n",
       "  'alexion',\n",
       "  'company',\n",
       "  'billion',\n",
       "  'companys',\n",
       "  'price',\n",
       "  'year',\n",
       "  'share',\n",
       "  'one',\n",
       "  'amgen',\n",
       "  'investors',\n",
       "  '19',\n",
       "  'covid',\n",
       "  'letter',\n",
       "  'drug',\n",
       "  'quarter',\n",
       "  'sales',\n",
       "  'board',\n",
       "  'industry'],\n",
       " 'cerner': ['data',\n",
       "  'said',\n",
       "  'per',\n",
       "  'cent',\n",
       "  'people',\n",
       "  'risk',\n",
       "  'around',\n",
       "  'lancet',\n",
       "  'studies',\n",
       "  'covid',\n",
       "  '19',\n",
       "  'year',\n",
       "  'medical',\n",
       "  'health',\n",
       "  'virus',\n",
       "  'new',\n",
       "  'nejm',\n",
       "  'surgisphere',\n",
       "  'population',\n",
       "  '000'],\n",
       " 'cisco ': ['postponed',\n",
       "  'said',\n",
       "  'police',\n",
       "  'may',\n",
       "  'canceled',\n",
       "  'season',\n",
       "  'world',\n",
       "  'april',\n",
       "  'people',\n",
       "  'protests',\n",
       "  'cup',\n",
       "  'june',\n",
       "  'new',\n",
       "  'floyd',\n",
       "  'march',\n",
       "  'protesters',\n",
       "  'league',\n",
       "  'cancelled',\n",
       "  'minneapolis',\n",
       "  'think'],\n",
       " 'cognizant': ['customers',\n",
       "  'quarter',\n",
       "  'year',\n",
       "  'think',\n",
       "  'analyst',\n",
       "  'officer',\n",
       "  'new',\n",
       "  'million',\n",
       "  'like',\n",
       "  'see',\n",
       "  'business',\n",
       "  'us',\n",
       "  'q1',\n",
       "  'really',\n",
       "  'going',\n",
       "  'atlas',\n",
       "  'mongodb',\n",
       "  'growth',\n",
       "  'customer',\n",
       "  'impact'],\n",
       " 'comcast': ['disney',\n",
       "  'said',\n",
       "  'parks',\n",
       "  'sports',\n",
       "  'world',\n",
       "  'million',\n",
       "  'theaters',\n",
       "  'would',\n",
       "  'companies',\n",
       "  'new',\n",
       "  'o2',\n",
       "  'mobile',\n",
       "  'like',\n",
       "  'mr',\n",
       "  'year',\n",
       "  'tv',\n",
       "  'suri',\n",
       "  'could',\n",
       "  'police',\n",
       "  'pay'],\n",
       " 'discovery communications': ['theaters',\n",
       "  'movie',\n",
       "  'says',\n",
       "  'going',\n",
       "  'like',\n",
       "  'bock',\n",
       "  'drive',\n",
       "  'back',\n",
       "  'theater',\n",
       "  'go',\n",
       "  'women',\n",
       "  'want',\n",
       "  'could',\n",
       "  'million',\n",
       "  'stock',\n",
       "  'pay',\n",
       "  'new',\n",
       "  '2021',\n",
       "  'also',\n",
       "  'films'],\n",
       " 'dish network': ['sports',\n",
       "  'think',\n",
       "  'tv',\n",
       "  'going',\n",
       "  'said',\n",
       "  'tivo',\n",
       "  'hill',\n",
       "  'know',\n",
       "  'people',\n",
       "  'asapp',\n",
       "  'lowe',\n",
       "  'like',\n",
       "  'companies',\n",
       "  'pay',\n",
       "  'right',\n",
       "  'would',\n",
       "  'really',\n",
       "  'want',\n",
       "  'depot',\n",
       "  'home'],\n",
       " 'ebay': ['elliott',\n",
       "  'said',\n",
       "  'unemployment',\n",
       "  'alexion',\n",
       "  'claims',\n",
       "  'job',\n",
       "  'says',\n",
       "  'amazon',\n",
       "  'jobs',\n",
       "  'april',\n",
       "  'work',\n",
       "  'people',\n",
       "  'company',\n",
       "  'price',\n",
       "  'year',\n",
       "  'trade',\n",
       "  'share',\n",
       "  'day',\n",
       "  'companys',\n",
       "  'benefits'],\n",
       " 'electronic arts': ['rousey',\n",
       "  'gloves',\n",
       "  'ronda',\n",
       "  'ufc',\n",
       "  'curtis',\n",
       "  'blaydes',\n",
       "  'usa',\n",
       "  'red',\n",
       "  'blue',\n",
       "  'today',\n",
       "  'credit',\n",
       "  'fight',\n",
       "  'mandatory',\n",
       "  'police',\n",
       "  'sports',\n",
       "  '2018',\n",
       "  'said',\n",
       "  'night',\n",
       "  'il',\n",
       "  'overeem'],\n",
       " 'equinix': ['space',\n",
       "  'black',\n",
       "  'said',\n",
       "  'launch',\n",
       "  'spacex',\n",
       "  'nasa',\n",
       "  'astronauts',\n",
       "  'first',\n",
       "  'think',\n",
       "  'officer',\n",
       "  'million',\n",
       "  'aboard',\n",
       "  'quarter',\n",
       "  'hurley',\n",
       "  'would',\n",
       "  'rocket',\n",
       "  'crew',\n",
       "  'business',\n",
       "  'restaurants',\n",
       "  'going'],\n",
       " 'facebook': ['said',\n",
       "  'police',\n",
       "  'trump',\n",
       "  'twitter',\n",
       "  'people',\n",
       "  'floyd',\n",
       "  'new',\n",
       "  'black',\n",
       "  'protesters',\n",
       "  'one',\n",
       "  'also',\n",
       "  'would',\n",
       "  'officers',\n",
       "  'coronavirus',\n",
       "  'companies',\n",
       "  'president',\n",
       "  'facebook',\n",
       "  'reuters',\n",
       "  'death',\n",
       "  'get'],\n",
       " 'alphabet': ['said',\n",
       "  'trump',\n",
       "  'companies',\n",
       "  'twitter',\n",
       "  'people',\n",
       "  'would',\n",
       "  'facebook',\n",
       "  'new',\n",
       "  'president',\n",
       "  'app',\n",
       "  'order',\n",
       "  'media',\n",
       "  'social',\n",
       "  'reuters',\n",
       "  'google',\n",
       "  'year',\n",
       "  'company',\n",
       "  'campaign',\n",
       "  'think',\n",
       "  'one'],\n",
       " 'intel': ['said',\n",
       "  'trump',\n",
       "  'china',\n",
       "  'quarter',\n",
       "  'hong',\n",
       "  'think',\n",
       "  'business',\n",
       "  'year',\n",
       "  'kong',\n",
       "  'people',\n",
       "  'would',\n",
       "  'new',\n",
       "  'police',\n",
       "  'president',\n",
       "  'us',\n",
       "  'officer',\n",
       "  'chief',\n",
       "  'analyst',\n",
       "  'coronavirus',\n",
       "  'million'],\n",
       " 'liberty global': ['think',\n",
       "  'quarter',\n",
       "  'going',\n",
       "  'analyst',\n",
       "  'alistair',\n",
       "  'officer',\n",
       "  'remote',\n",
       "  'sites',\n",
       "  'like',\n",
       "  'get',\n",
       "  'clinical',\n",
       "  'solutions',\n",
       "  'kind',\n",
       "  'site',\n",
       "  'back',\n",
       "  'business',\n",
       "  'covid',\n",
       "  'trials',\n",
       "  'jason',\n",
       "  'well'],\n",
       " 'liberty interactive': ['quarter',\n",
       "  'think',\n",
       "  'analyst',\n",
       "  'million',\n",
       "  'year',\n",
       "  'chief',\n",
       "  'officer',\n",
       "  'going',\n",
       "  'us',\n",
       "  'president',\n",
       "  'first',\n",
       "  'call',\n",
       "  'thank',\n",
       "  'business',\n",
       "  'well',\n",
       "  'question',\n",
       "  'see',\n",
       "  'customers',\n",
       "  'executive',\n",
       "  'really'],\n",
       " 'marriott international': ['wells',\n",
       "  'abandoned',\n",
       "  'oil',\n",
       "  'gas',\n",
       "  'said',\n",
       "  'state',\n",
       "  'methane',\n",
       "  'well',\n",
       "  'energy',\n",
       "  'according',\n",
       "  'year',\n",
       "  'climate',\n",
       "  'million',\n",
       "  'regulators',\n",
       "  'water',\n",
       "  'data',\n",
       "  'new',\n",
       "  'rowe',\n",
       "  'states',\n",
       "  'many'],\n",
       " 'mattel': ['toy',\n",
       "  'sale',\n",
       "  'big',\n",
       "  'black',\n",
       "  'price',\n",
       "  'popular',\n",
       "  'adam',\n",
       "  'saving',\n",
       "  '10',\n",
       "  'said',\n",
       "  'also',\n",
       "  'parents',\n",
       "  'outdoor',\n",
       "  'people',\n",
       "  'customers',\n",
       "  'smethurst',\n",
       "  'options',\n",
       "  'toys',\n",
       "  'pictured',\n",
       "  'thread'],\n",
       " 'mckesson ': ['said',\n",
       "  'think',\n",
       "  'antibody',\n",
       "  'quarter',\n",
       "  'company',\n",
       "  'elliott',\n",
       "  'would',\n",
       "  'antibodies',\n",
       "  'year',\n",
       "  'sales',\n",
       "  'going',\n",
       "  'vaccine',\n",
       "  'plasma',\n",
       "  'alexion',\n",
       "  'people',\n",
       "  'us',\n",
       "  'growth',\n",
       "  'one',\n",
       "  'surgical',\n",
       "  'first'],\n",
       " 'mckesson': ['company',\n",
       "  'year',\n",
       "  'billion',\n",
       "  'last',\n",
       "  'cardinal',\n",
       "  'mckesson',\n",
       "  'yield',\n",
       "  'one',\n",
       "  'quarter',\n",
       "  'pandemic',\n",
       "  'healthcare',\n",
       "  'health',\n",
       "  'myers',\n",
       "  'revenue',\n",
       "  'well',\n",
       "  'like',\n",
       "  'squibb',\n",
       "  'big',\n",
       "  'bristol',\n",
       "  'drug'],\n",
       " 'microsoft': ['said',\n",
       "  'quarter',\n",
       "  'police',\n",
       "  'officer',\n",
       "  'think',\n",
       "  'year',\n",
       "  'analyst',\n",
       "  'president',\n",
       "  'million',\n",
       "  'chief',\n",
       "  'new',\n",
       "  'first',\n",
       "  'business',\n",
       "  'trump',\n",
       "  'people',\n",
       "  'executive',\n",
       "  'would',\n",
       "  'financial',\n",
       "  'one',\n",
       "  'also'],\n",
       " 'netflix': ['carano',\n",
       "  'gina',\n",
       "  'premiere',\n",
       "  'california',\n",
       "  'ca',\n",
       "  'said',\n",
       "  'quarter',\n",
       "  'getty',\n",
       "  'amazon',\n",
       "  'arrives',\n",
       "  'billion',\n",
       "  'photo',\n",
       "  'los',\n",
       "  'images',\n",
       "  'angeles',\n",
       "  'year',\n",
       "  'actress',\n",
       "  'haywire',\n",
       "  'new',\n",
       "  'disney'],\n",
       " 'nvidia': ['black',\n",
       "  'said',\n",
       "  'company',\n",
       "  'would',\n",
       "  'says',\n",
       "  'employees',\n",
       "  'trump',\n",
       "  'market',\n",
       "  'quarter',\n",
       "  'economy',\n",
       "  'assets',\n",
       "  'children',\n",
       "  'president',\n",
       "  'graphic',\n",
       "  'people',\n",
       "  'also',\n",
       "  'biden',\n",
       "  'school',\n",
       "  'image',\n",
       "  'link'],\n",
       " 'paypal': ['space',\n",
       "  'launch',\n",
       "  'nasa',\n",
       "  'tesla',\n",
       "  'spacex',\n",
       "  'astronauts',\n",
       "  'musk',\n",
       "  'said',\n",
       "  'first',\n",
       "  'factory',\n",
       "  'county',\n",
       "  'crew',\n",
       "  'california',\n",
       "  'hurley',\n",
       "  'rocket',\n",
       "  'vehicle',\n",
       "  'aboard',\n",
       "  'zoom',\n",
       "  'quarter',\n",
       "  'time'],\n",
       " 'qualcomm': ['huawei',\n",
       "  'companies',\n",
       "  'reuters',\n",
       "  'said',\n",
       "  'could',\n",
       "  'chips',\n",
       "  'tsmc',\n",
       "  'standards',\n",
       "  'rule',\n",
       "  'chinese',\n",
       "  '5g',\n",
       "  'qualcomm',\n",
       "  'states',\n",
       "  'united',\n",
       "  'industry',\n",
       "  'commerce',\n",
       "  'market',\n",
       "  'china',\n",
       "  'micron',\n",
       "  'entity'],\n",
       " 'starbucks': ['said',\n",
       "  'mall',\n",
       "  'people',\n",
       "  'police',\n",
       "  'penney',\n",
       "  'melania',\n",
       "  'think',\n",
       "  'trump',\n",
       "  'johnson',\n",
       "  'one',\n",
       "  'day',\n",
       "  'would',\n",
       "  'home',\n",
       "  'could',\n",
       "  'also',\n",
       "  'company',\n",
       "  'new',\n",
       "  'st',\n",
       "  'center',\n",
       "  'business'],\n",
       " 'stericycle': ['think',\n",
       "  'quarter',\n",
       "  'analyst',\n",
       "  'year',\n",
       "  'million',\n",
       "  'president',\n",
       "  'executive',\n",
       "  'going',\n",
       "  'us',\n",
       "  'thank',\n",
       "  'chief',\n",
       "  'question',\n",
       "  'officer',\n",
       "  'business',\n",
       "  'first',\n",
       "  'well',\n",
       "  'customers',\n",
       "  'operator',\n",
       "  'really',\n",
       "  'see'],\n",
       " 'tesla motors': ['quarter',\n",
       "  'think',\n",
       "  'space',\n",
       "  'officer',\n",
       "  'million',\n",
       "  'business',\n",
       "  'tesla',\n",
       "  'year',\n",
       "  'said',\n",
       "  'first',\n",
       "  'analyst',\n",
       "  'launch',\n",
       "  'nasa',\n",
       "  'chief',\n",
       "  'spacex',\n",
       "  'musk',\n",
       "  'new',\n",
       "  'astronauts',\n",
       "  'services',\n",
       "  'revenue'],\n",
       " 'texas instruments': ['unemployment',\n",
       "  'twitter',\n",
       "  'follow',\n",
       "  'stimulus',\n",
       "  'charisse',\n",
       "  'check',\n",
       "  'get',\n",
       "  'payment',\n",
       "  'irs',\n",
       "  'benefits',\n",
       "  '600',\n",
       "  'receive',\n",
       "  'federal',\n",
       "  'money',\n",
       "  'tax',\n",
       "  'state',\n",
       "  'people',\n",
       "  'eligible',\n",
       "  'jones',\n",
       "  'file'],\n",
       " 'universal display ': ['police',\n",
       "  'immunity',\n",
       "  'court',\n",
       "  'leija',\n",
       "  'said',\n",
       "  'officers',\n",
       "  'think',\n",
       "  'cases',\n",
       "  'case',\n",
       "  'officer',\n",
       "  'courts',\n",
       "  'us',\n",
       "  'qualified',\n",
       "  'analyst',\n",
       "  'covid',\n",
       "  'people',\n",
       "  'cloud',\n",
       "  'lng',\n",
       "  '19',\n",
       "  'year'],\n",
       " 'universal display': ['oled',\n",
       "  'micron',\n",
       "  'market',\n",
       "  'display',\n",
       "  'term',\n",
       "  'growth',\n",
       "  'clean',\n",
       "  'year',\n",
       "  'stock',\n",
       "  'ultra',\n",
       "  'long',\n",
       "  'universal',\n",
       "  'tech',\n",
       "  'smartphones',\n",
       "  'trading',\n",
       "  'technology',\n",
       "  'years',\n",
       "  '2020',\n",
       "  'far',\n",
       "  'see']}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dict_relavant_words = {}\n",
    "for company in companies_w_articles: # for all companies in companies_w_articles\n",
    "\n",
    "    #tfidf_vectorizer=TfidfVectorizer(stop_words = {'english'},ngram_range = (1,3), binary = True)\n",
    "    tfidf_vectorizer=TfidfVectorizer(stop_words = {'english'},ngram_range = (1,1))\n",
    "    plain_text_list = list()\n",
    "    company_article = \"\"\n",
    "    for index, row in df_cleaned.iterrows():\n",
    "        if company in row[\"label\"]:\n",
    "            company_article = company_article+ \" \"+ row[\"plain_text\"]\n",
    "            plain_text_list.append(row[\"plain_text\"])\n",
    "    \n",
    "    plain_text_list.insert(0,company_article)\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(plain_text_list)\n",
    "\n",
    "    #Get the tf-idf scores for the words in the company article complication.\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    "\n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df = df.sort_values(by=[\"tfidf\"],ascending=False).head(20)\n",
    "    dict_relavant_words[company] = list(df.index)\n",
    "dict_relavant_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python program to generate word vectors using Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "#nltk.download('punkt')\n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'australia' and 'melbourne' - CBOW :  0.66155946\n",
      "[('portland', 0.8813670873641968), ('perez', 0.8812724351882935), ('rigel', 0.8715772032737732), ('heights', 0.8652781844139099), ('jaylen', 0.8628085255622864), ('pool', 0.8600395917892456), ('santa', 0.8597794771194458), ('cincinnati', 0.8594452738761902), ('hollywood', 0.8580102920532227), ('charleston', 0.8569580316543579)]\n",
      "Cosine similarity between 'australia' and 'melbourne' - Skip Gram :  0.5816691\n",
      "[('hollywood', 0.8261724710464478), ('capitan', 0.8180453181266785), ('dga', 0.8162583112716675), ('monica', 0.8151893615722656), ('gods', 0.809572696685791), ('augusta', 0.805156946182251), ('sands', 0.8041570782661438), ('citywest', 0.8036929368972778), ('lutheran', 0.8020343780517578), ('hangar', 0.800697922706604)]\n"
     ]
    }
   ],
   "source": [
    "# Apply 2 Word2Vec models to articles   \n",
    "\n",
    "data = [] \n",
    "  \n",
    "# iterate through each article in the file \n",
    "for i in clean_articles: \n",
    "    temp = [] \n",
    "    # tokenize the article into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "\n",
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'australia' \" + \n",
    "               \"and 'melbourne' - CBOW : \", \n",
    "    model1.similarity('melbourne', 'australia')) \n",
    "\n",
    "print(model1.wv.most_similar('melbourne'))\n",
    "    \n",
    "\n",
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 4) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'australia' \" +\n",
    "          \"and 'melbourne' - Skip Gram : \", \n",
    "    model2.similarity('melbourne', 'australia')) \n",
    "print(model2.wv.most_similar('melbourne'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliminated\n",
      "masterchef\n",
      "contestant\n",
      "harry\n",
      "foster\n"
     ]
    }
   ],
   "source": [
    "# FOR GENSIN USING CBOW Manipulations\n",
    "\n",
    "# enumerate data it is trained on\n",
    "for i, word in enumerate(model1.wv.vocab):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84611"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "# Show frequencies\n",
    "#print(\"Original List : \",data)\n",
    "data_flat = []\n",
    "for line in data:\n",
    "    for word in line:\n",
    "        data_flat.append(word)\n",
    "\n",
    "\n",
    "ctr = collections.Counter(data_flat)\n",
    "#print(\"Frequency of the elements in the List : \",ctr)\n",
    "ctr[\"the\"] # count of word \"the\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.itf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 29466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-125a418885c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print (tf_idf_vector.todense().sum())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#print (tf_idf_vector.T.todense())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tfidf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mdf_word_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tfidf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mword_importance_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_word_importance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m         return [t for t, i in sorted(self.vocabulary_.items(),\n\u001b[0;32m-> 1298\u001b[0;31m                                      key=itemgetter(1))]\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         return [t for t, i in sorted(self.vocabulary_.items(),\n\u001b[0m\u001b[1;32m   1298\u001b[0m                                      key=itemgetter(1))]\n\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Vectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X = vectorizer.fit_transform(clean_articles)\n",
    "\n",
    "#print(vectorizer.get_feature_names()[:10])\n",
    "#print(X.shape)\n",
    "#print(vectorizer.get_stop_words())\n",
    "#print(vectorizer.get_params(deep=True))\n",
    "\n",
    "n_articles, n_distinct_words = X.shape\n",
    "print(n_articles, n_distinct_words)\n",
    "\n",
    "collect_word_importance = []\n",
    "#place tf-idf values in a pandas data frame \n",
    "for tf_idf_vector_id in range(n_articles):\n",
    "    \n",
    "    tf_idf_vector=X[tf_idf_vector_id]\n",
    "    #print (tf_idf_vector.todense().sum())\n",
    "    #print (tf_idf_vector.T.todense())\n",
    "    df = pd.DataFrame(tf_idf_vector.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df_word_importance = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    word_importance_list = np.array(df_word_importance.index)\n",
    "    collect_word_importance.append(word_importance_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['melissa', 'masterchef', 'leong', ..., 'findlay', 'findings',\n",
       "        'zuocheng'],\n",
       "       ['the', 'burglary', 'bail', ..., 'firmware', 'firms', 'zuocheng'],\n",
       "       ['the', 'to', 'children', ..., 'flaring', 'flareups', 'zuocheng'],\n",
       "       ...,\n",
       "       ['the', 'to', 'in', ..., 'firestorm', 'fires', 'zuocheng'],\n",
       "       ['the', 'to', 'in', ..., 'firestorm', 'fires', 'zuocheng'],\n",
       "       ['the', 'to', 'in', ..., 'firestorm', 'fires', 'zuocheng']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each line corresponds to the highest scored words in the article of same index.\n",
    "collect_word_importance = np.array(collect_word_importance)\n",
    "collect_word_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfTransformer\n",
    "#TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]\n",
    "\n",
    "\n",
    "corpus = ['this is the first document',\n",
    "           'this document is the second document',\n",
    "          'and this is the third one',\n",
    "           'is this the first document']\n",
    "vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n",
    "               'and', 'one']\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()\n",
    "pipe['tfid'].idf_\n",
    "pipe.transform(corpus).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.21510797 7.90825515 7.90825515 ... 7.90825515 6.99196442 7.21510797]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 29466)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('count', CountVectorizer()),\n",
    "                  ('tfid', TfidfTransformer())]).fit(clean_articles)\n",
    "pipe['count'].transform(clean_articles).toarray().shape\n",
    "print (pipe['tfid'].idf_)\n",
    "Tfidf_res = pipe.transform(clean_articles)\n",
    "Tfidf_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x29466 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 666150 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tfidf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tutorial\n",
    "\n",
    "#Dataset and Imports\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    " \n",
    "# this is a very toy example, do not try this at home unless you want to understand the usage differences \n",
    "docs=[\"the house had a tiny little mouse\", \n",
    "\"the cat saw the mouse\", \n",
    "\"the mouse ran away from the house\", \n",
    "\"the cat finally ate the mouse\", \n",
    "\"the end of the mouse story\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize CountVectorizer\n",
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer() \n",
    "# this steps generates word counts for the words in your docs \n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector.shape\n",
    "# 5 texts, 9 distinct words -> gives the count for each word in each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x16 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute the IDF values\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         idf_weights\n",
       "mouse       1.000000\n",
       "the         1.000000\n",
       "cat         1.693147\n",
       "house       1.693147\n",
       "ate         2.098612\n",
       "away        2.098612\n",
       "end         2.098612\n",
       "finally     2.098612\n",
       "from        2.098612\n",
       "had         2.098612\n",
       "little      2.098612\n",
       "of          2.098612\n",
       "ran         2.098612\n",
       "saw         2.098612\n",
       "story       2.098612\n",
       "tiny        2.098612"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the TFIDF score for your documents\n",
    "# count matrix \n",
    "count_vector=cv.transform(docs) #<==> word_count_vector\n",
    "\n",
    "# tf-idf scores \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x16 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.398203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfidf\n",
       "had      0.493562\n",
       "little   0.493562\n",
       "tiny     0.493562\n",
       "house    0.398203\n",
       "mouse    0.235185\n",
       "the      0.235185\n",
       "ate      0.000000\n",
       "away     0.000000\n",
       "cat      0.000000\n",
       "end      0.000000\n",
       "finally  0.000000\n",
       "from     0.000000\n",
       "of       0.000000\n",
       "ran      0.000000\n",
       "saw      0.000000\n",
       "story    0.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names() \n",
    " \n",
    "#get tfidf vector for FFFFFFFFFirst document \n",
    "first_document_vector=tf_idf_vector[0] \n",
    " \n",
    "#print the scores (Tf-idf scores of first document)\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidfvectorizer Usage - Compute all at Once\n",
    "\n",
    " \n",
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    "#fitted_vectorizer=tfidf_vectorizer.fit(docs)               # This method would work too\n",
    "#tfidf_vectorizer_vectors=fitted_vectorizer.transform(docs)  \n",
    " \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.398203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfidf\n",
       "had      0.493562\n",
       "little   0.493562\n",
       "tiny     0.493562\n",
       "house    0.398203\n",
       "mouse    0.235185\n",
       "the      0.235185\n",
       "ate      0.000000\n",
       "away     0.000000\n",
       "cat      0.000000\n",
       "end      0.000000\n",
       "finally  0.000000\n",
       "from     0.000000\n",
       "of       0.000000\n",
       "ran      0.000000\n",
       "saw      0.000000\n",
       "story    0.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
