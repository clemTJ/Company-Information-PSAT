{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Labeling & Lexical Fields Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cprofile for evaulation of a function's speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile,pstats, io\n",
    "def profile(fct):\n",
    "    \"\"\" a decorator for the function \n",
    "        use by writing @profile before any function that needs evaluation\"\"\"\n",
    "    def inner(*args,**kwargs):\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()\n",
    "        retval = fct(*args,**kwargs)\n",
    "        s=i0.StringIO()\n",
    "        sortBy = 'cumulative'\n",
    "        ps = pstats.Stats(pr,stream = s).sort_stats(sortBy)\n",
    "        ps.print_stats()\n",
    "        print (s.getvalue())\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Unlabelled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "raw_json_data = []\n",
    "with open('./data/20200420_20200714_business_articles.json') as f:\n",
    "    for line in f:\n",
    "        raw_json_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type <class 'list'>\n",
      "json <class 'dict'>\n",
      "keys dict_keys(['published', 'link', 'message', 'Feed', 'title', '@version', 'author', '@timestamp', 'full-text', 'type'])\n",
      "length 416307\n"
     ]
    }
   ],
   "source": [
    "print (\"data type\",type (raw_json_data))\n",
    "print (\"json\",type (raw_json_data[0]))\n",
    "print (\"keys\",raw_json_data[0].keys())\n",
    "print (\"length\", len(raw_json_data))\n",
    "#print (raw_json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Company Names & Related Names(49companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0ad659690806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fetching company names (52 companies)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mr'./data/relevant_words/comapny_name-related_words.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Lower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split company name and related names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# fetching company names (52 companies)\n",
    "df = pd.read_excel (r'./data/relevant_words/comapny_name-related_words.xlsx', header = None)\n",
    "# Lower\n",
    "df[0] = [row[0].lower() for index, row in df.iterrows()] \n",
    "# Split company name and related names\n",
    "split = np.array([row[0].split(\";\") for index, row in df.iterrows()])\n",
    "df[\"company_name\"] = split[:,0]\n",
    "df[\"related_name\"] = split[:,1]\n",
    "df.drop(columns = [0], inplace=True)\n",
    "# build dictionary of related name of companies\n",
    "dict_companies = {}\n",
    "company_names = df[\"company_name\"].unique()\n",
    "for name in company_names:\n",
    "    df_tmp = df[df[\"company_name\"] == name]\n",
    "    dict_companies[name] = list(df_tmp[\"related_name\"])\n",
    "print (len(dict_companies.keys()), dict_companies.keys())\n",
    "print (dict_companies[\"21st century fox\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Company Names & Related Names(49 companies) unlowered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching company names (52 companies)\n",
    "df = pd.read_excel (r'./data/relevant_words/comapny_name-related_words.xlsx', header = None)\n",
    "# Lower\n",
    "df[0] = [row[0].replace(\" \", \"_\") for index, row in df.iterrows()] \n",
    "# Split company name and related names\n",
    "split = np.array([row[0].split(\";\") for index, row in df.iterrows()])\n",
    "df[\"company_name\"] = split[:,0]\n",
    "df[\"related_name\"] = split[:,1]\n",
    "df.drop(columns = [0], inplace=True)\n",
    "# build dictionary of related name of companies\n",
    "dict_companies_unlowered = {}\n",
    "company_names = df[\"company_name\"].unique()\n",
    "for name in company_names:\n",
    "    df_tmp = df[df[\"company_name\"] == name]\n",
    "    dict_companies_unlowered[name] = list(df_tmp[\"related_name\"])\n",
    "print (len(dict_companies_unlowered.keys()), dict_companies_unlowered.keys())\n",
    "#print (dict_companies_unlowered['21st_Century_Fox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting url, title & full_text of each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list()\n",
    "plain_texts = list()\n",
    "titles = list()\n",
    "labels = list()\n",
    "\n",
    "min_article_size = 2000\n",
    "for article in raw_json_data:\n",
    "    plain_text = article.get('full-text')\n",
    "    title = article.get('title')\n",
    "    url = article.get('link')\n",
    "    if (plain_text and \"Article `download()` failed\" != plain_text[:27] and \"Please enable cookies\" != plain_text[:21] and len(plain_text)>min_article_size):\n",
    "        plain_texts.append(plain_text)\n",
    "        urls.append(url)\n",
    "        titles.append(title)\n",
    "        labels.append(list())\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataFrame with extacted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#Statistics\n",
    "# 358192 removing \"Article `download()` failed\" \n",
    "# 340987 removing \"Article `download()` failed\" and \"Please enable cookies\"\n",
    "# 215039 removing \"Article `download()` failed\" and \"Please enable cookies\" and size<min_article_size = 2000\n",
    "data = np.array([urls,titles, plain_texts, labels]).T\n",
    "columns=[\"url\", \"title\", \"plain_text\", \"label\"]\n",
    "df_articles = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>plain_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>215034</th>\n",
       "      <td>http://rssfeeds.usatoday.com/~/t/0/0/usatodayc...</td>\n",
       "      <td>Michigan partygoers test positive for COVID-19...</td>\n",
       "      <td>Michigan partygoers test positive for COVID-19...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215035</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jul/...</td>\n",
       "      <td>Coast Guard officials decline to testify on ra...</td>\n",
       "      <td>NEW LONDON, Conn. (AP) - A planned congression...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215036</th>\n",
       "      <td>https://www.denverpost.com/2020/07/08/united-a...</td>\n",
       "      <td>United Airlines will slash nearly 36,000 jobs ...</td>\n",
       "      <td>United Airlines plans to furlough as many as 3...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215037</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jul/...</td>\n",
       "      <td>The Latest: Pence says CDC will issue guidance...</td>\n",
       "      <td>WASHINGTON - Vice President Mike Pence says th...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215038</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jul/...</td>\n",
       "      <td>US rejects nearly all Chinese claims in  South...</td>\n",
       "      <td>WASHINGTON (AP) - The Trump administration esc...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  \\\n",
       "215034  http://rssfeeds.usatoday.com/~/t/0/0/usatodayc...   \n",
       "215035  https://www.washingtontimes.com/news/2020/jul/...   \n",
       "215036  https://www.denverpost.com/2020/07/08/united-a...   \n",
       "215037  https://www.washingtontimes.com/news/2020/jul/...   \n",
       "215038  https://www.washingtontimes.com/news/2020/jul/...   \n",
       "\n",
       "                                                    title  \\\n",
       "215034  Michigan partygoers test positive for COVID-19...   \n",
       "215035  Coast Guard officials decline to testify on ra...   \n",
       "215036  United Airlines will slash nearly 36,000 jobs ...   \n",
       "215037  The Latest: Pence says CDC will issue guidance...   \n",
       "215038  US rejects nearly all Chinese claims in  South...   \n",
       "\n",
       "                                               plain_text label  \n",
       "215034  Michigan partygoers test positive for COVID-19...    []  \n",
       "215035  NEW LONDON, Conn. (AP) - A planned congression...    []  \n",
       "215036  United Airlines plans to furlough as many as 3...    []  \n",
       "215037  WASHINGTON - Vice President Mike Pence says th...    []  \n",
       "215038  WASHINGTON (AP) - The Trump administration esc...    []  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building df_clean - Cleaning full_text of articles (original way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def clean_plain_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    #[:punct:], ,[^0-9], [^a-z]\n",
    "    #text = re.sub(\"[^a-z],[^:punct:],[^0-9]\", ' ', text)\n",
    "    text = re.sub(\"[^a-z0-9]\", ' ', text)\n",
    "    #text = re.sub(r'\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliminated masterchef contestant harry foster has hit back at unfair criticism against judge melissa'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove every non-letter/number character\n",
    "#n_articles = 10000\n",
    "#df_cleaned = df_articles.head(n_articles).copy(deep= True)\n",
    "df_cleaned = df_articles.copy(deep= True)\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    row[\"plain_text\"] = row[\"plain_text\"].lower()\n",
    "    row[\"plain_text\"]= re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "    #[:punct:], ,[^0-9], [^a-z]\n",
    "    #row[\"plain_text\"] = re.sub(\"[^a-z],[^:punct:],[^0-9]\", ' ', row[\"plain_text\"])\n",
    "    row[\"plain_text\"] = re.sub(\"[^a-z0-9]\", ' ', row[\"plain_text\"])\n",
    "    #row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "df_cleaned[\"plain_text\"][0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Stop Words & Removing them from plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all stop words from plain text\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for stop_word in stop_words:\n",
    "        row[\"plain_text\"] = re.sub(' '+stop_word+' ', ' ', row[\"plain_text\"])\n",
    "    row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "df_cleaned[\"plain_text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Articles with Company Names \n",
    "### Check if Articles has Companies names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_cleaned.iterrows(): # initialize labels\n",
    "    row['label'] = list()\n",
    "company_names = dict_companies.keys()   \n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for company in company_names:\n",
    "        if company in row[\"plain_text\"]:\n",
    "            row['label'].append(company)\n",
    "        else:\n",
    "            for related_name in dict_companies[company]:\n",
    "                if related_name in row[\"plain_text\"]:\n",
    "                    row['label'].append(company)\n",
    "                    break\n",
    "df_cleaned[\"label\"].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# saving data to csv\n",
    "PATH = \"./data/\"\n",
    "file = \"cleaned_articles_200k\"\n",
    "df_cleaned.to_csv(PATH + file + \".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data from csv\n",
    "PATH = \"./data/\"\n",
    "file = \"cleaned_articles_200k\"\n",
    "df_cleaned = pd.read_csv(PATH + file + \".csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformating necessary after loading - send only once or will break\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    if row[\"label\"]==\"[]\":\n",
    "        row[\"label\"] = list()\n",
    "    else:\n",
    "        #print (type(row[\"label\"]))\n",
    "        row[\"label\"] = row[\"label\"].strip(\"']['\").split(\"', '\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia\n"
     ]
    }
   ],
   "source": [
    "print (df_cleaned[\"label\"][2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>plain_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>215034</th>\n",
       "      <td>http://rssfeeds.usatoday.com/~/t/0/0/usatodayc...</td>\n",
       "      <td>Michigan partygoers test positive for COVID-19...</td>\n",
       "      <td>michigan partygoers test positive covid 19 jul...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215035</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jul/...</td>\n",
       "      <td>Coast Guard officials decline to testify on ra...</td>\n",
       "      <td>new london conn ap planned congressional heari...</td>\n",
       "      <td>[autodesk, cisco]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215036</th>\n",
       "      <td>https://www.denverpost.com/2020/07/08/united-a...</td>\n",
       "      <td>United Airlines will slash nearly 36,000 jobs ...</td>\n",
       "      <td>united airlines plans furlough many 35 902 u e...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215037</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jul/...</td>\n",
       "      <td>The Latest: Pence says CDC will issue guidance...</td>\n",
       "      <td>washington vice president mike pence says cent...</td>\n",
       "      <td>[autodesk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215038</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jul/...</td>\n",
       "      <td>US rejects nearly all Chinese claims in  South...</td>\n",
       "      <td>washington ap trump administration escalated a...</td>\n",
       "      <td>[autodesk]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  \\\n",
       "215034  http://rssfeeds.usatoday.com/~/t/0/0/usatodayc...   \n",
       "215035  https://www.washingtontimes.com/news/2020/jul/...   \n",
       "215036  https://www.denverpost.com/2020/07/08/united-a...   \n",
       "215037  https://www.washingtontimes.com/news/2020/jul/...   \n",
       "215038  https://www.washingtontimes.com/news/2020/jul/...   \n",
       "\n",
       "                                                    title  \\\n",
       "215034  Michigan partygoers test positive for COVID-19...   \n",
       "215035  Coast Guard officials decline to testify on ra...   \n",
       "215036  United Airlines will slash nearly 36,000 jobs ...   \n",
       "215037  The Latest: Pence says CDC will issue guidance...   \n",
       "215038  US rejects nearly all Chinese claims in  South...   \n",
       "\n",
       "                                               plain_text              label  \n",
       "215034  michigan partygoers test positive covid 19 jul...                 []  \n",
       "215035  new london conn ap planned congressional heari...  [autodesk, cisco]  \n",
       "215036  united airlines plans furlough many 35 902 u e...                 []  \n",
       "215037  washington vice president mike pence says cent...         [autodesk]  \n",
       "215038  washington ap trump administration escalated a...         [autodesk]  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of articles with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should print a list: ['advanced micro devices', 'nvidia'] 2 nvidia\n",
      "There are 121097 labeled articles in the 215039 articles of the corpus\n"
     ]
    }
   ],
   "source": [
    "labeled = 0\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    \n",
    "    if len(row[\"label\"])>0:\n",
    "        if index ==0:\n",
    "            print (\"sould never print\",row[\"label\"],len(row[\"label\"]), row[\"label\"][1])\n",
    "        if index ==2:\n",
    "            print (\"should Print:\",row[\"label\"],len(row[\"label\"]), row[\"label\"][1])\n",
    "        labeled +=1\n",
    "print (\"There are %d labeled articles in the %d articles of the corpus\"%(labeled, len (df_cleaned[\"label\"])))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Number of Articles that each Company is Associated to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49 companies with associated articles over the 49 total companies\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "dict_count = {}\n",
    "for company in company_names: dict_count[company]= 0\n",
    "    \n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for company in company_names:\n",
    "        if company in row[\"label\"]:\n",
    "            dict_count[company]+=1\n",
    "\n",
    "#dict_count          \n",
    "companies_w_articles = list()\n",
    "for company in company_names:\n",
    "    if dict_count[company]>0:\n",
    "        companies_w_articles.append(company)\n",
    "print (\"there are %d companies with associated articles over the %d total companies\"%(len(companies_w_articles),len(company_names)) )\n",
    "#dict_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21649"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_count[\"apple\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS TAGGING prep for Tf.Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/pierre/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96676it [5:44:17, 25.85it/s] "
     ]
    }
   ],
   "source": [
    "nouns = [\"NN\",\"NNS\",\"NNP\",\"NNPS\"]\n",
    "\n",
    "df_pos = df_articles.copy(deep= True)\n",
    "df_pos[\"label\"] = df_cleaned[\"label\"]\n",
    "for index, row in tqdm(df_pos.iterrows()):\n",
    "    paragraph = row[\"plain_text\"]\n",
    "    sent_text = nltk.sent_tokenize(paragraph) # this gives us a list of sentences\n",
    "    # Now Loop over each sentence and tokenize it separately\n",
    "    tokenized_paragraph = list()\n",
    "    for sentence in sent_text:\n",
    "        tokenized_text = nltk.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(tokenized_text)\n",
    "        tokenized_paragraph += tagged\n",
    "    #tokenized_paragraph\n",
    "    noun_paragraph = list()\n",
    "    paragraph = \"\"\n",
    "    for token in tokenized_paragraph:\n",
    "        if token[1] in nouns:\n",
    "            #noun_paragraph.append(re.sub(\"[^a-z]\", '', token[0].lower()))\n",
    "            paragraph += \" \"+re.sub(\"[^a-z]\", '', token[0].lower())\n",
    "    #noun_paragraph  \n",
    "    row[\"plain_text\"] = paragraph\n",
    "#48s for 1000\n",
    "#2h50 for 216000\n",
    "#start 6pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data to csv\n",
    "PATH = \"./data/\"\n",
    "file = \"pos_articles_200k\"\n",
    "df_pos.to_csv(PATH + file + \".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data from csv\n",
    "PATH = \"./data/\"\n",
    "file = \"pos_articles_200k\"\n",
    "df_pos = pd.read_csv(PATH + file + \".csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>plain_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.dailymail.co.uk/tvshowbiz/article-...</td>\n",
       "      <td>MasterChef's Harry Foster hits back at claims ...</td>\n",
       "      <td>masterchef harry foster criticism judge melis...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jun/...</td>\n",
       "      <td>Protest arrests logjam tests NYC legal system,...</td>\n",
       "      <td>new york ap wave arrests new york city death ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-83114...</td>\n",
       "      <td>Labour's Anneliese Dodds says she will REFUSE ...</td>\n",
       "      <td>shadow minister today evidence children schoo...</td>\n",
       "      <td>['advanced micro devices', 'nvidia']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://feeds.reuters.com/~r/Reuters/worldNews/...</td>\n",
       "      <td>Civil unrest rages in Minneapolis over raciall...</td>\n",
       "      <td>minneapolis reuters rallies way night arson v...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-82734...</td>\n",
       "      <td>Australia 'beats the cr*p' out of coronavirus ...</td>\n",
       "      <td>australia c   p coronavirus states territorie...</td>\n",
       "      <td>['apple']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.dailymail.co.uk/tvshowbiz/article-...   \n",
       "1  https://www.washingtontimes.com/news/2020/jun/...   \n",
       "2  https://www.dailymail.co.uk/news/article-83114...   \n",
       "3  http://feeds.reuters.com/~r/Reuters/worldNews/...   \n",
       "4  https://www.dailymail.co.uk/news/article-82734...   \n",
       "\n",
       "                                               title  \\\n",
       "0  MasterChef's Harry Foster hits back at claims ...   \n",
       "1  Protest arrests logjam tests NYC legal system,...   \n",
       "2  Labour's Anneliese Dodds says she will REFUSE ...   \n",
       "3  Civil unrest rages in Minneapolis over raciall...   \n",
       "4  Australia 'beats the cr*p' out of coronavirus ...   \n",
       "\n",
       "                                          plain_text  \\\n",
       "0   masterchef harry foster criticism judge melis...   \n",
       "1   new york ap wave arrests new york city death ...   \n",
       "2   shadow minister today evidence children schoo...   \n",
       "3   minneapolis reuters rallies way night arson v...   \n",
       "4   australia c   p coronavirus states territorie...   \n",
       "\n",
       "                                  label  \n",
       "0                                    []  \n",
       "1                                    []  \n",
       "2  ['advanced micro devices', 'nvidia']  \n",
       "3                                    []  \n",
       "4                             ['apple']  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformating necessary after loading - send only once or will break\n",
    "for index, row in df_pos.iterrows():\n",
    "    if row[\"label\"]==\"[]\":\n",
    "        row[\"label\"] = list()\n",
    "    else:\n",
    "        #print (type(row[\"label\"]))\n",
    "        row[\"label\"] = row[\"label\"].strip(\"']['\").split(\"', '\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>plain_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.dailymail.co.uk/tvshowbiz/article-...</td>\n",
       "      <td>MasterChef's Harry Foster hits back at claims ...</td>\n",
       "      <td>masterchef harry foster criticism judge melis...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jun/...</td>\n",
       "      <td>Protest arrests logjam tests NYC legal system,...</td>\n",
       "      <td>new york ap wave arrests new york city death ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-83114...</td>\n",
       "      <td>Labour's Anneliese Dodds says she will REFUSE ...</td>\n",
       "      <td>shadow minister today evidence children schoo...</td>\n",
       "      <td>[advanced micro devices, nvidia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://feeds.reuters.com/~r/Reuters/worldNews/...</td>\n",
       "      <td>Civil unrest rages in Minneapolis over raciall...</td>\n",
       "      <td>minneapolis reuters rallies way night arson v...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-82734...</td>\n",
       "      <td>Australia 'beats the cr*p' out of coronavirus ...</td>\n",
       "      <td>australia c   p coronavirus states territorie...</td>\n",
       "      <td>[apple]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.dailymail.co.uk/tvshowbiz/article-...   \n",
       "1  https://www.washingtontimes.com/news/2020/jun/...   \n",
       "2  https://www.dailymail.co.uk/news/article-83114...   \n",
       "3  http://feeds.reuters.com/~r/Reuters/worldNews/...   \n",
       "4  https://www.dailymail.co.uk/news/article-82734...   \n",
       "\n",
       "                                               title  \\\n",
       "0  MasterChef's Harry Foster hits back at claims ...   \n",
       "1  Protest arrests logjam tests NYC legal system,...   \n",
       "2  Labour's Anneliese Dodds says she will REFUSE ...   \n",
       "3  Civil unrest rages in Minneapolis over raciall...   \n",
       "4  Australia 'beats the cr*p' out of coronavirus ...   \n",
       "\n",
       "                                          plain_text  \\\n",
       "0   masterchef harry foster criticism judge melis...   \n",
       "1   new york ap wave arrests new york city death ...   \n",
       "2   shadow minister today evidence children schoo...   \n",
       "3   minneapolis reuters rallies way night arson v...   \n",
       "4   australia c   p coronavirus states territorie...   \n",
       "\n",
       "                              label  \n",
       "0                                []  \n",
       "1                                []  \n",
       "2  [advanced micro devices, nvidia]  \n",
       "3                                []  \n",
       "4                           [apple]  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf.Idf to get top 20 words for each company (that have articles related to them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [23:20:51<00:00, 1715.33s/it]   \n"
     ]
    }
   ],
   "source": [
    "# Tf.Idf on Companies that have Associated Articles \n",
    "relevant_words_tfidf = {}\n",
    "for company in tqdm(companies_w_articles): # for all companies in companies_w_articles\n",
    "\n",
    "    #tfidf_vectorizer=TfidfVectorizer(stop_words = {'english'},ngram_range = (1,3), binary = True) #sublinear_tf=False\n",
    "    tfidf_vectorizer=TfidfVectorizer(stop_words = {'english'},ngram_range = (1,1))# bilinear doesn't work..\n",
    "    plain_text_list = list()\n",
    "    company_article = \"\"\n",
    "    for index, row in df_cleaned.iterrows():\n",
    "        if company in row[\"label\"]:\n",
    "            company_article = company_article+ \" \"+ row[\"plain_text\"] # add article to company BIG article\n",
    "        else:\n",
    "            plain_text_list.append(row[\"plain_text\"]) # otherwise add to corpus\n",
    "    \n",
    "    plain_text_list.insert(0,company_article) # add company article to begging of corpus\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(plain_text_list)\n",
    "\n",
    "    #Get the tf-idf scores for the words in the company article complication.\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] # discard tf.idf scores for the other texts\n",
    "\n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df = df.sort_values(by=[\"tfidf\"],ascending=False).head(40) # Take top 40 words\n",
    "    \n",
    "    relevant_words_tfidf[company] = list(zip(list(df.index),list(df[\"tfidf\"])))\n",
    "    #print (relevant_words_tfidf[company])\n",
    "    \n",
    "#100%|██████████| 52/52 [7:31:13<00:00, 520.64s/it]\n",
    "#100%|██████████| 52/52 [21:46:51<00:00, 1507.90s/it]    \n",
    "#100%|██████████| 49/49 [23:20:51<00:00, 1715.33s/it] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save dictionary\n",
    "PATH = \"./relevant_words/english/\"\n",
    "file = \"relevant_words_tfidf_binary\"\n",
    "a_file = open(PATH + file + \".json\", \"w\")\n",
    "json.dump(relevant_words_tfidf, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary \n",
    "PATH = \"./relevant_words/english/\"\n",
    "file = \"relevant_words_tfidf_nouns\"\n",
    "a_file = open(PATH + file + \".json\", \"r\")\n",
    "relevant_words_tfidf = json.load(a_file)\n",
    "#relevant_words_tfidf = dict(relevant_words_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in relevant_words_tfidf.keys():\n",
    "    tmp = []\n",
    "    for element in relevant_words_tfidf[company]:\n",
    "        tmp.append((element[0],element[1]))\n",
    "    \n",
    "    relevant_words_tfidf[company] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apple', 0.2475652136129911),\n",
       " ('quarter', 0.23112514445265792),\n",
       " ('analyst', 0.19460750540513994),\n",
       " ('officer', 0.17494318088381838),\n",
       " ('chief', 0.14839686237938338),\n",
       " ('executive', 0.138381270476057),\n",
       " ('thank', 0.1296944471748394),\n",
       " ('question', 0.12294664642888876),\n",
       " ('business', 0.12012922318884033),\n",
       " ('year', 0.11840247545454582),\n",
       " ('president', 0.101373101610324),\n",
       " ('operator', 0.10130650107513707),\n",
       " ('company', 0.09502443181911849),\n",
       " ('us', 0.09401801300102997),\n",
       " ('customers', 0.09348121939819964),\n",
       " ('people', 0.09190341719760198),\n",
       " ('call', 0.08875236217600309),\n",
       " ('thanks', 0.08659283250156699),\n",
       " ('market', 0.08640074291797184),\n",
       " ('cash', 0.08573599811088516),\n",
       " ('time', 0.0856133821129408),\n",
       " ('growth', 0.08391488004698593),\n",
       " ('covid', 0.0827728524084005),\n",
       " ('kind', 0.08204520626243436),\n",
       " ('revenue', 0.08165670645591498),\n",
       " ('sales', 0.07704032303820325),\n",
       " ('capital', 0.07505597343383999),\n",
       " ('today', 0.07087136603314527),\n",
       " ('coronavirus', 0.06969390375154015),\n",
       " ('portfolio', 0.0694342202168067),\n",
       " ('earnings', 0.06460133049963425),\n",
       " ('impact', 0.06376376509325798),\n",
       " ('companies', 0.06346590208350116),\n",
       " ('lot', 0.06346406630157961),\n",
       " ('vice', 0.0613317969171981),\n",
       " ('markets', 0.06002282205302478),\n",
       " ('terms', 0.05996451344723428),\n",
       " ('bit', 0.059204031567405856),\n",
       " ('app', 0.05863112606106393),\n",
       " ('demand', 0.05860519456671186)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_words_tfidf['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#companies_w_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a give model, we want to get the first 20 words related to a company of: companies_w_articles\n",
    "# And store everything into a dictionary like for tf.idf\n",
    "#dict_companies_unlowered\n",
    "def getTopWords(model, n_words, dict_companies):\n",
    "    companies = dict_companies.keys()\n",
    "    \n",
    "    #Word2Vec.most_similar(positive=[], negative=[], topn=10, restrict_vocab=None, indexer=None)\n",
    "    relevant_words = {}\n",
    "    for company in companies:\n",
    "        #print (\"company\", company)\n",
    "        if company in model.wv.vocab:\n",
    "            relevant_words[company] = model.most_similar(company,topn=n_words)\n",
    "        else:\n",
    "            for related_word in dict_companies[company]:\n",
    "                if related_word in model.wv.vocab:\n",
    "                    #print (\"related_word in model.wv.vocab\",related_word in model.wv.vocab)\n",
    "                    #print (related_word)\n",
    "                    relevant_words[company] = model.most_similar(related_word,topn=n_words)\n",
    "                    break\n",
    "    return relevant_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "#nltk.download('punkt')\n",
    "import warnings \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import collections\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "  \n",
    "# iterate through each article in the file \n",
    "data = [word_tokenize(plain_text) for plain_text in df_cleaned[\"plain_text\"]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PATH = \"./data/\"\n",
    "file = \"list_tokenized_pt\"\n",
    "with open(PATH +file , 'wb') as fp:\n",
    "    pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/\"\n",
    "file = \"list_tokenized_pt\"\n",
    "with open (PATH +file, 'rb') as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Matrix factorization to get top 20 words of a company"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# LSA\n",
    "# HAL (Hyper Analogue Language)\n",
    "# CBOW\n",
    "\n",
    "# Create CBOW model \n",
    "model_cbow = Word2Vec(data, min_count = 1, size = 100, window = 5)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save CBOW\n",
    "PATH = \"./data/models/\"\n",
    "file = \"CBOW_model_buff\"\n",
    "word_vectors = model_cbow.wv\n",
    "word_vectors.save(PATH + file+\".kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CBOW\n",
    "PATH = \"./data/models/\"\n",
    "file = \"CBOW_model_200k\"\n",
    "word_vectors = KeyedVectors.load(PATH + file+\".kv\", mmap='r')\n",
    "model_cbow = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('googles', 0.7780945301055908), ('apple', 0.7284873723983765), ('googl', 0.704851508140564), ('alphabet', 0.6509729623794556), ('spotify', 0.6320379972457886), ('facebook', 0.6315559148788452), ('microsoft', 0.6176584362983704), ('alphabets', 0.6062008142471313), ('apps', 0.605197548866272), ('stadia', 0.5996901988983154)]\n",
      "\n",
      "[('king', 0.8107793927192688), ('godfather', 0.5998413562774658), ('thatcher', 0.5920987129211426), ('mitford', 0.5835937261581421), ('altimus', 0.5723137259483337), ('chemouny', 0.5638600587844849), ('atwood', 0.5631056427955627), ('macbeth', 0.5596096515655518), ('enid', 0.5574297308921814), ('antoinette', 0.5557938814163208)]\n",
      "\n",
      "-0.056162722\n"
     ]
    }
   ],
   "source": [
    "print (model_cbow.most_similar('google'))\n",
    "vec = model_cbow['king'] - model_cbow['man'] + model_cbow['woman']\n",
    "print ()\n",
    "print (model_cbow.most_similar([vec]))\n",
    "print()\n",
    "print(model_cbow.similarity('apple', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local context window methods to get top 20 words on a company"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#skip- gram\n",
    "\n",
    "# Create Skip Gram model \n",
    "model_sg = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 4) \n",
    "# Start 12:49"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save sg\n",
    "PATH = \"./data/models/\"\n",
    "file = \"skip-gram_model\"\n",
    "word_vectors = model_sg.wv\n",
    "word_vectors.save(PATH + file+\".kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sg\n",
    "PATH = \"./data/models/\"\n",
    "file = \"skip-gram_model\"\n",
    "word_vectors = KeyedVectors.load(PATH + file+\".kv\", mmap='r')\n",
    "model_sg = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255358"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_sg.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aapl', 0.7750053405761719), ('iphone', 0.7719913125038147), ('google', 0.7477635145187378), ('iphones', 0.6974998712539673), ('android', 0.6899538636207581), ('spotify', 0.6810340285301208), ('9to5mac', 0.6795322895050049), ('watchos', 0.679144024848938), ('alphabet', 0.6761608123779297), ('carkey', 0.6720519661903381)]\n",
      "\n",
      "[('king', 0.8746283054351807), ('coretta', 0.6387332677841187), ('suffragettes', 0.6350522041320801), ('zog', 0.5962809324264526), ('stenhammar', 0.5960381031036377), ('foiling', 0.5941222906112671), ('hietpas', 0.5872020721435547), ('ducruet', 0.5851268768310547), ('luther', 0.5800729990005493), ('khesar', 0.5800399780273438)]\n",
      "\n",
      "0.21863858\n"
     ]
    }
   ],
   "source": [
    "print (model_sg.most_similar('apple'))\n",
    "vec = model_sg['king'] - model_sg['man'] +model_sg['woman']\n",
    "print ()\n",
    "print (model_sg.most_similar([vec]))\n",
    "print()\n",
    "print(model_sg.similarity('apple', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe to get top 20 words of a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe is a global log-bilinear regression model\n",
    "#from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "#glove_input_file = 'glove.txt'\n",
    "#word2vec_output_file = 'word2vec.txt'\n",
    "#glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load('glove-wiki-gigaword-300')\n",
    "#https://github.com/stanfordnlp/GloVe"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save glove\n",
    "PATH = \"./data/models/\"\n",
    "file = \"glove_model\"\n",
    "word_vectors = glove_model.wv\n",
    "word_vectors.save(PATH + file+\".kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove\n",
    "PATH = \"./data/models/\"\n",
    "file = \"glove_model\"\n",
    "word_vectors = KeyedVectors.load(PATH + file+\".kv\", mmap='r')\n",
    "glove_model = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('iphone', 0.5987042188644409), ('macintosh', 0.5836331248283386), ('ipod', 0.5761123895645142), ('microsoft', 0.5663833022117615), ('ipad', 0.5628098249435425), ('intel', 0.5457563400268555), ('ibm', 0.5286195278167725), ('google', 0.5282472372055054), ('imac', 0.5072520971298218), ('software', 0.4962984323501587)]\n",
      "\n",
      "[('king', 0.8065859079360962), ('queen', 0.689616322517395), ('monarch', 0.5575490593910217), ('throne', 0.5565374493598938), ('princess', 0.5518684387207031), ('mother', 0.5142154693603516), ('daughter', 0.5133156776428223), ('kingdom', 0.5025345087051392), ('prince', 0.5017740726470947), ('elizabeth', 0.49080315232276917)]\n",
      "\n",
      "0.090478964\n"
     ]
    }
   ],
   "source": [
    "print (glove_model.most_similar('apple'))\n",
    "vec = glove_model['king'] - glove_model['man'] +glove_model['woman']\n",
    "print ()\n",
    "print (glove_model.most_similar([vec]))\n",
    "print()\n",
    "print(glove_model.similarity('apple', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google sg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PATH = \"./data/models/\"\n",
    "file = \"freebase-vectors-skipgram1000.bin\"\n",
    "model_google_sg = gensim.models.KeyedVectors.load_word2vec_format(PATH+file, binary=True)\n",
    "# Start 10:03"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gensim.downloader as api\n",
    "model_google_sg = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save google sg\n",
    "PATH = \"./data/models/\"\n",
    "file = \"model_google_sg\"\n",
    "word_vectors = model_google_sg.wv\n",
    "word_vectors.save(PATH + file+\".kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load google sg\n",
    "PATH = \"./data/models/\"\n",
    "file = \"model_google_sg\"\n",
    "word_vectors = KeyedVectors.load(PATH + file+\".kv\", mmap='r')\n",
    "model_google_sg = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_google_sg.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple_AAPL', 0.7456986308097839), ('Apple_Nasdaq_AAPL', 0.7300410270690918), ('Apple_NASDAQ_AAPL', 0.7175089120864868), ('Apple_Computer', 0.7145973443984985), ('iPhone', 0.6924266815185547), ('Apple_NSDQ_AAPL', 0.6868604421615601), ('Steve_Jobs', 0.6758421659469604), ('iPad', 0.6580768823623657), ('Apple_nasdaq_AAPL', 0.6444970369338989), ('AAPL_PriceWatch_Alert', 0.6439753174781799)]\n",
      "\n",
      "[('king', 0.8449392318725586), ('queen', 0.7300517559051514), ('monarch', 0.6454660892486572), ('princess', 0.6156250834465027), ('crown_prince', 0.5818676352500916), ('prince', 0.577711820602417), ('kings', 0.5613664388656616), ('sultan', 0.5376776456832886), ('Queen_Consort', 0.5344247817993164), ('queens', 0.5289887189865112)]\n",
      "\n",
      "0.11685416\n"
     ]
    }
   ],
   "source": [
    "print (model_google_sg.most_similar('apple'))\n",
    "vec = model_google_sg['king'] - model_google_sg['man'] +model_google_sg['woman']\n",
    "print ()\n",
    "print (model_google_sg.most_similar([vec]))\n",
    "print()\n",
    "print(model_google_sg.similarity('apple', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext & LTSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all the models into one & score each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_words_tfidf 49 40\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dict_companies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1801daf6f251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"relevant_words_tfidf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_words_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_words_tfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_words_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#GloVe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrelevant_words_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTopWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_relevant_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdict_companies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"relevant_words_glove\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_words_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_words_glove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_words_glove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#CBOW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dict_companies' is not defined"
     ]
    }
   ],
   "source": [
    "# retrieve all similarity words\n",
    "#tf.idf\n",
    "n_relevant_words = 20\n",
    "\n",
    "print (\"relevant_words_tfidf\", len(relevant_words_tfidf), len(relevant_words_tfidf[list(relevant_words_tfidf.keys())[0]]))\n",
    "#GloVe\n",
    "relevant_words_glove = getTopWords(glove_model,n_relevant_words,dict_companies)\n",
    "print (\"relevant_words_glove\",len(relevant_words_glove), len(relevant_words_glove[list(relevant_words_glove.keys())[0]]))\n",
    "#CBOW\n",
    "relevant_words_cbow = getTopWords(model_cbow,n_relevant_words,dict_companies)\n",
    "print (\"relevant_words_cbow\",len(relevant_words_cbow), len(relevant_words_cbow[list(relevant_words_cbow.keys())[0]]))\n",
    "#SG\n",
    "relevant_words_sg = getTopWords(model_sg,n_relevant_words,dict_companies)\n",
    "print (\"relevant_words_sg\",len(relevant_words_sg), len(relevant_words_sg[list(relevant_words_sg.keys())[0]]))\n",
    "# google SG\n",
    "relevant_words_google_sg = getTopWords(model_google_sg,n_relevant_words,dict_companies_unlowered)\n",
    "print (\"relevant_words_google_sg\",len(relevant_words_google_sg), len(relevant_words_google_sg[list(relevant_words_google_sg.keys())[0]]))\n",
    "\n",
    "#print(relevant_words_glove.keys())\n",
    "#print(relevant_words_sg.keys())\n",
    "#print(relevant_words_cbow.keys())\n",
    "\n",
    "print (\"relevant_words_tfidf\",relevant_words_tfidf[\"apple\"])\n",
    "print(\"relevant_words_glove\",relevant_words_glove[\"apple\"])\n",
    "print (\"relevant_words_cbow\",relevant_words_cbow[\"apple\"])\n",
    "print (\"relevant_words_sg\",relevant_words_sg[\"apple\"])\n",
    "print (\"relevant_words_google_sg\",relevant_words_google_sg[\"Apple\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in sg but not in glove\n",
      "stericycle\n",
      "words in glove but not in sg\n",
      "dish network\n"
     ]
    }
   ],
   "source": [
    "print (\"Words in sg but not in glove:\")\n",
    "for relevant_word_sg in relevant_words_sg.keys():\n",
    "    if relevant_word_sg not in relevant_words_glove.keys():\n",
    "        print (relevant_word_sg)\n",
    "print (\"Words in glove but not in sg:\")\n",
    "for relevant_word_glove in relevant_words_glove.keys():\n",
    "    if relevant_word_glove not in relevant_words_sg.keys():\n",
    "        print (relevant_word_glove)\n",
    "print(\"Words in \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_words_sg [('apples', 0.720359742641449), ('pear', 0.6450697183609009), ('fruit', 0.641014575958252), ('berry', 0.6302294135093689), ('pears', 0.6133961081504822), ('strawberry', 0.6058261394500732), ('peach', 0.6025872230529785), ('potato', 0.5960935354232788), ('grape', 0.5935864448547363), ('blueberry', 0.5866668224334717), ('cherries', 0.5784382224082947), ('mango', 0.5751855373382568), ('apricot', 0.5727777481079102), ('melon', 0.5719985365867615), ('almond', 0.5704830288887024), ('Granny_Smiths', 0.5695333480834961), ('grapes', 0.5692256093025208), ('peaches', 0.5659247040748596), ('pumpkin', 0.5651882886886597), ('apricots', 0.5645568370819092)]\n"
     ]
    }
   ],
   "source": [
    "print (\"relevant_words_sg\",relevant_words_google_sg[\"apple\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word(dictionary,tup):\n",
    "    for key in dictionary.keys():\n",
    "        if tup[0]==key:\n",
    "            #dictionary[key] = max(tup[1],dictionary[key])\n",
    "            dictionary[key] +=tup[1]\n",
    "            \n",
    "            return\n",
    "    #l.append(tup)\n",
    "    dictionary[tup[0]] = tup[1]\n",
    "\n",
    "def switch_tup(l, tup1, tup2):\n",
    "    tmp = l[tup1]\n",
    "    l[tup1] = l[tup2]\n",
    "    l[tup2] = tmp\n",
    "    #print (\"switch\")\n",
    "def bubble_sort_list_tup(l):\n",
    "    for i in range (len(l)):\n",
    "        for j in range (0,len(l)-i-1):\n",
    "            if l[j][1]<l[j+1][1]:\n",
    "                switch_tup(l, j, j+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Use the words’ similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iphone': 2.0682634711265564, 'google': 2.0044981241226196, 'iphones': 1.7639256119728088, 'aapl': 1.40452641248703, 'spotify': 1.3477738499641418, 'android': 1.3440640568733215, 'googles': 1.323325276374817, 'homekit': 1.2814541459083557, 'alphabet': 1.2761602401733398, 'ios': 1.2752171754837036, 'macos': 1.255492925643921, 'airpods': 1.249264419078827, 'microsoft': 1.1840619444847107, 'apples': 1.0988799929618835, '9to5mac': 0.6795322895050049, 'watchos': 0.679144024848938, 'carkey': 0.6720519661903381, 'jailbreak': 0.660087525844574, 'earpods': 0.6569204926490784, 'app': 0.6537714004516602, 'tmsc': 0.65050208568573, 'betwildwood': 0.6501995921134949, 'osx': 0.6468653678894043, 'apps': 0.595978856086731, 'wearables': 0.5949651002883911, 'nvidia': 0.5927913188934326, 'googl': 0.5908902883529663, 'sonos': 0.5848830938339233, 'macintosh': 0.5836331248283386, 'ipod': 0.5761123895645142, 'ipad': 0.5628098249435425, 'intel': 0.5457563400268555, 'ibm': 0.5286195278167725, 'imac': 0.5072520971298218, 'software': 0.4962984323501587, 'motorola': 0.47161784768104553, 'computer': 0.4711154103279114, 'itunes': 0.4646926522254944, 'pc': 0.4600933790206909, 'mac': 0.4503524601459503, 'ipods': 0.44740575551986694, 'cherry': 0.4464744031429291, 'computers': 0.44292744994163513, 'apple': 0.19221091642898255, 'quarter': 0.1813757896319136, 'analyst': 0.15118983680084835, 'officer': 0.13039576289958923, 'think': 0.12978398724001325, 'chief': 0.11686635544725837, 'said': 0.11546809934419339, 'million': 0.10470667399352565, 'year': 0.10416846454512013, 'executive': 0.10082235178310875, 'thank': 0.09817567118056518, 'question': 0.09223465868111332, 'business': 0.08953489880929696, 'first': 0.08804188508963462, 'going': 0.08717832601671512, 'us': 0.0832110708925914, 'operator': 0.0831805862027488, 'new': 0.08301255662265287, 'would': 0.08202024828466181, 'financial': 0.07905948282017072, 'like': 0.07856134695086009, 'president': 0.07620409424496975, 'well': 0.07347791690670509, 'covid': 0.07175791753631373, 'call': 0.07142795430315475, 'company': 0.07105299660625786, 'one': 0.07070389586874286, 'customers': 0.06964788053166336, 'see': 0.0685976459883667, 'also': 0.06854789015504241, 'people': 0.06850500586183966, 'good': 0.06826272239409702, 'market': 0.06756314929921786, 'time': 0.06636995729168095, 'really': 0.06632632566987039, 'thanks': 0.0650085462292002, 'cash': 0.06475359072492178, '19': 0.06300962430267361, 'growth': 0.06293439642351165, 'kind': 0.06107871771847573}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat all the words using their similarity scores\n",
    "# remove two same words and keep highest score OR add both scores\n",
    "# sort the words\n",
    "#one_word_companies = relevant_words_glove.keys()\n",
    "company_names= list(dict_companies.keys())\n",
    "related_words_concat_1 = {}\n",
    "for company in company_names: related_words_concat_1[company]= {}\n",
    "# Creat a unique list of words\n",
    "for company in related_words_concat_1.keys():\n",
    "    \n",
    "    if company in relevant_words_glove.keys():\n",
    "        for word in relevant_words_glove[company]:\n",
    "            add_word(related_words_concat_1[company], word)\n",
    "    \n",
    "    if company in relevant_words_sg.keys():\n",
    "        for word in relevant_words_sg[company]:\n",
    "            add_word(related_words_concat_1[company], word)\n",
    "    \n",
    "    if company in relevant_words_cbow.keys():\n",
    "        for word in relevant_words_cbow[company]:\n",
    "            add_word(related_words_concat_1[company], word)\n",
    "    \n",
    "    if company in relevant_words_tfidf.keys():   \n",
    "        for word in relevant_words_tfidf[company]:\n",
    "            add_word(related_words_concat_1[company], word) # convert to tuple\n",
    "    #Sort the list of words\n",
    "    related_words_concat_1[company] = {k: v for k, v in sorted(related_words_concat_1[company].items(), key=lambda item: -item[1])}\n",
    "        \n",
    "print (related_words_concat_1[\"apple\"])\n",
    "len(related_words_concat_1.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Score wrt. the number of lists they belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iphone': 3, 'google': 3, 'iphones': 3, 'microsoft': 2, 'apples': 2, 'aapl': 2, 'android': 2, 'spotify': 2, 'alphabet': 2, 'googles': 2, 'airpods': 2, 'homekit': 2, 'macos': 2, 'ios': 2, 'macintosh': 1, 'ipod': 1, 'ipad': 1, 'intel': 1, 'ibm': 1, 'imac': 1, 'software': 1, 'motorola': 1, 'computer': 1, 'itunes': 1, 'pc': 1, 'mac': 1, 'ipods': 1, 'cherry': 1, 'computers': 1, '9to5mac': 1, 'watchos': 1, 'carkey': 1, 'jailbreak': 1, 'app': 1, 'tmsc': 1, 'betwildwood': 1, 'osx': 1, 'earpods': 1, 'apps': 1, 'wearables': 1, 'nvidia': 1, 'googl': 1, 'sonos': 1, 'apple': 1, 'quarter': 1, 'analyst': 1, 'officer': 1, 'think': 1, 'chief': 1, 'said': 1, 'million': 1, 'year': 1, 'executive': 1, 'thank': 1, 'question': 1, 'business': 1, 'first': 1, 'going': 1, 'us': 1, 'operator': 1, 'new': 1, 'would': 1, 'financial': 1, 'like': 1, 'president': 1, 'well': 1, 'covid': 1, 'call': 1, 'company': 1, 'one': 1, 'customers': 1, 'see': 1, 'also': 1, 'people': 1, 'good': 1, 'market': 1, 'time': 1, 'really': 1, 'thanks': 1, 'cash': 1, '19': 1, 'growth': 1, 'kind': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets score the words compared to the number of times they appear in a list\n",
    "\n",
    "company_names= list(dict_companies.keys())\n",
    "related_words_concat_2 = {}\n",
    "for company in company_names: related_words_concat_2[company]= 0\n",
    "\n",
    "for company in related_words_concat_2.keys():\n",
    "    #creat list of all the words\n",
    "    frequency_map = {}\n",
    "    list_words = list()\n",
    "    if company in relevant_words_glove.keys():\n",
    "        list_words = list_words + relevant_words_glove[company]\n",
    "    if company in relevant_words_sg.keys():\n",
    "        list_words = list_words +relevant_words_sg[company] \n",
    "    if company in relevant_words_cbow.keys():\n",
    "        list_words = list_words + relevant_words_cbow[company] \n",
    "    if company in relevant_words_tfidf.keys():\n",
    "        list_words = list_words + relevant_words_tfidf[company]\n",
    "\n",
    "    for word in list_words:\n",
    "        if word[0] in frequency_map.keys():\n",
    "            frequency_map[word[0]] +=1\n",
    "        else:\n",
    "            frequency_map[word[0]] =1\n",
    "    frequency_map = {k: v for k, v in sorted(frequency_map.items(), key=lambda item: -item[1])}\n",
    "    related_words_concat_2[company] = frequency_map   \n",
    "\n",
    "print (related_words_concat_2[\"apple\"])\n",
    "len(related_words_concat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Compute precision score for each word (using the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [29:17<00:00, 35.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# Ranking the words using the \"precison\" score (with the corpus)\n",
    "#df_cleaned.tail(2)\n",
    "#data\n",
    "company_names= list(dict_companies.keys())\n",
    "related_words_concat_3 = {}\n",
    "related_words_concat_3_count = {}\n",
    "for company in company_names: related_words_concat_3[company]= 0\n",
    "\n",
    "for company in tqdm(related_words_concat_3.keys()):\n",
    "    frequency_map = {}\n",
    "    count_map = {}\n",
    "    n_artcles = 0\n",
    "    list_words = list()\n",
    "    if company in relevant_words_glove.keys():\n",
    "        list_words = list_words + [word[0] for word in relevant_words_glove[company]]\n",
    "    if company in relevant_words_sg.keys():\n",
    "        list_words = list_words +[word[0] for word in relevant_words_sg[company]] \n",
    "    if company in relevant_words_cbow.keys():\n",
    "        list_words = list_words + [word[0] for word in relevant_words_cbow[company]] \n",
    "    if company in relevant_words_tfidf.keys():\n",
    "        list_words = list_words + [word[0] for word in relevant_words_tfidf[company]]\n",
    "    set_words = set(list_words)\n",
    "                      \n",
    "    for index, row in df_cleaned.iterrows():\n",
    "        if company in row['label']:   # if related to company\n",
    "            n_artcles +=1\n",
    "            for word in set_words:\n",
    "                if word in data[index]:\n",
    "                    if word in frequency_map.keys():\n",
    "                        frequency_map[word] +=1\n",
    "                    else:\n",
    "                        frequency_map[word] =1\n",
    "    sorted_frequency_map = {k: v/n_artcles for k, v in sorted(frequency_map.items(), key=lambda item: -item[1])}\n",
    "    sorted_count_map = {k: v for k, v in sorted(frequency_map.items(), key=lambda item: -item[1])}\n",
    "    related_words_concat_3[company] = sorted_frequency_map\n",
    "    related_words_concat_3_count[company] = sorted_count_map\n",
    "\n",
    "#100%|██████████| 23/23 [13:43<00:00, 35.82s/it] \n",
    "#100%|██████████| 43/43 [22:57<00:00, 32.02s/it]\n",
    "#100%|██████████| 49/49 [29:17<00:00, 35.86s/it]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save dictionary\n",
    "PATH = \"./data/\"\n",
    "file = \"related_words_precison\"\n",
    "a_file = open(PATH + file + \"score\"+\".json\", \"w\")\n",
    "b_file = open(PATH + file + \"count\"+\".json\", \"w\")\n",
    "json.dump(related_words_concat_3 , a_file)\n",
    "json.dump(related_words_concat_3_count, b_file)\n",
    "a_file.close()\n",
    "b_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary \n",
    "PATH = \"./data/\"\n",
    "file = \"related_words_precison\"\n",
    "a_file = open(PATH + file +\"score\"+ \".json\", \"r\")\n",
    "b_file = open(PATH + file +\"count\"+ \".json\", \"r\")\n",
    "related_words_concat_3 = json.load(a_file)\n",
    "related_words_concat_3_count = json.load(b_file)\n",
    "#relevant_words_tfidf = dict(relevant_words_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'said': 0.8869693750288697, 'also': 0.7427132892974271, 'new': 0.727701048547277, 'would': 0.6748117695967482, 'one': 0.6467735230264677, 'first': 0.6188276594761882, 'year': 0.6062173772460622, 'people': 0.6017829922860178, '19': 0.5762390872557623, 'time': 0.5720818513557209, 'covid': 0.5450136264954502, 'like': 0.49577347683495776, 'well': 0.4456556884844566, 'million': 0.41932652778419327, 'company': 0.40722435216407227, 'going': 0.3755369763037554, 'president': 0.3690701649036907, 'see': 0.36514388655365143, 'chief': 0.3454663032934547, 'market': 0.34398817497343986, 'business': 0.32874497667328745, 'apple': 0.32135433507321354, 'us': 0.3015381772830154, 'think': 0.28846597995288464, 'good': 0.2808905723128089, 'financial': 0.2792276779527923, 'executive': 0.2790891034227909, 'quarter': 0.25848768996258487, 'really': 0.23696244630236962, 'officer': 0.23673148875236733, 'call': 0.23405238117234053, 'customers': 0.18952376553189523, 'growth': 0.18841516929188415, 'cash': 0.18795325419187953, 'kind': 0.1539101113215391, 'question': 0.15349438773153495, 'analyst': 0.144302277241443, 'google': 0.12522518361125226, 'app': 0.12245369301122454, 'thanks': 0.1166797542611668, 'aapl': 0.10531664280105317, 'operator': 0.10323802485103238, 'thank': 0.09533927664095339, 'apps': 0.07898748210078987, 'iphone': 0.07875652455078756, 'software': 0.07843318398078433, 'apples': 0.0622199639706222, 'alphabet': 0.05524504596055245, 'microsoft': 0.05519885445055199, 'iphones': 0.0453600628204536, 'googl': 0.034135525890341356, 'computer': 0.03353503626033535, 'android': 0.03016305603030163, 'ios': 0.019631391750196313, 'googles': 0.018799944570188, 'intel': 0.01593607095015936, 'spotify': 0.014088410550140883, 'ipad': 0.013811261490138112, 'computers': 0.011917409580119174, 'mac': 0.009561642570095617, 'wearables': 0.009053535960090535, 'cherry': 0.008499237840084993, 'pc': 0.008222088780082222, 'airpods': 0.006974918010069749, 'ibm': 0.006466811400064668, 'nvidia': 0.005866321770058663, 'itunes': 0.0046191510000461915, 'macos': 0.0013857453000138574, 'tmsc': 0.0008314471800083145, 'motorola': 0.0007852556700078526, 'sonos': 0.0006928726500069287, 'ipod': 0.0006928726500069287, 'watchos': 0.000554298120005543, 'imac': 0.0005081066100050811, 'homekit': 0.00046191510000461914, '9to5mac': 0.00046191510000461914, 'earpods': 0.00041572359000415724, 'carkey': 0.0003233405700032334, 'macintosh': 0.00023095755000230957, 'jailbreak': 0.00018476604000184767, 'osx': 0.00018476604000184767, 'ipods': 9.238302000092384e-05, 'betwildwood': 9.238302000092384e-05}\n",
      "{'said': 19202, 'also': 16079, 'new': 15754, 'would': 14609, 'one': 14002, 'first': 13397, 'year': 13124, 'people': 13028, '19': 12475, 'time': 12385, 'covid': 11799, 'like': 10733, 'well': 9648, 'million': 9078, 'company': 8816, 'going': 8130, 'president': 7990, 'see': 7905, 'chief': 7479, 'market': 7447, 'business': 7117, 'apple': 6957, 'us': 6528, 'think': 6245, 'good': 6081, 'financial': 6045, 'executive': 6042, 'quarter': 5596, 'really': 5130, 'officer': 5125, 'call': 5067, 'customers': 4103, 'growth': 4079, 'cash': 4069, 'kind': 3332, 'question': 3323, 'analyst': 3124, 'google': 2711, 'app': 2651, 'thanks': 2526, 'aapl': 2280, 'operator': 2235, 'thank': 2064, 'apps': 1710, 'iphone': 1705, 'software': 1698, 'apples': 1347, 'alphabet': 1196, 'microsoft': 1195, 'iphones': 982, 'googl': 739, 'computer': 726, 'android': 653, 'ios': 425, 'googles': 407, 'intel': 345, 'spotify': 305, 'ipad': 299, 'computers': 258, 'mac': 207, 'wearables': 196, 'cherry': 184, 'pc': 178, 'airpods': 151, 'ibm': 140, 'nvidia': 127, 'itunes': 100, 'macos': 30, 'tmsc': 18, 'motorola': 17, 'sonos': 15, 'ipod': 15, 'watchos': 12, 'imac': 11, 'homekit': 10, '9to5mac': 10, 'earpods': 9, 'carkey': 7, 'macintosh': 5, 'jailbreak': 4, 'osx': 4, 'ipods': 2, 'betwildwood': 2}\n"
     ]
    }
   ],
   "source": [
    "print (related_words_concat_3[\"apple\"])\n",
    "print(related_words_concat_3_count[\"apple\"])\n",
    "#rwc3_frac = dict(related_words_concat_3)\n",
    "#rwc3_frac[\"apple\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_plain_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    #[:punct:], ,[^0-9], [^a-z]\n",
    "    #text = re.sub(\"[^a-z],[^:punct:],[^0-9]\", ' ', text)\n",
    "    text = re.sub(\"[^a-z0-9]\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_company(plain_text, word_list):\n",
    "    n_words = len(word_list)\n",
    "    words_in_text = 0\n",
    "    #print (word_list)\n",
    "    for token in plain_text.split(\" \"):\n",
    "        for word in word_list:\n",
    "             if word == token:\n",
    "                words_in_text +=1\n",
    "                #print(word)\n",
    "    return words_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def label_text(plain_text,related_words,n_sig_words=10, min_score = 0.01):\n",
    "    label_dict = {}\n",
    "    #print (sig_words_list)\n",
    "    for company in related_words.keys():\n",
    "        #print(\"Company\", company)\n",
    "        sig_words_list = list(related_words[company].keys())[:n_sig_words] + [company]\n",
    "        score = score_company(plain_text, sig_words_list)\n",
    "        #print (score)\n",
    "        if score>min_score:\n",
    "            label_dict[company]= score\n",
    "    # Soft_max\n",
    "    sum_exp = sum([np.exp(v) for v in label_dict.values()])\n",
    "    label_dict = {k: np.exp(v)/sum_exp for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    # Exponomial\n",
    "    #max_val = max(label_dict.values())\n",
    "    #label_dict = {k: v/max_val for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 0.9982787595210628,\n",
       " 'alphabet': 0.0009103123974033909,\n",
       " 'amazon': 0.00033488521604819544,\n",
       " 'intel': 0.00033488521604819544,\n",
       " 'microsoft': 0.00012319738613638786,\n",
       " '21st century fox': 1.667295314677839e-05,\n",
       " 'cisco': 3.053757890451772e-07,\n",
       " 'comcast': 3.053757890451772e-07,\n",
       " 'starbucks': 3.053757890451772e-07,\n",
       " 'autodesk': 1.1234147462122804e-07,\n",
       " 'advanced micro devices': 4.1328118904033145e-08,\n",
       " 'ebay': 4.1328118904033145e-08,\n",
       " 'netflix': 4.1328118904033145e-08,\n",
       " 'nvidia': 4.1328118904033145e-08,\n",
       " 'universal display': 4.1328118904033145e-08,\n",
       " 'equinix': 1.5203765287082636e-08,\n",
       " 'adobe': 5.593152677513733e-09,\n",
       " 'ca technologies': 5.593152677513733e-09,\n",
       " 'facebook': 5.593152677513733e-09,\n",
       " 'mckesson': 5.593152677513733e-09,\n",
       " 'qualcomm': 5.593152677513733e-09,\n",
       " 'tesla motors': 5.593152677513733e-09,\n",
       " 'liberty global': 2.0576058813903088e-09,\n",
       " 'marriott international': 7.569509017969398e-10,\n",
       " 'mattel': 2.784666747472775e-10,\n",
       " 'bed bath & beyond': 1.0244216469089824e-10,\n",
       " 'discovery communications': 1.0244216469089824e-10,\n",
       " 'the priceline group': 1.0244216469089824e-10,\n",
       " 'activision blizzard': 3.76863662988805e-11}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_text = \"\"\"\n",
    "The New York Times said on Monday that it was exiting its partnership with Apple News, as news organizations struggle to compete with large tech companies for readers’ attention and dollars.\n",
    "\n",
    "Starting on Monday, Times articles were no longer appearing alongside those from other publications in the curated Apple News feed available on Apple devices.\n",
    "\n",
    "The Times is one of the first media organizations to pull out of Apple News. The Times, which has made adding new subscribers a key business goal, said Apple had given it little in the way of direct relationships with readers and little control over the business. It said it hoped to instead drive readers directly to its own website and mobile app so that it could “fund quality journalism.”\n",
    "\n",
    "“Core to a healthy model between The Times and the platforms is a direct path for sending those readers back into our environments, where we control the presentation of our report, the relationships with our readers and the nature of our business rules,” Meredith Kopit Levien, chief operating officer, wrote in a memo to employees. “Our relationship with Apple News does not fit within these parameters.”\n",
    "\n",
    "An Apple spokesman said that The Times “only offered Apple News a few stories a day,” and that the company would continue to provide readers with trusted information from thousands of publishers.\n",
    "\n",
    "“We are also committed to supporting quality journalism through the proven business models of advertising, subscriptions and commerce,” he said.\"\n",
    "\"\"\"\n",
    "plain_text = clean_plain_text(plain_text)\n",
    "related_words = related_words_concat_1\n",
    "n_sig_words= 100\n",
    "min_score = 10 # nbr of sig words in text\n",
    "#print (plain_text)\n",
    "label_dict = label_text(plain_text,related_words, n_sig_words, min_score)\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------- Annexe Testing -------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
