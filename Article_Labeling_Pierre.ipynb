{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Labeling & Lexical Fields Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cprofile for evaulation of a function's speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile,pstats, io\n",
    "def profile(fct):\n",
    "    \"\"\" a decorator for the function \n",
    "        use by writing @profile before any function that needs evaluation\"\"\"\n",
    "    def inner(*args,**kwargs):\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()\n",
    "        retval = fct(*args,**kwargs)\n",
    "        s=i0.StringIO()\n",
    "        sortBy = 'cumulative'\n",
    "        ps = pstats.Stats(pr,stream = s).sort_stats(sortBy)\n",
    "        ps.print_stats()\n",
    "        print (s.getvalue())\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Unlabelled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "raw_json_data = []\n",
    "with open('./data/20200420_20200714_business_articles.json') as f:\n",
    "    for line in f:\n",
    "        raw_json_data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type <class 'list'>\n",
      "json <class 'dict'>\n",
      "keys dict_keys(['published', 'link', 'message', 'Feed', 'title', '@version', 'author', '@timestamp', 'full-text', 'type'])\n",
      "length 416307\n"
     ]
    }
   ],
   "source": [
    "print (\"data type\",type (raw_json_data))\n",
    "print (\"json\",type (raw_json_data[0]))\n",
    "print (\"keys\",raw_json_data[0].keys())\n",
    "print (\"length\", len(raw_json_data))\n",
    "#print (raw_json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Company Names & Related Names(52 companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['21st century fox', 'activision blizzard', 'adobe ', 'advanced micro devices', 'akamai technologies', 'akamai tecnologies', 'alexion pharmaceuticals', 'amazon', 'american airlines group', 'amgen', 'analog devices', 'apple', 'autodesk', 'automatic data processing', 'baidu', 'bed bath & beyond', 'biogen', 'ca technologies', 'celgene', 'cerner', 'cisco ', 'cognizant', 'comcast', 'discovery communications', 'dish network', 'ebay', 'electronic arts', 'equinix', 'expeditors international', 'facebook', 'alphabet', 'intel', 'liberty global', 'liberty interactive', 'linear technology', 'marriott international', 'mattle', 'mattel', 'mckesson ', 'mckesson', 'microsoft', 'netflix', 'nvidia', 'paypal', 'qualcomm', 'starbucks', 'stericycle', 'tesla motors', 'texas instruments', 'the priceline group', 'universal display ', 'universal display'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetching company names (52 companies)\n",
    "df = pd.read_excel (r'./data/comapny_name-related_words.xlsx', header = None)\n",
    "# Lower\n",
    "df[0] = [row[0].lower() for index, row in df.iterrows()] \n",
    "# Split company name and related names\n",
    "split = np.array([row[0].split(\";\") for index, row in df.iterrows()])\n",
    "df[\"company_name\"] = split[:,0]\n",
    "df[\"related_name\"] = split[:,1]\n",
    "df.drop(columns = [0], inplace=True)\n",
    "# build dictionary of related name of companies\n",
    "dict_companies = {}\n",
    "company_names = df[\"company_name\"].unique()\n",
    "for name in company_names:\n",
    "    df_tmp = df[df[\"company_name\"] == name]\n",
    "    dict_companies[name] = list(df_tmp[\"related_name\"])\n",
    "dict_companies.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting url, title & full_text of each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list()\n",
    "plain_texts = list()\n",
    "titles = list()\n",
    "labels = list()\n",
    "\n",
    "min_article_size = 2000\n",
    "for article in raw_json_data:\n",
    "    plain_text = article.get('full-text')\n",
    "    title = article.get('title')\n",
    "    url = article.get('link')\n",
    "    if (plain_text and \"Article `download()` failed\" != plain_text[:27] and \"Please enable cookies\" != plain_text[:21] and len(plain_text)>min_article_size):\n",
    "        plain_texts.append(plain_text)\n",
    "        urls.append(url)\n",
    "        titles.append(title)\n",
    "        labels.append(list())\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataFrame with extacted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistics\n",
    "# 358192 removing \"Article `download()` failed\" \n",
    "# 340987 removing \"Article `download()` failed\" and \"Please enable cookies\"\n",
    "# 215039 removing \"Article `download()` failed\" and \"Please enable cookies\" and size<min_article_size = 2000\n",
    "data = np.array([urls,titles, plain_texts, labels]).T\n",
    "columns=[\"url\", \"title\", \"plain_text\", \"label\"]\n",
    "df_articles = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>plain_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>215034</th>\n",
       "      <td>http://rssfeeds.usatoday.com/~/t/0/0/usatodayc...</td>\n",
       "      <td>Michigan partygoers test positive for COVID-19...</td>\n",
       "      <td>Michigan partygoers test positive for COVID-19...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215035</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jul/...</td>\n",
       "      <td>Coast Guard officials decline to testify on ra...</td>\n",
       "      <td>NEW LONDON, Conn. (AP) - A planned congression...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215036</th>\n",
       "      <td>https://www.denverpost.com/2020/07/08/united-a...</td>\n",
       "      <td>United Airlines will slash nearly 36,000 jobs ...</td>\n",
       "      <td>United Airlines plans to furlough as many as 3...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215037</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jul/...</td>\n",
       "      <td>The Latest: Pence says CDC will issue guidance...</td>\n",
       "      <td>WASHINGTON - Vice President Mike Pence says th...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215038</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jul/...</td>\n",
       "      <td>US rejects nearly all Chinese claims in  South...</td>\n",
       "      <td>WASHINGTON (AP) - The Trump administration esc...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  \\\n",
       "215034  http://rssfeeds.usatoday.com/~/t/0/0/usatodayc...   \n",
       "215035  https://www.washingtontimes.com/news/2020/jul/...   \n",
       "215036  https://www.denverpost.com/2020/07/08/united-a...   \n",
       "215037  https://www.washingtontimes.com/news/2020/jul/...   \n",
       "215038  https://www.washingtontimes.com/news/2020/jul/...   \n",
       "\n",
       "                                                    title  \\\n",
       "215034  Michigan partygoers test positive for COVID-19...   \n",
       "215035  Coast Guard officials decline to testify on ra...   \n",
       "215036  United Airlines will slash nearly 36,000 jobs ...   \n",
       "215037  The Latest: Pence says CDC will issue guidance...   \n",
       "215038  US rejects nearly all Chinese claims in  South...   \n",
       "\n",
       "                                               plain_text label  \n",
       "215034  Michigan partygoers test positive for COVID-19...    []  \n",
       "215035  NEW LONDON, Conn. (AP) - A planned congression...    []  \n",
       "215036  United Airlines plans to furlough as many as 3...    []  \n",
       "215037  WASHINGTON - Vice President Mike Pence says th...    []  \n",
       "215038  WASHINGTON (AP) - The Trump administration esc...    []  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning full_text of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliminated masterchef contestant harry foster has hit back at unfair criticism against judge melissa'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove every non-letter/number character\n",
    "#n_articles = 10000\n",
    "#df_cleaned = df_articles.head(n_articles).copy(deep= True)\n",
    "df_cleaned = df_articles.copy(deep= True)\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    row[\"plain_text\"] = row[\"plain_text\"].lower()\n",
    "    row[\"plain_text\"]= re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "    #[:punct:], ,[^0-9], [^a-z]\n",
    "    #row[\"plain_text\"] = re.sub(\"[^a-z],[^:punct:],[^0-9]\", ' ', row[\"plain_text\"])\n",
    "    row[\"plain_text\"] = re.sub(\"[^a-z0-9]\", ' ', row[\"plain_text\"])\n",
    "    #row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "df_cleaned[\"plain_text\"][0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Stop Words & Removing them from plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliminated masterchef contestant harry foster hit back unfair criticism judge melissa leong show first female judge 40 faced barrage trolling haters taking aim everything behaviour set fashion sense despite eliminated tuesday night episode harry nothing good things say melbourne based food writer could truth eliminated masterchef australia contestant harry foster pictured hit back unfair criticism judge melissa leong queen love harry told huffpost australia energetic passionate really vibrant asked accusations melissa rude biased show said could truth three judges received overwhelmingly positive response fans melissa copped backlash vocal minority queen show first female judge 40 faced barrage trolling haters taking aim everything behaviour set fashion sense many praised fashion sense positivity others claim waits feedback jock zonfrillo andy allen repeating chance melissa leong original idea masterchef continue wait others tell think dish one viewer tweeted another added new judge melissa speaks like affected melbourne millennial affected melbourne millennial good position note grating mixed three masterchef judges received overwhelmingly positive response fans melissa copped backlash vocal minority kind words energetic passionate really vibrant said harry left eliminated tuesday night episode love masterchef bringing back old contestants loathe new judges especially melissa leong watching ten minutes already irritating third fan wrote twitter others claimed biased towards certain contestants hayden quinn khanh ong masterchef continues thursday 7 30pm channel 10 '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all stop words from plain text\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for stop_word in stop_words:\n",
    "        row[\"plain_text\"] = re.sub(' '+stop_word+' ', ' ', row[\"plain_text\"])\n",
    "    row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "df_cleaned[\"plain_text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Articles with Company Names \n",
    "### Check if Articles Talk of Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  []\n",
       "1                                  []\n",
       "2    [advanced micro devices, nvidia]\n",
       "3                                  []\n",
       "4                             [apple]\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in df_cleaned.iterrows(): # initialize labels\n",
    "    row['label'] = []\n",
    "    \n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for company in company_names:\n",
    "        if company in row[\"plain_text\"]:\n",
    "            row['label'].append(company)\n",
    "        else:\n",
    "            for related_name in dict_companies[company]:\n",
    "                if related_name in row[\"plain_text\"]:\n",
    "                    row['label'].append(company)\n",
    "                    break\n",
    "df_cleaned[\"label\"].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# saving data to csv\n",
    "PATH = \"./data/\"\n",
    "file = \"cleaned_articles\"\n",
    "df_cleaned.to_csv(PATH + file + \".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data from csv\n",
    "PATH = \"./data/\"\n",
    "file = \"cleaned_articles_200k\"\n",
    "df_cleaned = pd.read_csv(PATH + file + \".csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of articles with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 522 labeled articles in the 1000 articles of the corpus\n"
     ]
    }
   ],
   "source": [
    "labeled = 0\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    if row[\"label\"]:\n",
    "        labeled +=1\n",
    "print (\"There are %d labeled articles in the %d articles of the corpus\"%(labeled, len (df_cleaned[\"label\"])))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Number of Articles that each Company is Associated to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 33 companies with associated articles over the 52 total companies\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "dict_count = {}\n",
    "for company in company_names: dict_count[company]= 0\n",
    "    \n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for company in company_names:\n",
    "        if company in row[\"label\"]:\n",
    "            dict_count[company]+=1\n",
    "dict_count          \n",
    "\n",
    "companies_w_articles = list()\n",
    "for company in company_names:\n",
    "    if dict_count[company]>0:\n",
    "        companies_w_articles.append(company)\n",
    "print (\"there are %d companies with associated articles over the %d total companies\"%(len(companies_w_articles),len(company_names)) )\n",
    "#dict_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf.Idf to get top 20 words for each company (that have articles related to them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:08<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tf.Idf on Companies that have Associated Articles \n",
    "dict_relavant_words = {}\n",
    "for company in tqdm(companies_w_articles): # for all companies in companies_w_articles\n",
    "\n",
    "    #tfidf_vectorizer=TfidfVectorizer(stop_words = {'english'},ngram_range = (1,3), binary = True)\n",
    "    tfidf_vectorizer=TfidfVectorizer(stop_words = {'english'},ngram_range = (1,1))\n",
    "    plain_text_list = list()\n",
    "    company_article = \"\"\n",
    "    for index, row in df_cleaned.iterrows():\n",
    "        if company in row[\"label\"]:\n",
    "            company_article = company_article+ \" \"+ row[\"plain_text\"]\n",
    "            plain_text_list.append(row[\"plain_text\"])\n",
    "    \n",
    "    plain_text_list.insert(0,company_article)\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(plain_text_list)\n",
    "\n",
    "    #Get the tf-idf scores for the words in the company article complication.\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    "\n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df = df.sort_values(by=[\"tfidf\"],ascending=False).head(20) # Take top 20 words\n",
    "    dict_relavant_words[company] = list(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'21st century fox': ['sports',\n",
       "  'tv',\n",
       "  'police',\n",
       "  'pay',\n",
       "  'said',\n",
       "  'floyd',\n",
       "  'womens',\n",
       "  'adams',\n",
       "  'protesters',\n",
       "  'officers',\n",
       "  'rights',\n",
       "  'george',\n",
       "  'fans',\n",
       "  'black',\n",
       "  'streaming',\n",
       "  'indigenous',\n",
       "  'live',\n",
       "  'mlb',\n",
       "  'new',\n",
       "  'like'],\n",
       " 'activision blizzard': ['paddy',\n",
       "  'car',\n",
       "  'top',\n",
       "  'gear',\n",
       "  'road',\n",
       "  'filming',\n",
       "  'crash',\n",
       "  'freddie',\n",
       "  'back',\n",
       "  'pictured',\n",
       "  'series',\n",
       "  'chris',\n",
       "  'bbc',\n",
       "  'show',\n",
       "  'said',\n",
       "  'park',\n",
       "  'presenter',\n",
       "  'high',\n",
       "  'crashing',\n",
       "  'speed'],\n",
       " 'advanced micro devices': ['black',\n",
       "  'said',\n",
       "  'employees',\n",
       "  'workers',\n",
       "  'says',\n",
       "  'school',\n",
       "  'children',\n",
       "  'company',\n",
       "  'schools',\n",
       "  'government',\n",
       "  'hospital',\n",
       "  'lives',\n",
       "  'matter',\n",
       "  'would',\n",
       "  'social',\n",
       "  'racial',\n",
       "  'people',\n",
       "  'security',\n",
       "  'companies',\n",
       "  'faison'],\n",
       " 'alexion pharmaceuticals': ['elliott',\n",
       "  'said',\n",
       "  'alexion',\n",
       "  'company',\n",
       "  'companys',\n",
       "  'share',\n",
       "  'price',\n",
       "  'letter',\n",
       "  'investors',\n",
       "  'billion',\n",
       "  'one',\n",
       "  'board',\n",
       "  'sale',\n",
       "  'drug',\n",
       "  'hedge',\n",
       "  'industry',\n",
       "  'alexions',\n",
       "  'direction',\n",
       "  'strategy',\n",
       "  'elliotts'],\n",
       " 'amazon': ['black',\n",
       "  'year',\n",
       "  'growth',\n",
       "  'million',\n",
       "  'cannabis',\n",
       "  'sports',\n",
       "  'company',\n",
       "  'officer',\n",
       "  'says',\n",
       "  'quarter',\n",
       "  'said',\n",
       "  'analyst',\n",
       "  'claims',\n",
       "  'sales',\n",
       "  'medical',\n",
       "  'chief',\n",
       "  'products',\n",
       "  'thank',\n",
       "  'employees',\n",
       "  'brendan'],\n",
       " 'american airlines group': ['quarter',\n",
       "  'year',\n",
       "  'think',\n",
       "  'executive',\n",
       "  'analyst',\n",
       "  'us',\n",
       "  'president',\n",
       "  'officer',\n",
       "  'chief',\n",
       "  'first',\n",
       "  'customers',\n",
       "  'well',\n",
       "  'million',\n",
       "  'question',\n",
       "  'business',\n",
       "  'see',\n",
       "  'kind',\n",
       "  'growth',\n",
       "  'thank',\n",
       "  'products'],\n",
       " 'amgen': ['antibody',\n",
       "  'elliott',\n",
       "  'antibodies',\n",
       "  'alexion',\n",
       "  'plasma',\n",
       "  'said',\n",
       "  'company',\n",
       "  'treatments',\n",
       "  'use',\n",
       "  'share',\n",
       "  'companys',\n",
       "  'price',\n",
       "  'patients',\n",
       "  'convalescent',\n",
       "  'therapies',\n",
       "  '19',\n",
       "  'treatment',\n",
       "  'letter',\n",
       "  'investors',\n",
       "  'one'],\n",
       " 'apple': ['quarter',\n",
       "  'brueckner',\n",
       "  'year',\n",
       "  'said',\n",
       "  'chief',\n",
       "  'president',\n",
       "  'think',\n",
       "  'analyst',\n",
       "  'officer',\n",
       "  'executive',\n",
       "  'black',\n",
       "  'million',\n",
       "  'antibody',\n",
       "  'business',\n",
       "  'police',\n",
       "  'first',\n",
       "  'german',\n",
       "  'us',\n",
       "  'would',\n",
       "  'time'],\n",
       " 'autodesk': ['police',\n",
       "  'said',\n",
       "  'quarter',\n",
       "  'year',\n",
       "  'officer',\n",
       "  'think',\n",
       "  'chief',\n",
       "  'analyst',\n",
       "  'first',\n",
       "  'tesla',\n",
       "  'executive',\n",
       "  'president',\n",
       "  'brueckner',\n",
       "  'floyd',\n",
       "  'black',\n",
       "  'people',\n",
       "  'million',\n",
       "  'minneapolis',\n",
       "  'would',\n",
       "  'protesters'],\n",
       " 'automatic data processing': ['quarter',\n",
       "  'analyst',\n",
       "  'year',\n",
       "  'think',\n",
       "  'million',\n",
       "  'chief',\n",
       "  'officer',\n",
       "  'president',\n",
       "  'executive',\n",
       "  'well',\n",
       "  'question',\n",
       "  'us',\n",
       "  'thank',\n",
       "  'financial',\n",
       "  'business',\n",
       "  'first',\n",
       "  'call',\n",
       "  'see',\n",
       "  'customers',\n",
       "  'good'],\n",
       " 'ca technologies': ['customers',\n",
       "  'analyst',\n",
       "  'year',\n",
       "  'quarter',\n",
       "  'going',\n",
       "  'officer',\n",
       "  'chief',\n",
       "  'products',\n",
       "  'million',\n",
       "  'olivier',\n",
       "  'covid',\n",
       "  'thank',\n",
       "  'us',\n",
       "  'new',\n",
       "  'growth',\n",
       "  'see',\n",
       "  'co',\n",
       "  'like',\n",
       "  'pomel',\n",
       "  'revenue'],\n",
       " 'celgene': ['elliott',\n",
       "  'said',\n",
       "  'alexion',\n",
       "  'company',\n",
       "  'companys',\n",
       "  'share',\n",
       "  'price',\n",
       "  'letter',\n",
       "  'investors',\n",
       "  'billion',\n",
       "  'one',\n",
       "  'board',\n",
       "  'sale',\n",
       "  'drug',\n",
       "  'hedge',\n",
       "  'industry',\n",
       "  'alexions',\n",
       "  'direction',\n",
       "  'strategy',\n",
       "  'elliotts'],\n",
       " 'cerner': ['data',\n",
       "  'said',\n",
       "  'lancet',\n",
       "  'medical',\n",
       "  'studies',\n",
       "  'surgisphere',\n",
       "  'nejm',\n",
       "  'new',\n",
       "  'patient',\n",
       "  'firms',\n",
       "  'research',\n",
       "  'hospital',\n",
       "  '19',\n",
       "  'covid',\n",
       "  'retraction',\n",
       "  'retracted',\n",
       "  'researchers',\n",
       "  'healthcare',\n",
       "  'authors',\n",
       "  'harvard'],\n",
       " 'cisco ': ['ticket',\n",
       "  'holders',\n",
       "  'police',\n",
       "  'said',\n",
       "  'season',\n",
       "  'refunds',\n",
       "  'games',\n",
       "  'credit',\n",
       "  'game',\n",
       "  'single',\n",
       "  'bonus',\n",
       "  'receive',\n",
       "  'may',\n",
       "  'protesters',\n",
       "  'protests',\n",
       "  'available',\n",
       "  'refund',\n",
       "  'students',\n",
       "  'minneapolis',\n",
       "  'floyd'],\n",
       " 'cognizant': ['customers',\n",
       "  'year',\n",
       "  'analyst',\n",
       "  'quarter',\n",
       "  'going',\n",
       "  'officer',\n",
       "  'chief',\n",
       "  'million',\n",
       "  'new',\n",
       "  'products',\n",
       "  'olivier',\n",
       "  'see',\n",
       "  'covid',\n",
       "  'thank',\n",
       "  'us',\n",
       "  'also',\n",
       "  'like',\n",
       "  'time',\n",
       "  'co',\n",
       "  'pomel'],\n",
       " 'comcast': ['sports',\n",
       "  'tv',\n",
       "  'police',\n",
       "  'said',\n",
       "  'pay',\n",
       "  'like',\n",
       "  'floyd',\n",
       "  'womens',\n",
       "  'airstream',\n",
       "  'adams',\n",
       "  'protesters',\n",
       "  'officers',\n",
       "  'rights',\n",
       "  'black',\n",
       "  'backyard',\n",
       "  'fans',\n",
       "  'george',\n",
       "  'streaming',\n",
       "  'live',\n",
       "  'indigenous'],\n",
       " 'dish network': ['sports',\n",
       "  'tv',\n",
       "  'pay',\n",
       "  'fans',\n",
       "  'mlb',\n",
       "  'live',\n",
       "  'like',\n",
       "  'streaming',\n",
       "  'rights',\n",
       "  'nasdaq',\n",
       "  'nfl',\n",
       "  'espn',\n",
       "  'prices',\n",
       "  'companies',\n",
       "  'billion',\n",
       "  'leagues',\n",
       "  'season',\n",
       "  'league',\n",
       "  'new',\n",
       "  'channels'],\n",
       " 'ebay': ['elliott',\n",
       "  'claims',\n",
       "  'said',\n",
       "  'alexion',\n",
       "  'unemployment',\n",
       "  'company',\n",
       "  'share',\n",
       "  'companys',\n",
       "  'price',\n",
       "  'april',\n",
       "  'billion',\n",
       "  'job',\n",
       "  'jobless',\n",
       "  'benefits',\n",
       "  'investors',\n",
       "  'letter',\n",
       "  'rate',\n",
       "  'labor',\n",
       "  'amazon',\n",
       "  'week'],\n",
       " 'electronic arts': ['rousey',\n",
       "  'ronda',\n",
       "  'ufc',\n",
       "  'york',\n",
       "  'new',\n",
       "  'said',\n",
       "  'white',\n",
       "  'tate',\n",
       "  '157',\n",
       "  'police',\n",
       "  'people',\n",
       "  'mma',\n",
       "  'carmouche',\n",
       "  'liz',\n",
       "  '190',\n",
       "  '193',\n",
       "  'miesha',\n",
       "  'protesters',\n",
       "  'nearly',\n",
       "  'jpg'],\n",
       " 'equinix': ['black',\n",
       "  'space',\n",
       "  'launch',\n",
       "  'nasa',\n",
       "  'spacex',\n",
       "  'says',\n",
       "  'rocket',\n",
       "  'hurley',\n",
       "  'astronauts',\n",
       "  'first',\n",
       "  'said',\n",
       "  'behnken',\n",
       "  'aboard',\n",
       "  'crew',\n",
       "  'matter',\n",
       "  'lives',\n",
       "  'employees',\n",
       "  'american',\n",
       "  'racial',\n",
       "  'company'],\n",
       " 'facebook': ['police',\n",
       "  'said',\n",
       "  'floyd',\n",
       "  'black',\n",
       "  'people',\n",
       "  'protesters',\n",
       "  'minneapolis',\n",
       "  'officers',\n",
       "  'twitter',\n",
       "  'death',\n",
       "  'george',\n",
       "  'protests',\n",
       "  'new',\n",
       "  'brueckner',\n",
       "  'billion',\n",
       "  'year',\n",
       "  'one',\n",
       "  'statue',\n",
       "  'company',\n",
       "  'also'],\n",
       " 'alphabet': ['black',\n",
       "  'sports',\n",
       "  'said',\n",
       "  'says',\n",
       "  'employees',\n",
       "  'companies',\n",
       "  'year',\n",
       "  'tv',\n",
       "  'company',\n",
       "  'covid',\n",
       "  'new',\n",
       "  '19',\n",
       "  'remdesivir',\n",
       "  'alstom',\n",
       "  'million',\n",
       "  '500',\n",
       "  'would',\n",
       "  'month',\n",
       "  'matter',\n",
       "  'tuesday'],\n",
       " 'intel': ['said',\n",
       "  'police',\n",
       "  'statue',\n",
       "  'hong',\n",
       "  'london',\n",
       "  'officers',\n",
       "  'people',\n",
       "  'virus',\n",
       "  'black',\n",
       "  'kong',\n",
       "  'antibodies',\n",
       "  'apple',\n",
       "  'immunity',\n",
       "  'covid',\n",
       "  'bristol',\n",
       "  '19',\n",
       "  'protesters',\n",
       "  'lives',\n",
       "  'coronavirus',\n",
       "  'protest'],\n",
       " 'liberty interactive': ['quarter',\n",
       "  'year',\n",
       "  'think',\n",
       "  'analyst',\n",
       "  'million',\n",
       "  'president',\n",
       "  'chief',\n",
       "  'us',\n",
       "  'officer',\n",
       "  'well',\n",
       "  'business',\n",
       "  'question',\n",
       "  'first',\n",
       "  'executive',\n",
       "  'see',\n",
       "  'thank',\n",
       "  'going',\n",
       "  'call',\n",
       "  'customers',\n",
       "  'financial'],\n",
       " 'mckesson ': ['antibody',\n",
       "  'elliott',\n",
       "  'antibodies',\n",
       "  'alexion',\n",
       "  'plasma',\n",
       "  'said',\n",
       "  'buffett',\n",
       "  'company',\n",
       "  'treatments',\n",
       "  'use',\n",
       "  'companys',\n",
       "  'share',\n",
       "  'price',\n",
       "  'convalescent',\n",
       "  'therapies',\n",
       "  '19',\n",
       "  'patients',\n",
       "  'treatment',\n",
       "  'one',\n",
       "  'letter'],\n",
       " 'microsoft': ['police',\n",
       "  'said',\n",
       "  'president',\n",
       "  'trump',\n",
       "  'quarter',\n",
       "  'house',\n",
       "  'year',\n",
       "  'justices',\n",
       "  'officer',\n",
       "  'records',\n",
       "  'analyst',\n",
       "  'think',\n",
       "  'brueckner',\n",
       "  'financial',\n",
       "  'presidents',\n",
       "  'new',\n",
       "  'would',\n",
       "  'floyd',\n",
       "  'subpoenas',\n",
       "  'justice'],\n",
       " 'netflix': ['gina',\n",
       "  'carano',\n",
       "  'premiere',\n",
       "  'california',\n",
       "  'photo',\n",
       "  'ca',\n",
       "  'los',\n",
       "  'images',\n",
       "  'angeles',\n",
       "  'getty',\n",
       "  'arrives',\n",
       "  'actress',\n",
       "  'film',\n",
       "  'disney',\n",
       "  'customers',\n",
       "  'sports',\n",
       "  'haywire',\n",
       "  'quarter',\n",
       "  'november',\n",
       "  '13'],\n",
       " 'nvidia': ['black',\n",
       "  'says',\n",
       "  'said',\n",
       "  'employees',\n",
       "  'children',\n",
       "  'school',\n",
       "  'schools',\n",
       "  'company',\n",
       "  'lives',\n",
       "  'matter',\n",
       "  'racial',\n",
       "  'social',\n",
       "  'would',\n",
       "  'faison',\n",
       "  'people',\n",
       "  'companies',\n",
       "  'apple',\n",
       "  'government',\n",
       "  'workers',\n",
       "  'public'],\n",
       " 'paypal': ['space',\n",
       "  'tesla',\n",
       "  'launch',\n",
       "  'nasa',\n",
       "  'said',\n",
       "  'spacex',\n",
       "  'musk',\n",
       "  'factory',\n",
       "  'quarter',\n",
       "  'first',\n",
       "  'rocket',\n",
       "  'county',\n",
       "  'astronauts',\n",
       "  'hurley',\n",
       "  'california',\n",
       "  'vehicle',\n",
       "  'plant',\n",
       "  'trump',\n",
       "  'crew',\n",
       "  'behnken'],\n",
       " 'starbucks': ['said',\n",
       "  'police',\n",
       "  'office',\n",
       "  'investors',\n",
       "  'senate',\n",
       "  'year',\n",
       "  'domino',\n",
       "  'house',\n",
       "  'deal',\n",
       "  'peets',\n",
       "  'jde',\n",
       "  'coronavirus',\n",
       "  'county',\n",
       "  'starbucks',\n",
       "  'stores',\n",
       "  'million',\n",
       "  'lawmakers',\n",
       "  'offices',\n",
       "  'costs',\n",
       "  'cases'],\n",
       " 'stericycle': ['quarter',\n",
       "  'year',\n",
       "  'think',\n",
       "  'executive',\n",
       "  'analyst',\n",
       "  'president',\n",
       "  'us',\n",
       "  'see',\n",
       "  'chief',\n",
       "  'officer',\n",
       "  'customers',\n",
       "  'first',\n",
       "  'well',\n",
       "  'question',\n",
       "  'million',\n",
       "  'business',\n",
       "  'kind',\n",
       "  'growth',\n",
       "  'thank',\n",
       "  'products'],\n",
       " 'tesla motors': ['quarter',\n",
       "  'tesla',\n",
       "  'space',\n",
       "  'launch',\n",
       "  'nasa',\n",
       "  'first',\n",
       "  'said',\n",
       "  'year',\n",
       "  'cannabis',\n",
       "  'musk',\n",
       "  'spacex',\n",
       "  'factory',\n",
       "  'products',\n",
       "  'vehicle',\n",
       "  'million',\n",
       "  'rocket',\n",
       "  'officer',\n",
       "  'growth',\n",
       "  'think',\n",
       "  'astronauts'],\n",
       " 'universal display ': ['virus',\n",
       "  'immunity',\n",
       "  'antibodies',\n",
       "  'covid',\n",
       "  '19',\n",
       "  'spending',\n",
       "  'vaccine',\n",
       "  'immune',\n",
       "  'economy',\n",
       "  'test',\n",
       "  'second',\n",
       "  'since',\n",
       "  'april',\n",
       "  'people',\n",
       "  'infection',\n",
       "  'government',\n",
       "  'said',\n",
       "  'consumer',\n",
       "  'month',\n",
       "  'time']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_relavant_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "PATH = \"./data/\"\n",
    "file = \"dict_relavant_words_200k\"\n",
    "a_file = open(PATH + file + \".json\", \"w\")\n",
    "json.dump(dict_relavant_words, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load dictionary \n",
    "PATH = \"./data/\"\n",
    "file = \"dict_relavant_words_200k\"\n",
    "a_file = open(PATH + file + \".json\", \"r\")\n",
    "dict_relavant_words = json.load(a_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "#nltk.download('punkt')\n",
    "import warnings \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import collections\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "  \n",
    "# iterate through each article in the file \n",
    "data = [word_tokenize(plain_text) for plain_text in df_cleaned[\"plain_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "PATH = \"./data/\"\n",
    "file = \"list_tokenized_pt\"\n",
    "with open(PATH +file , 'wb') as fp:\n",
    "    pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/\"\n",
    "file = \"list_tokenized_pt\"\n",
    "with open (PATH +file, 'rb') as fp:\n",
    "    data_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Matrix factorization to get top 20 words of a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "# HAL (Hyper Analogue Language)\n",
    "# CBOW\n",
    "\n",
    "# Create CBOW model \n",
    "model_cbow = Word2Vec(data, min_count = 1, size = 100, window = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/models/\"\n",
    "file = \"CBOW_model_buff\"\n",
    "word_vectors = model_cbow.wv\n",
    "word_vectors.save(PATH + file+\".kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/models/\"\n",
    "file = \"CBOW_model_200k\"\n",
    "word_vectors = KeyedVectors.load(PATH + file+\".kv\", mmap='r')\n",
    "model_cbow_test = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('googles', 0.7780945301055908), ('apple', 0.7284873723983765), ('googl', 0.704851508140564), ('alphabet', 0.6509729623794556), ('spotify', 0.6320379972457886), ('facebook', 0.6315559148788452), ('microsoft', 0.6176584362983704), ('alphabets', 0.6062008142471313), ('apps', 0.605197548866272), ('stadia', 0.5996901988983154)]\n",
      "\n",
      "[('king', 0.8107793927192688), ('godfather', 0.5998413562774658), ('thatcher', 0.5920987129211426), ('mitford', 0.5835937261581421), ('altimus', 0.5723137259483337), ('chemouny', 0.5638600587844849), ('atwood', 0.5631056427955627), ('macbeth', 0.5596096515655518), ('enid', 0.5574297308921814), ('antoinette', 0.5557938814163208)]\n",
      "\n",
      "-0.056162722\n"
     ]
    }
   ],
   "source": [
    "print (model_cbow.most_similar('google'))\n",
    "vec = model_cbow['king'] - model_cbow['man'] + model_cbow['woman']\n",
    "print ()\n",
    "print (model_cbow.most_similar([vec]))\n",
    "print()\n",
    "print(model_cbow.similarity('apple', 'man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('googles', 0.7780945301055908), ('apple', 0.7284873723983765), ('googl', 0.704851508140564), ('alphabet', 0.6509729623794556), ('spotify', 0.6320379972457886), ('facebook', 0.6315559148788452), ('microsoft', 0.6176584362983704), ('alphabets', 0.6062008142471313), ('apps', 0.605197548866272), ('stadia', 0.5996901988983154)]\n"
     ]
    }
   ],
   "source": [
    "print (model_cbow_test.most_similar('google'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local context window methods to get top 20 words on a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip- gram\n",
    "\n",
    "# Create Skip Gram model \n",
    "model_sg = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/models/\"\n",
    "file = \"skip-gram_model_test\"\n",
    "word_vectors = model_cbow.wv\n",
    "word_vectors.save(PATH + file+\".kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/models/\"\n",
    "file = \"skip-gram_model_test\"\n",
    "word_vectors = KeyedVectors.load(PATH + file+\".kv\", mmap='r')\n",
    "model_cbow_test = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aapl', 0.7436023950576782), ('google', 0.7167742252349854), ('iphone', 0.6852249503135681), ('android', 0.6673775911331177), ('microsoft', 0.6640886068344116), ('googl', 0.660868763923645), ('ios', 0.6570302248001099), ('arcade', 0.6384004950523376), ('nflx', 0.6328017711639404), ('iphones', 0.6309572458267212)]\n",
      "\n",
      "[('king', 0.7986758947372437), ('luther', 0.5585871934890747), ('stenhammar', 0.4959256947040558), ('prestney', 0.488489031791687), ('kings', 0.48475682735443115), ('vajiralongkorn', 0.48195749521255493), ('grodensky', 0.47945284843444824), ('wykes', 0.4762948155403137), ('icon', 0.4760589003562927), ('humphris', 0.4751470386981964)]\n",
      "\n",
      "0.3127151\n"
     ]
    }
   ],
   "source": [
    "print (model_sg.most_similar('apple'))\n",
    "vec = model_sg['king'] - model_sg['man'] +model_sg['woman']\n",
    "print ()\n",
    "print (model_sg.most_similar([vec]))\n",
    "print()\n",
    "print(model_sg.similarity('apple', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe to get top 20 words of a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe is a global log-bilinear regression model\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "#glove_input_file = 'glove.txt'\n",
    "#word2vec_output_file = 'word2vec.txt'\n",
    "#glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load('glove-wiki-gigaword-300')\n",
    "#https://github.com/stanfordnlp/GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('iphone', 0.5987042188644409), ('macintosh', 0.5836331248283386), ('ipod', 0.5761123895645142), ('microsoft', 0.5663833022117615), ('ipad', 0.5628098249435425), ('intel', 0.5457563400268555), ('ibm', 0.5286195278167725), ('google', 0.5282472372055054), ('imac', 0.5072520971298218), ('software', 0.4962984323501587)]\n",
      "\n",
      "[('king', 0.8065859079360962), ('queen', 0.689616322517395), ('monarch', 0.5575490593910217), ('throne', 0.5565374493598938), ('princess', 0.5518684387207031), ('mother', 0.5142154693603516), ('daughter', 0.5133156776428223), ('kingdom', 0.5025345087051392), ('prince', 0.5017740726470947), ('elizabeth', 0.49080315232276917)]\n",
      "\n",
      "0.090478964\n"
     ]
    }
   ],
   "source": [
    "print (glove_model.most_similar('apple'))\n",
    "vec = glove_model['king'] - glove_model['man'] +glove_model['woman']\n",
    "print ()\n",
    "print (glove_model.most_similar([vec]))\n",
    "print()\n",
    "print(glove_model.similarity('apple', 'man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195884 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "DIR = \"./data/\"\n",
    "embeddings_index = {}\n",
    "\n",
    "# Pre-trained Glove\n",
    "#if option == 1:\n",
    "with open(os.path.join(DIR, 'glove.840B.300d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.061054"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embeddings_index[\"apple\"],embeddings_index[\"iphone\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, ','.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-01642303a30c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'glove.840B.300d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordEmbeddingsKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastTextKeyedVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'compatible_hash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, ','."
     ]
    }
   ],
   "source": [
    "wv = KeyedVectors.load(\"./data/\" + 'glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------- Annexe Testing -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python program to generate word vectors using Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "#nltk.download('punkt')\n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-ed853e0b9298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# iterate through each article in the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclean_articles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# tokenize the article into words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_articles' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply 2 Word2Vec models to articles   \n",
    "\n",
    "data = [] \n",
    "  \n",
    "# iterate through each article in the file \n",
    "for i in clean_articles: \n",
    "    temp = [] \n",
    "    # tokenize the article into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "\n",
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'australia' \" + \n",
    "               \"and 'melbourne' - CBOW : \", \n",
    "    model1.similarity('melbourne', 'australia')) \n",
    "\n",
    "print(model1.wv.most_similar('melbourne'))\n",
    "    \n",
    "\n",
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 4) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'australia' \" +\n",
    "          \"and 'melbourne' - Skip Gram : \", \n",
    "    model2.similarity('melbourne', 'australia')) \n",
    "print(model2.wv.most_similar('melbourne'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-855081c8fe11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# enumerate data it is trained on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "# FOR GENSIN USING CBOW Manipulations\n",
    "\n",
    "# enumerate data it is trained on\n",
    "for i, word in enumerate(model1.wv.vocab):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "# Show frequencies\n",
    "#print(\"Original List : \",data)\n",
    "data_flat = []\n",
    "for line in data:\n",
    "    for word in line:\n",
    "        data_flat.append(word)\n",
    "\n",
    "\n",
    "ctr = collections.Counter(data_flat)\n",
    "#print(\"Frequency of the elements in the List : \",ctr)\n",
    "ctr[\"the\"] # count of word \"the\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.itf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X = vectorizer.fit_transform(clean_articles)\n",
    "\n",
    "#print(vectorizer.get_feature_names()[:10])\n",
    "#print(X.shape)\n",
    "#print(vectorizer.get_stop_words())\n",
    "#print(vectorizer.get_params(deep=True))\n",
    "\n",
    "n_articles, n_distinct_words = X.shape\n",
    "print(n_articles, n_distinct_words)\n",
    "\n",
    "collect_word_importance = []\n",
    "#place tf-idf values in a pandas data frame \n",
    "for tf_idf_vector_id in range(n_articles):\n",
    "    \n",
    "    tf_idf_vector=X[tf_idf_vector_id]\n",
    "    #print (tf_idf_vector.todense().sum())\n",
    "    #print (tf_idf_vector.T.todense())\n",
    "    df = pd.DataFrame(tf_idf_vector.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df_word_importance = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    word_importance_list = np.array(df_word_importance.index)\n",
    "    collect_word_importance.append(word_importance_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each line corresponds to the highest scored words in the article of same index.\n",
    "collect_word_importance = np.array(collect_word_importance)\n",
    "collect_word_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfTransformer\n",
    "#TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]\n",
    "\n",
    "\n",
    "corpus = ['this is the first document',\n",
    "           'this document is the second document',\n",
    "          'and this is the third one',\n",
    "           'is this the first document']\n",
    "vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n",
    "               'and', 'one']\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()\n",
    "pipe['tfid'].idf_\n",
    "pipe.transform(corpus).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('count', CountVectorizer()),\n",
    "                  ('tfid', TfidfTransformer())]).fit(clean_articles)\n",
    "pipe['count'].transform(clean_articles).toarray().shape\n",
    "print (pipe['tfid'].idf_)\n",
    "Tfidf_res = pipe.transform(clean_articles)\n",
    "Tfidf_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tutorial\n",
    "\n",
    "#Dataset and Imports\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    " \n",
    "# this is a very toy example, do not try this at home unless you want to understand the usage differences \n",
    "docs=[\"the house had a tiny little mouse\", \n",
    "\"the cat saw the mouse\", \n",
    "\"the mouse ran away from the house\", \n",
    "\"the cat finally ate the mouse\", \n",
    "\"the end of the mouse story\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer\n",
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer() \n",
    "# this steps generates word counts for the words in your docs \n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector.shape\n",
    "# 5 texts, 9 distinct words -> gives the count for each word in each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the IDF values\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the TFIDF score for your documents\n",
    "# count matrix \n",
    "count_vector=cv.transform(docs) #<==> word_count_vector\n",
    "\n",
    "# tf-idf scores \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names() \n",
    " \n",
    "#get tfidf vector for FFFFFFFFFirst document \n",
    "first_document_vector=tf_idf_vector[0] \n",
    " \n",
    "#print the scores (Tf-idf scores of first document)\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidfvectorizer Usage - Compute all at Once\n",
    "\n",
    " \n",
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    "#fitted_vectorizer=tfidf_vectorizer.fit(docs)               # This method would work too\n",
    "#tfidf_vectorizer_vectors=fitted_vectorizer.transform(docs)  \n",
    " \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
