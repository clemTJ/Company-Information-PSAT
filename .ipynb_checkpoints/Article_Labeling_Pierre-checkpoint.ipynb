{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Labeling & Lexical Fields Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('./data/20200420_20200714_business_articles.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type <class 'list'>\n",
      "json <class 'dict'>\n",
      "keys dict_keys(['published', 'link', 'message', 'Feed', 'title', '@version', 'author', '@timestamp', 'full-text', 'type'])\n",
      "length 416307\n"
     ]
    }
   ],
   "source": [
    "print (\"data type\",type (data))\n",
    "print (\"json\",type (data[0]))\n",
    "print (\"keys\",data[0].keys())\n",
    "print (\"length\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting full_text of each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = list()\n",
    "min_length = 1000000\n",
    "min_article_size = 2000\n",
    "for article in data:\n",
    "    plain_text = article.get('full-text')\n",
    "    if (plain_text and \"Article `download()` failed\" != plain_text[:27] and \"Please enable cookies\" != plain_text[:21] and len(plain_text)>min_article_size):\n",
    "        articles.append(plain_text)\n",
    "        # Check for small articles\n",
    "        #if len(plain_text)<min_length:\n",
    "        #    min_length = len(plain_text)\n",
    "        #    print(\"*****************\")\n",
    "        #    print (plain_text)\n",
    "        #    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(articles) 215039\n",
      "min_length 1000000\n"
     ]
    }
   ],
   "source": [
    "# 358192 removing \"Article `download()` failed\" \n",
    "# 340987 removing \"Article `download()` failed\" and \"Please enable cookies\"\n",
    "# 215039 removing \"Article `download()` failed\" and \"Please enable cookies\" and size<min_article_size = 2000\n",
    "print (\"len(articles)\",len(articles) )\n",
    "print (\"min_length\",min_length)\n",
    "\n",
    "#for i in range (3001, 3002, 1):\n",
    "#    print (\"article\", i)\n",
    "#    print (\"article\", i, articles[i])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "d.clear()\n",
    "d.get(<key>[, <default>])\n",
    "d.items()\n",
    "d.keys()\n",
    "d.values()\n",
    "d.pop(<key>[, <default>])\n",
    "d.popitem()\n",
    "d.update(<obj>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning full_text of articles\n",
    "# Remove every non-letter character\n",
    "clean_articles = list()\n",
    "n_articles = 2000\n",
    "for article in articles[:n_articles]:\n",
    "    clean_text = article.lower()\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    clean_text = re.sub(\"[^a-z]\", ' ', clean_text)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    clean_articles.append(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliminated masterchef contestant harry foster has hit back at unfair criticism against judge melissa leong the show s first female judge has faced a barrage of trolling with haters taking aim at everything from her behaviour on set to her fashion sense despite being eliminated on tuesday night s episode harry had nothing but good things to say about the melbourne based food writer this could not be further from the truth eliminated masterchef australia contestant harry foster pictured has hit back at unfair criticism against judge melissa leong she s a queen i love her harry told huffpost australia she is energetic passionate and really just vibrant when asked about accusations melissa was rude and biased on the show he said this could not be further from the truth all three judges have received an overwhelmingly positive response from fans but melissa has copped a backlash from a vocal minority she s a queen the show s first female judge has faced a barrage of trolling with haters taking aim at everything from her behaviour on set to her fashion sense while many have praised her for her fashion sense and positivity others claim she waits for feedback from jock zonfrillo and andy allen before repeating it as her own any chance melissa leong has an original idea on masterchef or just continue to wait for others to tell her what to think about the dish one viewer tweeted another added new judge melissa speaks like an affected melbourne millennial and as an affected melbourne millennial myself i am in a good position to note how grating that is mixed all three masterchef judges have received an overwhelmingly positive response from fans but melissa has copped a backlash from a vocal minority kind words she is energetic passionate and really just vibrant said harry left who was eliminated on tuesday night s episode i love masterchef bringing back the old contestants but loathe the new judges especially melissa leong i ve only been watching for ten minutes and she s already irritating a third fan wrote on twitter others claimed she was biased towards certain contestants such as hayden quinn and khanh ong masterchef continues thursday at pm on channel '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python program to generate word vectors using Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "#nltk.download('punkt')\n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'australia' and 'melbourne' - CBOW :  0.9941112\n",
      "[('committee', 0.9950198531150818), ('classrooms', 0.9950098395347595), ('university', 0.9949347376823425), ('seconds', 0.9949148893356323), ('effect', 0.9948941469192505), ('process', 0.994837760925293), ('low', 0.9947916865348816), ('usa', 0.9947860240936279), ('each', 0.9947835803031921), ('chaos', 0.9947378635406494)]\n",
      "Cosine similarity between 'australia' and 'melbourne' - Skip Gram :  0.99408007\n",
      "[('nose', 0.9993689060211182), ('harassed', 0.9993646144866943), ('patients', 0.9993616342544556), ('compounds', 0.9993556141853333), ('professional', 0.9993484020233154), ('operation', 0.9993415474891663), ('reluctance', 0.9993293285369873), ('harsh', 0.9993184208869934), ('almost', 0.9993061423301697), ('fresh', 0.999296247959137)]\n"
     ]
    }
   ],
   "source": [
    "# Apply 2 Word2Vec models to articles   \n",
    "\n",
    "data = [] \n",
    "  \n",
    "# iterate through each article in the file \n",
    "for i in clean_articles: \n",
    "    temp = [] \n",
    "    # tokenize the article into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "\n",
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'australia' \" + \n",
    "               \"and 'melbourne' - CBOW : \", \n",
    "    model1.similarity('melbourne', 'australia')) \n",
    "\n",
    "print(model1.wv.most_similar('melbourne'))\n",
    "    \n",
    "\n",
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 4) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'australia' \" +\n",
    "          \"and 'melbourne' - Skip Gram : \", \n",
    "    model2.similarity('melbourne', 'australia')) \n",
    "print(model2.wv.most_similar('melbourne'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliminated\n",
      "masterchef\n",
      "contestant\n",
      "harry\n",
      "foster\n"
     ]
    }
   ],
   "source": [
    "# FOR GENSIN USING CBOW Manipulations\n",
    "\n",
    "# enumerate data it is trained on\n",
    "for i, word in enumerate(model1.wv.vocab):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4045"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "# Show frequencies\n",
    "#print(\"Original List : \",data)\n",
    "data_flat = []\n",
    "for line in data:\n",
    "    for word in line:\n",
    "        data_flat.append(word)\n",
    "\n",
    "\n",
    "ctr = collections.Counter(data_flat)\n",
    "#print(\"Frequency of the elements in the List : \",ctr)\n",
    "ctr[\"the\"] # count of word \"the\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.itf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 29466\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X = vectorizer.fit_transform(clean_articles)\n",
    "\n",
    "#print(vectorizer.get_feature_names()[:10])\n",
    "#print(X.shape)\n",
    "#print(vectorizer.get_stop_words())\n",
    "#print(vectorizer.get_params(deep=True))\n",
    "\n",
    "n_articles, n_distinct_words = X.shape\n",
    "print(n_articles, n_distinct_words)\n",
    "\n",
    "collect_word_importance = []\n",
    "#place tf-idf values in a pandas data frame \n",
    "for tf_idf_vector_id in range(n_articles):\n",
    "    \n",
    "    tf_idf_vector=X[tf_idf_vector_id]\n",
    "    #print (tf_idf_vector.todense().sum())\n",
    "    #print (tf_idf_vector.T.todense())\n",
    "    df = pd.DataFrame(tf_idf_vector.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df_word_importance = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    word_importance_list = np.array(df_word_importance.index)\n",
    "    collect_word_importance.append(word_importance_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['melissa', 'masterchef', 'leong', ..., 'findlay', 'findings',\n",
       "        'zuocheng'],\n",
       "       ['the', 'burglary', 'bail', ..., 'firmware', 'firms', 'zuocheng'],\n",
       "       ['the', 'to', 'children', ..., 'flaring', 'flareups', 'zuocheng'],\n",
       "       ...,\n",
       "       ['the', 'to', 'in', ..., 'firestorm', 'fires', 'zuocheng'],\n",
       "       ['the', 'to', 'in', ..., 'firestorm', 'fires', 'zuocheng'],\n",
       "       ['the', 'to', 'in', ..., 'firestorm', 'fires', 'zuocheng']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each line corresponds to the highest scored words in the article of same index.\n",
    "collect_word_importance = np.array(collect_word_importance)\n",
    "collect_word_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfTransformer\n",
    "#TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]\n",
    "\n",
    "\n",
    "corpus = ['this is the first document',\n",
    "           'this document is the second document',\n",
    "          'and this is the third one',\n",
    "           'is this the first document']\n",
    "vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n",
    "               'and', 'one']\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()\n",
    "pipe['tfid'].idf_\n",
    "pipe.transform(corpus).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.2156076  7.2156076  7.2156076  ... 6.81014249 7.2156076  6.52246042]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 21395)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('count', CountVectorizer()),\n",
    "                  ('tfid', TfidfTransformer())]).fit(clean_articles)\n",
    "pipe['count'].transform(clean_articles).toarray().shape\n",
    "print (pipe['tfid'].idf_)\n",
    "Tfidf_res = pipe.transform(clean_articles)\n",
    "Tfidf_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x21395 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 334368 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tfidf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tutorial\n",
    "\n",
    "#Dataset and Imports\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    " \n",
    "# this is a very toy example, do not try this at home unless you want to understand the usage differences \n",
    "docs=[\"the house had a tiny little mouse\", \n",
    "\"the cat saw the mouse\", \n",
    "\"the mouse ran away from the house\", \n",
    "\"the cat finally ate the mouse\", \n",
    "\"the end of the mouse story\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer\n",
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer() \n",
    "# this steps generates word counts for the words in your docs \n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector.shape\n",
    "# 5 texts, 9 distinct words -> gives the count for each word in each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute the IDF values\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         idf_weights\n",
       "mouse       1.000000\n",
       "the         1.000000\n",
       "cat         1.693147\n",
       "house       1.693147\n",
       "ate         2.098612\n",
       "away        2.098612\n",
       "end         2.098612\n",
       "finally     2.098612\n",
       "from        2.098612\n",
       "had         2.098612\n",
       "little      2.098612\n",
       "of          2.098612\n",
       "ran         2.098612\n",
       "saw         2.098612\n",
       "story       2.098612\n",
       "tiny        2.098612"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the TFIDF score for your documents\n",
    "# count matrix \n",
    "count_vector=cv.transform(docs) #<==> word_count_vector\n",
    "\n",
    "# tf-idf scores \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x16 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.398203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfidf\n",
       "had      0.493562\n",
       "little   0.493562\n",
       "tiny     0.493562\n",
       "house    0.398203\n",
       "mouse    0.235185\n",
       "the      0.235185\n",
       "ate      0.000000\n",
       "away     0.000000\n",
       "cat      0.000000\n",
       "end      0.000000\n",
       "finally  0.000000\n",
       "from     0.000000\n",
       "of       0.000000\n",
       "ran      0.000000\n",
       "saw      0.000000\n",
       "story    0.000000"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names() \n",
    " \n",
    "#get tfidf vector for FFFFFFFFFirst document \n",
    "first_document_vector=tf_idf_vector[0] \n",
    " \n",
    "#print the scores (Tf-idf scores of first document)\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidfvectorizer Usage - Compute all at Once\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    " \n",
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    "#fitted_vectorizer=tfidf_vectorizer.fit(docs)               # This method would work too\n",
    "#tfidf_vectorizer_vectors=fitted_vectorizer.transform(docs)  \n",
    " \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.398203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfidf\n",
       "had      0.493562\n",
       "little   0.493562\n",
       "tiny     0.493562\n",
       "house    0.398203\n",
       "mouse    0.235185\n",
       "the      0.235185\n",
       "ate      0.000000\n",
       "away     0.000000\n",
       "cat      0.000000\n",
       "end      0.000000\n",
       "finally  0.000000\n",
       "from     0.000000\n",
       "of       0.000000\n",
       "ran      0.000000\n",
       "saw      0.000000\n",
       "story    0.000000"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
