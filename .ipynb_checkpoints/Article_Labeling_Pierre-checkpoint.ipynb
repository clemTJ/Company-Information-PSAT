{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Labeling & Lexical Fields Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "raw_json_data = []\n",
    "with open('./data/20200420_20200714_business_articles.json') as f:\n",
    "    for line in f:\n",
    "        raw_json_data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type <class 'list'>\n",
      "json <class 'dict'>\n",
      "keys dict_keys(['published', 'link', 'message', 'Feed', 'title', '@version', 'author', '@timestamp', 'full-text', 'type'])\n",
      "length 416307\n"
     ]
    }
   ],
   "source": [
    "print (\"data type\",type (raw_json_data))\n",
    "print (\"json\",type (raw_json_data[0]))\n",
    "print (\"keys\",raw_json_data[0].keys())\n",
    "print (\"length\", len(raw_json_data))\n",
    "#print (raw_json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Company Names & Related Names(52 companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['21st century fox', 'activision blizzard', 'adobe ', 'advanced micro devices', 'akamai technologies', 'akamai tecnologies', 'alexion pharmaceuticals', 'amazon', 'american airlines group', 'amgen', 'analog devices', 'apple', 'autodesk', 'automatic data processing', 'baidu', 'bed bath & beyond', 'biogen', 'ca technologies', 'celgene', 'cerner', 'cisco ', 'cognizant', 'comcast', 'discovery communications', 'dish network', 'ebay', 'electronic arts', 'equinix', 'expeditors international', 'facebook', 'alphabet', 'intel', 'liberty global', 'liberty interactive', 'linear technology', 'marriott international', 'mattle', 'mattel', 'mckesson ', 'mckesson', 'microsoft', 'netflix', 'nvidia', 'paypal', 'qualcomm', 'starbucks', 'stericycle', 'tesla motors', 'texas instruments', 'the priceline group', 'universal display ', 'universal display'])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetching company names (52 companies)\n",
    "df = pd.read_excel (r'./data/comapny_name-related_words.xlsx', header = None)\n",
    "# Lower\n",
    "df[0] = [row[0].lower() for index, row in df.iterrows()] \n",
    "# Split company name and related names\n",
    "split = np.array([row[0].split(\";\") for index, row in df.iterrows()])\n",
    "df[\"company_name\"] = split[:,0]\n",
    "df[\"related_name\"] = split[:,1]\n",
    "df.drop(columns = [0], inplace=True)\n",
    "# build dictionary of related name of companies\n",
    "dict_companies = {}\n",
    "company_names = df[\"company_name\"].unique()\n",
    "for name in company_names:\n",
    "    df_tmp = df[df[\"company_name\"] == name]\n",
    "    dict_companies[name] = list(df_tmp[\"related_name\"])\n",
    "dict_companies.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting url, title & full_text of each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list()\n",
    "plain_texts = list()\n",
    "titles = list()\n",
    "labels = list()\n",
    "\n",
    "min_article_size = 2000\n",
    "for article in raw_json_data:\n",
    "    plain_text = article.get('full-text')\n",
    "    title = article.get('title')\n",
    "    url = article.get('link')\n",
    "    if (plain_text and \"Article `download()` failed\" != plain_text[:27] and \"Please enable cookies\" != plain_text[:21] and len(plain_text)>min_article_size):\n",
    "        plain_texts.append(plain_text)\n",
    "        urls.append(url)\n",
    "        titles.append(title)\n",
    "        labels.append(list())\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>plain_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.dailymail.co.uk/tvshowbiz/article-...</td>\n",
       "      <td>MasterChef's Harry Foster hits back at claims ...</td>\n",
       "      <td>Eliminated MasterChef contestant Harry Foster ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.washingtontimes.com/news/2020/jun/...</td>\n",
       "      <td>Protest arrests logjam tests NYC legal system,...</td>\n",
       "      <td>NEW YORK (AP) - A wave of arrests in the New Y...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-83114...</td>\n",
       "      <td>Labour's Anneliese Dodds says she will REFUSE ...</td>\n",
       "      <td>A top shadow minister today said there was not...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://feeds.reuters.com/~r/Reuters/worldNews/...</td>\n",
       "      <td>Civil unrest rages in Minneapolis over raciall...</td>\n",
       "      <td>MINNEAPOLIS (Reuters) - Peaceful rallies gave ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-82734...</td>\n",
       "      <td>Australia 'beats the cr*p' out of coronavirus ...</td>\n",
       "      <td>Australia is 'beating the c**p' out of coronav...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.dailymail.co.uk/tvshowbiz/article-...   \n",
       "1  https://www.washingtontimes.com/news/2020/jun/...   \n",
       "2  https://www.dailymail.co.uk/news/article-83114...   \n",
       "3  http://feeds.reuters.com/~r/Reuters/worldNews/...   \n",
       "4  https://www.dailymail.co.uk/news/article-82734...   \n",
       "\n",
       "                                               title  \\\n",
       "0  MasterChef's Harry Foster hits back at claims ...   \n",
       "1  Protest arrests logjam tests NYC legal system,...   \n",
       "2  Labour's Anneliese Dodds says she will REFUSE ...   \n",
       "3  Civil unrest rages in Minneapolis over raciall...   \n",
       "4  Australia 'beats the cr*p' out of coronavirus ...   \n",
       "\n",
       "                                          plain_text label  \n",
       "0  Eliminated MasterChef contestant Harry Foster ...    []  \n",
       "1  NEW YORK (AP) - A wave of arrests in the New Y...    []  \n",
       "2  A top shadow minister today said there was not...    []  \n",
       "3  MINNEAPOLIS (Reuters) - Peaceful rallies gave ...    []  \n",
       "4  Australia is 'beating the c**p' out of coronav...    []  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Statistics\n",
    "# 358192 removing \"Article `download()` failed\" \n",
    "# 340987 removing \"Article `download()` failed\" and \"Please enable cookies\"\n",
    "# 215039 removing \"Article `download()` failed\" and \"Please enable cookies\" and size<min_article_size = 2000\n",
    "\n",
    "data = np.array([urls,titles, plain_texts, labels]).T\n",
    "columns=[\"url\", \"title\", \"plain_text\", \"label\"]\n",
    "df_articles = pd.DataFrame(data=data, columns=columns)\n",
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning full_text of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliminated masterchef contestant harry foster has hit back at unfair criticism against judge melissa'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove every non-letter/number character\n",
    "#df_cleaned = df_articles.copy(deep= True)\n",
    "df_cleaned = df_articles.head(2000).copy(deep= True)\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    row[\"plain_text\"] = row[\"plain_text\"].lower()\n",
    "    row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "    #[:punct:], ,[^0-9], [^a-z]\n",
    "    #row[\"plain_text\"] = re.sub(\"[^a-z],[^:punct:],[^0-9]\", ' ', row[\"plain_text\"])\n",
    "    row[\"plain_text\"] = re.sub(\"[^a-z0-9]\", ' ', row[\"plain_text\"])\n",
    "    row[\"plain_text\"] = re.sub(r'\\s+', ' ', row[\"plain_text\"])\n",
    "df_cleaned[\"plain_text\"][0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo\n",
    "# remove stop words using CountVectorizer() on corpus OR\n",
    "#from nltk.corpus import stopwords\n",
    "#stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Articles with Company Names \n",
    "### (if their name/related names are present in the article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1990                    [autodesk, autodesk]\n",
       "1991                    [autodesk, autodesk]\n",
       "1992                                      []\n",
       "1993    [apple, microsoft, apple, microsoft]\n",
       "1994                                      []\n",
       "1995                                      []\n",
       "1996                                      []\n",
       "1997                          [intel, intel]\n",
       "1998                          [intel, intel]\n",
       "1999                          [intel, intel]\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in df_cleaned.iterrows():\n",
    "    for company in company_names:\n",
    "        if company in row[\"plain_text\"]:\n",
    "            row['label'].append(company)\n",
    "        else:\n",
    "            for related_name in dict_companies[company]:\n",
    "                if related_name in row[\"plain_text\"]:\n",
    "                    row['label'].append(company)\n",
    "                    break\n",
    "df_cleaned[\"label\"].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of articles that each company is associated to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 36 companies with associated articels over the 52 total companies\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "dict_count = {}\n",
    "for company in company_names: dict_count[company]= 0\n",
    "    \n",
    "for index, row in df_cleaned.iterrows():\n",
    "    for company in company_names:\n",
    "        if company in row[\"label\"]:\n",
    "            dict_count[company]+=1\n",
    "dict_count          \n",
    "\n",
    "companies_w_articles = list()\n",
    "for company in company_names:\n",
    "    if dict_count[company]>0:\n",
    "        companies_w_articles.append(company)\n",
    "print (\"there are %d companies with associated articels over the %d total companies\"%(len(companies_w_articles),len(company_names)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.idf on Companies that are associated to articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all companies in companies_w_articles\n",
    "# concat all articles associated to it\n",
    "# Tf.idf with the remaining articles \n",
    "# Collect the top 20 most important words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python program to generate word vectors using Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "#nltk.download('punkt')\n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'australia' and 'melbourne' - CBOW :  0.66155946\n",
      "[('portland', 0.8813670873641968), ('perez', 0.8812724351882935), ('rigel', 0.8715772032737732), ('heights', 0.8652781844139099), ('jaylen', 0.8628085255622864), ('pool', 0.8600395917892456), ('santa', 0.8597794771194458), ('cincinnati', 0.8594452738761902), ('hollywood', 0.8580102920532227), ('charleston', 0.8569580316543579)]\n",
      "Cosine similarity between 'australia' and 'melbourne' - Skip Gram :  0.5816691\n",
      "[('hollywood', 0.8261724710464478), ('capitan', 0.8180453181266785), ('dga', 0.8162583112716675), ('monica', 0.8151893615722656), ('gods', 0.809572696685791), ('augusta', 0.805156946182251), ('sands', 0.8041570782661438), ('citywest', 0.8036929368972778), ('lutheran', 0.8020343780517578), ('hangar', 0.800697922706604)]\n"
     ]
    }
   ],
   "source": [
    "# Apply 2 Word2Vec models to articles   \n",
    "\n",
    "data = [] \n",
    "  \n",
    "# iterate through each article in the file \n",
    "for i in clean_articles: \n",
    "    temp = [] \n",
    "    # tokenize the article into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "\n",
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'australia' \" + \n",
    "               \"and 'melbourne' - CBOW : \", \n",
    "    model1.similarity('melbourne', 'australia')) \n",
    "\n",
    "print(model1.wv.most_similar('melbourne'))\n",
    "    \n",
    "\n",
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 4) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'australia' \" +\n",
    "          \"and 'melbourne' - Skip Gram : \", \n",
    "    model2.similarity('melbourne', 'australia')) \n",
    "print(model2.wv.most_similar('melbourne'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliminated\n",
      "masterchef\n",
      "contestant\n",
      "harry\n",
      "foster\n"
     ]
    }
   ],
   "source": [
    "# FOR GENSIN USING CBOW Manipulations\n",
    "\n",
    "# enumerate data it is trained on\n",
    "for i, word in enumerate(model1.wv.vocab):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84611"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "# Show frequencies\n",
    "#print(\"Original List : \",data)\n",
    "data_flat = []\n",
    "for line in data:\n",
    "    for word in line:\n",
    "        data_flat.append(word)\n",
    "\n",
    "\n",
    "ctr = collections.Counter(data_flat)\n",
    "#print(\"Frequency of the elements in the List : \",ctr)\n",
    "ctr[\"the\"] # count of word \"the\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.itf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 29466\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X = vectorizer.fit_transform(clean_articles)\n",
    "\n",
    "#print(vectorizer.get_feature_names()[:10])\n",
    "#print(X.shape)\n",
    "#print(vectorizer.get_stop_words())\n",
    "#print(vectorizer.get_params(deep=True))\n",
    "\n",
    "n_articles, n_distinct_words = X.shape\n",
    "print(n_articles, n_distinct_words)\n",
    "\n",
    "collect_word_importance = []\n",
    "#place tf-idf values in a pandas data frame \n",
    "for tf_idf_vector_id in range(n_articles):\n",
    "    \n",
    "    tf_idf_vector=X[tf_idf_vector_id]\n",
    "    #print (tf_idf_vector.todense().sum())\n",
    "    #print (tf_idf_vector.T.todense())\n",
    "    df = pd.DataFrame(tf_idf_vector.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df_word_importance = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    word_importance_list = np.array(df_word_importance.index)\n",
    "    collect_word_importance.append(word_importance_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['melissa', 'masterchef', 'leong', ..., 'findlay', 'findings',\n",
       "        'zuocheng'],\n",
       "       ['the', 'burglary', 'bail', ..., 'firmware', 'firms', 'zuocheng'],\n",
       "       ['the', 'to', 'children', ..., 'flaring', 'flareups', 'zuocheng'],\n",
       "       ...,\n",
       "       ['the', 'to', 'in', ..., 'firestorm', 'fires', 'zuocheng'],\n",
       "       ['the', 'to', 'in', ..., 'firestorm', 'fires', 'zuocheng'],\n",
       "       ['the', 'to', 'in', ..., 'firestorm', 'fires', 'zuocheng']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each line corresponds to the highest scored words in the article of same index.\n",
    "collect_word_importance = np.array(collect_word_importance)\n",
    "collect_word_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfTransformer\n",
    "#TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]\n",
    "\n",
    "\n",
    "corpus = ['this is the first document',\n",
    "           'this document is the second document',\n",
    "          'and this is the third one',\n",
    "           'is this the first document']\n",
    "vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n",
    "               'and', 'one']\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()\n",
    "pipe['tfid'].idf_\n",
    "pipe.transform(corpus).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.21510797 7.90825515 7.90825515 ... 7.90825515 6.99196442 7.21510797]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 29466)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('count', CountVectorizer()),\n",
    "                  ('tfid', TfidfTransformer())]).fit(clean_articles)\n",
    "pipe['count'].transform(clean_articles).toarray().shape\n",
    "print (pipe['tfid'].idf_)\n",
    "Tfidf_res = pipe.transform(clean_articles)\n",
    "Tfidf_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x29466 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 666150 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tfidf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tutorial\n",
    "\n",
    "#Dataset and Imports\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    " \n",
    "# this is a very toy example, do not try this at home unless you want to understand the usage differences \n",
    "docs=[\"the house had a tiny little mouse\", \n",
    "\"the cat saw the mouse\", \n",
    "\"the mouse ran away from the house\", \n",
    "\"the cat finally ate the mouse\", \n",
    "\"the end of the mouse story\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize CountVectorizer\n",
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer() \n",
    "# this steps generates word counts for the words in your docs \n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector.shape\n",
    "# 5 texts, 9 distinct words -> gives the count for each word in each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute the IDF values\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         idf_weights\n",
       "mouse       1.000000\n",
       "the         1.000000\n",
       "cat         1.693147\n",
       "house       1.693147\n",
       "ate         2.098612\n",
       "away        2.098612\n",
       "end         2.098612\n",
       "finally     2.098612\n",
       "from        2.098612\n",
       "had         2.098612\n",
       "little      2.098612\n",
       "of          2.098612\n",
       "ran         2.098612\n",
       "saw         2.098612\n",
       "story       2.098612\n",
       "tiny        2.098612"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the TFIDF score for your documents\n",
    "# count matrix \n",
    "count_vector=cv.transform(docs) #<==> word_count_vector\n",
    "\n",
    "# tf-idf scores \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x16 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.398203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfidf\n",
       "had      0.493562\n",
       "little   0.493562\n",
       "tiny     0.493562\n",
       "house    0.398203\n",
       "mouse    0.235185\n",
       "the      0.235185\n",
       "ate      0.000000\n",
       "away     0.000000\n",
       "cat      0.000000\n",
       "end      0.000000\n",
       "finally  0.000000\n",
       "from     0.000000\n",
       "of       0.000000\n",
       "ran      0.000000\n",
       "saw      0.000000\n",
       "story    0.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names() \n",
    " \n",
    "#get tfidf vector for FFFFFFFFFirst document \n",
    "first_document_vector=tf_idf_vector[0] \n",
    " \n",
    "#print the scores (Tf-idf scores of first document)\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidfvectorizer Usage - Compute all at Once\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    " \n",
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    "#fitted_vectorizer=tfidf_vectorizer.fit(docs)               # This method would work too\n",
    "#tfidf_vectorizer_vectors=fitted_vectorizer.transform(docs)  \n",
    " \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.398203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfidf\n",
       "had      0.493562\n",
       "little   0.493562\n",
       "tiny     0.493562\n",
       "house    0.398203\n",
       "mouse    0.235185\n",
       "the      0.235185\n",
       "ate      0.000000\n",
       "away     0.000000\n",
       "cat      0.000000\n",
       "end      0.000000\n",
       "finally  0.000000\n",
       "from     0.000000\n",
       "of       0.000000\n",
       "ran      0.000000\n",
       "saw      0.000000\n",
       "story    0.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
