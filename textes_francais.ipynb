{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de textes Francais par entreprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation de données avec label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import ast\n",
    "from scipy import stats\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching data\n",
    "PATH = \"./data/ArticleCompany_2020-11-17/\"\n",
    "coprus = \"corpus_check_long_SIREN_UPDATED2\"\n",
    "names = \"siren_name_map_clean\"\n",
    "\n",
    "with open(PATH + names +\".json\") as json_file: \n",
    "    dict_names = json.load(json_file) \n",
    "\n",
    "with open(PATH + coprus +\".json\") as json_file: \n",
    "    corpus_list = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The siren list is: <class 'str'>\n",
      "NOW the type of the siren list is: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Convert labels from string to list\n",
    "print (\"The siren list is:\",type(corpus_list[0][\"siren\"]))\n",
    "for document in corpus_list:\n",
    "    document[\"siren\"] = ast.literal_eval(document[\"siren\"]) # convert list in string format to list\n",
    "    for i in range(len(document[\"siren\"])): # Convert each int siren to string \n",
    "        document[\"siren\"][i] = str(document[\"siren\"][i])\n",
    "print (\"NOW the type of the siren list is:\",type(corpus_list[0][\"siren\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enlever les articles vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of removed article: 29805   \n",
      "Index of removed article: 30158     \n"
     ]
    }
   ],
   "source": [
    "corpus_list_inter = list()\n",
    "for i in range(len(corpus_list)):\n",
    "    if len(corpus_list[i][\"corpus\"])<100: # small enough\n",
    "        text = corpus_list[i][\"corpus\"]\n",
    "        text = re.sub(\"^(\\s+)\", '', text)\n",
    "        if (len(text)>0):\n",
    "            corpus_list_inter.append(corpus_list[i])\n",
    "        else:\n",
    "            print (\"Index of removed article:\",i,corpus_list[i][\"corpus\"])\n",
    "    else:\n",
    "        corpus_list_inter.append(corpus_list[i])\n",
    "corpus_list = corpus_list_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57538 articles in the corpus\n",
      "There are 30178 companies in the list\n"
     ]
    }
   ],
   "source": [
    "print (\"There are\", len(corpus_list), \"articles in the corpus\")\n",
    "print (\"There are\", len(dict_names), \"companies in the list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compter Nombre d'Entreprises sans Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28690 companies with articles out of the 30178 companies\n",
      "There are 1488 companies with no articles\n",
      "95.07 % of the companies have at least one article\n",
      "Each article of the corpus has: dict_keys(['id', 'siren', 'corpus', 'url_article'])\n"
     ]
    }
   ],
   "source": [
    "dict_count = dict()\n",
    "#for company in dict_names.keys(): dict_count[company] = 0\n",
    "for document in corpus_list:\n",
    "    sir_list = document[\"siren\"]\n",
    "    for siren in sir_list:\n",
    "        if len(siren)>10 or len(siren)<4 : # Should not be triggered\n",
    "            print (\"ALERT:\",siren)\n",
    "        if siren in dict_count.keys():\n",
    "            dict_count[siren] +=1\n",
    "        else:\n",
    "            dict_count[siren] = 1\n",
    "print (\"There are\",len(dict_count.keys()),\"companies with articles out of the\", len(dict_names.keys()), \"companies\")\n",
    "print (\"There are\",len(dict_names.keys())-len(dict_count.keys()),\"companies with no articles\")\n",
    "print (round(len(dict_count)/(len(dict_names))*100,2),\"% of the companies have at least one article\")\n",
    "print (\"Each article of the corpus has:\",corpus_list[0].keys())\n",
    "#corpus_list[0][\"corpus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouver les entreprises sans articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['512554767', '435404702', '339875817', '797809845', '393239728', '381413947', '433082070', '799911045']\n"
     ]
    }
   ],
   "source": [
    "dict_no_acticle_companies = dict()\n",
    "for company in dict_names.keys():\n",
    "    if company not in dict_count.keys():\n",
    "        dict_no_acticle_companies[company] = dict_names[company] \n",
    "print (list(dict_no_acticle_companies.keys())[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etude du nombre d'entreprises associé à chaque Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=57538, minmax=(1, 29), mean=1.2476276547672842, variance=0.6042338178020358, skewness=8.133981486117998, kurtosis=136.56851777466036)\n",
      "There are 48176 arcticles with ONLY ONE label out of the 57538 articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of labels per article')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdUElEQVR4nO3df7xVdZ3v8ddbMDUVlUBSII836Yc6pXlCS2fGwkZKR7yNFs6oOFGkY/6YW3fC5j6mrKHReytHx9Qxf4C/UsRKrJxkMHVKAw/+QlCThOSMJEchRR9pgp/7x/ruWmw2hw3fs852H97Px2M/9trfvb5rfb8b3e+zvmvt71JEYGZmtqW2aXUDzMysvTlIzMwsi4PEzMyyOEjMzCyLg8TMzLI4SMzMLIuDxNqKpOmS/rlF+5akqyWtljS/wfunSPpZk9v6iqTrtrAdW1z3jU7S7ZImNbHeMklH9EebbNMcJJYl/Q/9rKQdS2WflnRXC5tVlcOAjwCjImJsqxvT7hoFYkR8NCJmtKpNtmUcJNYXBgNntboRm0vSoM2sshewLCJerqI97ULS4DfCNuyNw0FifeH/AV+QtGv9G5I6JEX5i0PSXZI+nZZPkfRzSRdI+q2kpyR9MJUvl7SywVDHMElzJK2RdLekvUrbfld6b5WkJyR9ovTedEmXSvqxpJeBDzVo756SZqf6SyR9JpVPBq4APiDpJUnnbupDkXRh6sOLkhZI+tO6VbaXdFPqxwOS3lvXjlsk9UhaKunMjexje0nXSXo+fX73SxqxkXWXSTpH0uI0PHe1pO1L7x8t6aG0nXslvaeu7hclPQK83CgIeutvOvqYldr6InAq8CXgk+nzfDit94f/NtLrz0h6LH1GiyW9r8F+t5E0VdKv0ucwU9LQRp+BVcNBYn2hC7gL+MIW1j8YeAR4C3ADcCPwfmAf4ETgYkk7ldb/G+BrwDDgIeB6gDS8NidtY3fgBOASSfuV6v41MA3YGWh0PuO7QDewJ3Ac8HVJ4yLiSoovv/siYqeI+HIT/bofOAAYmtp0c/mLG5gA3Fx6/weStpW0DXAb8DAwEhgHnC3pyAb7mATsAoym+PxOBX7XS5v+BjgSeDvwDuD/AKQv6KuAz6bt/DswW9J2pbonAEcBu0bE2i3s7yxgV+BK4OvATenzfC91JB0PfAU4GRgCHAM832C/ZwLHAn9O8e+2Gvh2L5+B9TEHifWVfwLOkDR8C+oujYirI2IdcBPFl+JXI+LViLgD+D1FqNT8KCLuiYhXgX+kOEoYDRxNMfR0dUSsjYgHgFsoAqHm1oj4eUS8HhGvlBuRtnEY8MWIeCUiHqI4CjlpC/pERFwXEc+ntnwT2A54Z2mVBRExKyJeA74FbA8cQhGiwyPiqxHx+4h4CvgOMLHBbl6j+OLfJyLWRcSCiHixl2ZdHBHLI2IVRaCekMo/A/x7RMxL25kBvJraU3NRqtswqJro730R8YP02fcWdjWfBv5vRNwfhSUR8esG630W+MeI6E7/TXwFOM7DZ/3HH7T1iYh4VNIPganAY5tZ/dnS8u/S9urLykcky0v7fUnSKoq/RPcCDpb029K6g4FrG9VtYE9gVUSsKZX9Guhsog8bkPR5ii/DPYGg+Kt6WKO2RMTrkrpL6+5Z149BwH812M21FMF7YxpavI7iS/W1jTSr3P9fp/1B8dlNknRG6f03ld6vr7uBzelvk0YDv2pivb2A70t6vVS2DhgB/Pdm7tO2gIPE+tKXgQeAb5bKaiem3wzU/lJ+a+Z+RtcW0pDXUOAZii+quyPiI73U7W2662eAoZJ2LoXJ29iCL6N0fuCLFMNSi1JQrAa0kX5sA4xKbVhLcZQ2ZlP7SYFxLnCupA7gx8ATFENHjYwuLb8t7Q+Kz25aREzrbXcbe6PJ/tbX39TU48sphuA2ZTnwqYj4eRPrWgU8tGV9JiKWUAxNnVkq66H4Ij5R0iBJn6K5L4fefEzSYZLeRHGuZF5ELAd+CLxD0knpXMO2kt4v6d1Ntn85cC/wL+kk9nuAyaRzMJtpZ4pA6AEGS/onir/Qyw6S9PE0BHM2xVDSL4D5wIvp5PYO6XPbX9L763ci6UOS/kTFFWgvUgx1reulXadLGpVORn+J4t8LiqGzUyUdrMKOko6StHMf9rfes0BHCtFGrqC4iOOg1KZ9VLqwouQyYFrtPUnDJU1ost3WBxwk1te+CuxYV/YZ4H9TnCjdj+LLOscNFEc/q4CDKE4gk44i/oLiXMIzwG+A8ynG6pt1AtCR6n8f+HJEzNmCNv4EuB34JcUQ0itsOLRzK/BJipPDJwEfj4jX0rmiv6Q4cb0UeI7iS3WXBvt5K8UJ7BcphhTvphje2pgbgDuAp9LjnwEioovi3+ni1J4lwCnNd7ep/ta7OT0/L+mB+jcj4maK8zg3AGuAH1Acfda7EJgN3CFpDUUYH7wZbbdM8o2tzLYOkpYBn46I/2x1W2xg8RGJmZllcZCYmVkWD22ZmVkWH5GYmVmWSn9Hkk7uraG4HHFtRHSmyw5vorgyZhnwiYhYndY/h+Jyy3XAmRHxk1R+EDAd2IHiOvmzIiLS9A3XUFy58zzwyYhY1lubhg0bFh0dHX3ZTTOzAW/BggXPRUTDmSv64weJH4qI50qvpwJzI+I8SVPT6y9K2pfiss39KH4Z+5+S3pEuhbwUmEJxWd+PgfEUlxpOBlZHxD6SJlJc6vnJ3hrT0dFBV1dX3/bQzGyAk9RoehqgNUNbE4Da/QZmUEy2Viu/Mc2vtJTiOvaxkvYAhkTEfVGc0Lmmrk5tW7OAcZLKv6Q1M7OKVR0kQfEjoQWSpqSyERGxAiA9757KR7L+D5i6U9nItFxfvl6dNBvpCxQT2K1H0hRJXZK6enp6+qRjZmZWqHpo69CIeEbS7sAcSY/3sm6jI4nopby3OusXRFwOXA7Q2dnpy9TMzPpQpUckEfFMel5JMd3EWODZNFxFel6ZVu9m/QnlahPYdafl+vL16qT5inahmDbDzMz6SWVBkiZ927m2TDEH0qMUc+LU7ng3iWK+IVL5REnbSdobGAPMT8NfayQdks5/nFxXp7at44A7wz+MMTPrV1UObY2guEdAbT83RMR/SLofmKni1qVPA8cDRMQiSTOBxRSziJ6ertgCOI0/Xv57e3pAMVX2tZKWUByJNLrxj5mZVWir+2V7Z2dn+PJfM7PNI2lBRDS8yZt/2W5mZlkcJGZmlsW32m0THVN/1JL9LjvvqJbs18zah49IzMwsi4PEzMyyOEjMzCyLg8TMzLI4SMzMLIuDxMzMsjhIzMwsi4PEzMyyOEjMzCyLg8TMzLI4SMzMLIuDxMzMsjhIzMwsi4PEzMyyOEjMzCyLg8TMzLI4SMzMLIuDxMzMsjhIzMwsi4PEzMyyOEjMzCyLg8TMzLI4SMzMLIuDxMzMsjhIzMwsi4PEzMyyOEjMzCyLg8TMzLI4SMzMLIuDxMzMsjhIzMwsS+VBImmQpAcl/TC9HippjqQn0/NupXXPkbRE0hOSjiyVHyRpYXrvIklK5dtJuimVz5PUUXV/zMxsff1xRHIW8Fjp9VRgbkSMAeam10jaF5gI7AeMBy6RNCjVuRSYAoxJj/GpfDKwOiL2AS4Azq+2K2ZmVq/SIJE0CjgKuKJUPAGYkZZnAMeWym+MiFcjYimwBBgraQ9gSETcFxEBXFNXp7atWcC42tGKmZn1j6qPSP4V+Afg9VLZiIhYAZCed0/lI4HlpfW6U9nItFxfvl6diFgLvAC8pb4RkqZI6pLU1dPTk9klMzMrqyxIJB0NrIyIBc1WaVAWvZT3Vmf9gojLI6IzIjqHDx/eZHPMzKwZgyvc9qHAMZI+BmwPDJF0HfCspD0iYkUatlqZ1u8GRpfqjwKeSeWjGpSX63RLGgzsAqyqqkNmZrahyo5IIuKciBgVER0UJ9HvjIgTgdnApLTaJODWtDwbmJiuxNqb4qT6/DT8tUbSIen8x8l1dWrbOi7tY4MjEjMzq06VRyQbcx4wU9Jk4GngeICIWCRpJrAYWAucHhHrUp3TgOnADsDt6QFwJXCtpCUURyIT+6sTZmZW6JcgiYi7gLvS8vPAuI2sNw2Y1qC8C9i/QfkrpCAyM7PW8C/bzcwsi4PEzMyyOEjMzCyLg8TMzLI4SMzMLIuDxMzMsjhIzMwsi4PEzMyyOEjMzCyLg8TMzLI4SMzMLIuDxMzMsjhIzMwsi4PEzMyyOEjMzCyLg8TMzLI4SMzMLIuDxMzMsjhIzMwsi4PEzMyyOEjMzCyLg8TMzLI4SMzMLIuDxMzMsjhIzMwsi4PEzMyyOEjMzCyLg8TMzLI4SMzMLIuDxMzMsjhIzMwsi4PEzMyyOEjMzCyLg8TMzLJUFiSStpc0X9LDkhZJOjeVD5U0R9KT6Xm3Up1zJC2R9ISkI0vlB0lamN67SJJS+XaSbkrl8yR1VNUfMzNrrMojkleBD0fEe4EDgPGSDgGmAnMjYgwwN71G0r7ARGA/YDxwiaRBaVuXAlOAMekxPpVPBlZHxD7ABcD5FfbHzMwaqCxIovBSerltegQwAZiRymcAx6blCcCNEfFqRCwFlgBjJe0BDImI+yIigGvq6tS2NQsYVztaMTOz/lHpORJJgyQ9BKwE5kTEPGBERKwASM+7p9VHAstL1btT2ci0XF++Xp2IWAu8ALylQTumSOqS1NXT09NHvTMzM6g4SCJiXUQcAIyiOLrYv5fVGx1JRC/lvdWpb8flEdEZEZ3Dhw/fRKvNzGxz9MtVWxHxW+AuinMbz6bhKtLzyrRaNzC6VG0U8EwqH9WgfL06kgYDuwCrquiDmZk1VuVVW8Ml7ZqWdwCOAB4HZgOT0mqTgFvT8mxgYroSa2+Kk+rz0/DXGkmHpPMfJ9fVqW3rOODOdB7FzMz6yeAKt70HMCNdebUNMDMifijpPmCmpMnA08DxABGxSNJMYDGwFjg9ItalbZ0GTAd2AG5PD4ArgWslLaE4EplYYX/MzKyByoIkIh4BDmxQ/jwwbiN1pgHTGpR3ARucX4mIV0hBZGZmreFftpuZWRYHiZmZZXGQmJlZlqaCRNLcZsrMzGzr0+vJdknbA28GhqXJFWs/ABwC7Flx28zMrA1s6qqtzwJnU4TGAv4YJC8C366uWWZm1i56DZKIuBC4UNIZEfFv/dQmMzNrI039jiQi/k3SB4GOcp2IuKaidpmZWZtoKkgkXQu8HXgIqP3avDalu5mZbcWa/WV7J7Cv57EyM7N6zf6O5FHgrVU2xMzM2lOzRyTDgMWS5lPcQheAiDimklaZmVnbaDZIvlJlI8zMrH01e9XW3VU3xMzM2lOzV22t4Y+3sH0TsC3wckQMqaphZmbWHpo9Itm5/FrSscDYKhpkZmbtZYtm/42IHwAf7tummJlZO2p2aOvjpZfbUPyuxL8pMTOzpq/a+svS8lpgGTChz1tjZmZtp9lzJH9bdUPMzKw9NXtjq1GSvi9ppaRnJd0iaVTVjTMzsze+Zk+2Xw3MprgvyUjgtlRmZmZbuWaDZHhEXB0Ra9NjOjC8wnaZmVmbaDZInpN0oqRB6XEi8HyVDTMzs/bQbJB8CvgE8BtgBXAc4BPwZmbW9OW/XwMmRcRqAElDgW9QBIyZmW3Fmj0ieU8tRAAiYhVwYDVNMjOzdtJskGwjabfai3RE0uzRjJmZDWDNhsE3gXslzaKYGuUTwLTKWmVmZm2j2V+2XyOpi2KiRgEfj4jFlbbMzMzaQtPDUyk4HB5mZraeLZpG3szMrMZBYmZmWRwkZmaWpbIgkTRa0k8lPSZpkaSzUvlQSXMkPZmey5cVnyNpiaQnJB1ZKj9I0sL03kWSlMq3k3RTKp8nqaOq/piZWWNVHpGsBT4fEe8GDgFOl7QvMBWYGxFjgLnpNem9icB+wHjgEkmD0rYuBaYAY9JjfCqfDKyOiH2AC4DzK+yPmZk1UFmQRMSKiHggLa8BHqOYgn4CMCOtNgM4Ni1PAG6MiFcjYimwBBgraQ9gSETcFxEBXFNXp7atWcC42tGKmZn1j345R5KGnA4E5gEjImIFFGED7J5WGwksL1XrTmUj03J9+Xp1ImIt8ALwlgb7nyKpS1JXT09PH/XKzMygH4JE0k7ALcDZEfFib6s2KIteynurs35BxOUR0RkRncOH+zYqZmZ9qdIgkbQtRYhcHxHfS8XPpuEq0vPKVN4NjC5VHwU8k8pHNShfr46kwcAuwKq+74mZmW1MlVdtCbgSeCwivlV6azYwKS1PAm4tlU9MV2LtTXFSfX4a/loj6ZC0zZPr6tS2dRxwZzqPYmZm/aTKGXwPBU4CFkp6KJV9CTgPmClpMvA0cDxARCySNJNiGpa1wOkRsS7VOw2YDuwA3J4eUATVtZKWUByJTKywP2Zm1kBlQRIRP6PxOQyAcRupM40GswpHRBewf4PyV0hBZGZmreFftpuZWRYHiZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZmWVxkJiZWRYHiZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZmWVxkJiZWRYHiZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZmWVxkJiZWRYHiZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZmWVxkJiZWRYHiZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZmWVxkJiZWRYHiZmZZXGQmJlZFgeJmZllqSxIJF0laaWkR0tlQyXNkfRket6t9N45kpZIekLSkaXygyQtTO9dJEmpfDtJN6XyeZI6quqLmZltXJVHJNOB8XVlU4G5ETEGmJteI2lfYCKwX6pziaRBqc6lwBRgTHrUtjkZWB0R+wAXAOdX1hMzM9uoyoIkIu4BVtUVTwBmpOUZwLGl8hsj4tWIWAosAcZK2gMYEhH3RUQA19TVqW1rFjCudrRiZmb9p7/PkYyIiBUA6Xn3VD4SWF5arzuVjUzL9eXr1YmItcALwFsa7VTSFEldkrp6enr6qCtmZgZvnJPtjY4kopfy3upsWBhxeUR0RkTn8OHDt7CJZmbWSH8HybNpuIr0vDKVdwOjS+uNAp5J5aMalK9XR9JgYBc2HEozM7OK9XeQzAYmpeVJwK2l8onpSqy9KU6qz0/DX2skHZLOf5xcV6e2reOAO9N5FDMz60eDq9qwpO8ChwPDJHUDXwbOA2ZKmgw8DRwPEBGLJM0EFgNrgdMjYl3a1GkUV4DtANyeHgBXAtdKWkJxJDKxqr6YmdnGVRYkEXHCRt4at5H1pwHTGpR3Afs3KH+FFERmZtY6b5ST7WZm1qYcJGZmlsVBYmZmWRwkZmaWxUFiZmZZHCRmZpbFQWJmZlkq+x2JDQwdU3/Usn0vO++olu3bzJrnIxIzM8viIDEzsywOEjMzy+IgMTOzLA4SMzPL4iAxM7MsDhIzM8viIDEzsywOEjMzy+IgMTOzLA4SMzPL4iAxM7MsDhIzM8viIDEzsywOEjMzy+IgMTOzLA4SMzPL4iAxM7MsDhIzM8viIDEzsywOEjMzy+IgMTOzLINb3QCzjemY+qOW7HfZeUe1ZL9m7cpHJGZmlsVBYmZmWRwkZmaWpe2DRNJ4SU9IWiJpaqvbY2a2tWnrk+2SBgHfBj4CdAP3S5odEYtb2zJrZz7Jb7Z52jpIgLHAkoh4CkDSjcAEwEFibadVAdZKDs+Bod2DZCSwvPS6Gzi4fiVJU4Ap6eVLkp7Ywv0NA57bwrrtyn3eOrSkzzq/v/e4Hv87b569NvZGuweJGpTFBgURlwOXZ+9M6oqIztzttBP3eevgPm8dqupzu59s7wZGl16PAp5pUVvMzLZK7R4k9wNjJO0t6U3ARGB2i9tkZrZVaeuhrYhYK+lzwE+AQcBVEbGowl1mD4+1Ifd56+A+bx0q6bMiNjilYGZm1rR2H9oyM7MWc5CYmVkWB0mTtrapWCSNlvRTSY9JWiTprFa3qT9IGiTpQUk/bHVb+oOkXSXNkvR4+rf+QKvbVDVJf5/+m35U0nclbd/qNvU1SVdJWinp0VLZUElzJD2Znnfrq/05SJpQmorlo8C+wAmS9m1tqyq3Fvh8RLwbOAQ4fSvoM8BZwGOtbkQ/uhD4j4h4F/BeBnjfJY0EzgQ6I2J/iot0Jra2VZWYDoyvK5sKzI2IMcDc9LpPOEia84epWCLi90BtKpYBKyJWRMQDaXkNxRfMyNa2qlqSRgFHAVe0ui39QdIQ4M+AKwEi4vcR8duWNqp/DAZ2kDQYeDMD8LdnEXEPsKqueAIwIy3PAI7tq/05SJrTaCqWAf2lWiapAzgQmNfiplTtX4F/AF5vcTv6y/8AeoCr03DeFZJ2bHWjqhQR/w18A3gaWAG8EBF3tLZV/WZERKyA4g9FYPe+2rCDpDlNTcUyEEnaCbgFODsiXmx1e6oi6WhgZUQsaHVb+tFg4H3ApRFxIPAyfTjc8UaUzgtMAPYG9gR2lHRia1vV/hwkzdkqp2KRtC1FiFwfEd9rdXsqdihwjKRlFEOXH5Z0XWubVLluoDsiakeasyiCZSA7AlgaET0R8RrwPeCDLW5Tf3lW0h4A6XllX23YQdKcrW4qFkmiGDt/LCK+1er2VC0izomIURHRQfHve2dEDOi/VCPiN8BySe9MReMY+LdgeBo4RNKb03/j4xjgFxiUzAYmpeVJwK19teG2niKlv7RgKpY3gkOBk4CFkh5KZV+KiB+3rklWgTOA69MfSE8Bf9vi9lQqIuZJmgU8QHFl4oMMwKlSJH0XOBwYJqkb+DJwHjBT0mSKQD2+z/bnKVLMzCyHh7bMzCyLg8TMzLI4SMzMLIuDxMzMsjhIzMwsi4PErETSXZI6+2E/Z6bZdq+vel99SdIxW8Ps17Z5/DsSsz4iaXBErG1y9b8DPhoRS6tsU1+LiNkM8B/j2ubzEYm1HUkd6a/576T7StwhaYf03h+OKCQNS1OeIOkUST+QdJukpZI+J+l/pckKfyFpaGkXJ0q6N92vYmyqv2O6x8P9qc6E0nZvlnQbsMHkf2kfj6bH2ansMooJE2dL+vu69QdJ+oakhZIekXRGKh+X9rswtWO7VL5M0tcl3SepS9L7JP1E0q8knZrWOVzSPZK+L2mxpMskbZPeuzTVWyTp3FI7lkk6V9IDaZ/vKvX34rQ8XNIt6TO5X9KhqfzPJT2UHg9K2jnn39vaQET44UdbPYAOil8lH5BezwROTMt3UdxrAmAYsCwtnwIsAXYGhgMvAKem9y6gmJSyVv87afnPgEfT8tdL+9gV+CWwY9puNzC0QTsPAham9XYCFgEHpveWAcMa1DmNYn6zwen1UGB7itmn35HKrim1dxlwWqkfj5T6uDKVHw68QhFeg4A5wHG17afnQanv7ylt94y0/HfAFaXP8eK0fANwWFp+G8V0OgC3AYem5Z1qffFj4D58RGLtamlEPJSWF1CEy6b8NCLWREQPRZDclsoX1tX/Lvzhng5DJO0K/AUwNU0XcxfFl/vb0vpzIqL+3g8AhwHfj4iXI+IligkC/3QTbTwCuCzSEFna7jsp+vvLtM4MipCrqQ01LQTmlfr4Smo7wPwo7qezLvXvsFT+CUkPUEwVsh/FjdtqahN1buzzPQK4OH0msyk+q52BnwPfknQmsGs0P9xnbcrnSKxdvVpaXgfskJbX8sch2/pbqJbrvF56/Trr/79QP29QUNxK4K8i4onyG5IOpph+vZFGtx/YFDXY/6a2U+5HfR9r/dqgT5L2Br4AvD8iVkuazvqfWW1b62j8XbEN8IGI+F1d+XmSfgR8DPiFpCMi4vFN9MHamI9IbKBZRjGkBHDcFm7jkwCSDqO48dELFBN2npFmjEXSgU1s5x7g2DTT7I7A/wT+axN17gBOVXH3PtK5m8eBDkn7pHVOAu7ezD6NTbNXb0PRv58BQyhC8AVJIyhuJb057gA+V3sh6YD0/PaIWBgR5wNdwLs2c7vWZhwkNtB8AzhN0r0U50i2xOpU/zJgcir7GrAt8IikR9PrXkVxq+LpwHyKu0teEREPbqLaFRQzsz4i6WHgryPiFYpZeW+WtJDiSOOyzezTfRSzvz4KLKUYcnuYYkhrEXAVxZDU5jgT6EwXBSwGTk3lZ6eLCx4GfgfcvpnbtTbj2X/NBjhJhwNfiIijW9wUG6B8RGJmZll8RGJmZll8RGJmZlkcJGZmlsVBYmZmWRwkZmaWxUFiZmZZ/j8V7ZL2SxFTwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "multiple_siren_list = list()\n",
    "for document in corpus_list:\n",
    "    if len(document[\"siren\"])==0:\n",
    "        print (\"ALERT article sans label, id:\",document[\"id\"])\n",
    "    multiple_siren_list.append(len(document[\"siren\"]))\n",
    "    \n",
    "print(stats.describe(multiple_siren_list))   \n",
    "print (\"There are\",multiple_siren_list.count(1),\"arcticles with ONLY ONE label out of the\",len(corpus_list),\"articles\") \n",
    "\n",
    "plt.hist(multiple_siren_list,range =(0,10), bins=10) #bins=20\n",
    "plt.xlabel('number of companies')\n",
    "plt.ylabel('count')\n",
    "plt.title('Number of labels per article')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etudes du nombre d'Articles associé à chaque Entreprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=28690, minmax=(1, 175), mean=2.5021261763680727, variance=28.017371476985655, skewness=11.347634082651822, kurtosis=211.56847613512954)\n",
      "There are 63.58 % of companies with ONLY ONE associated article\n",
      "There are 96.35 % of companies with LESS THAN 10 associated articles\n",
      "There are 1046 companies with more than 10 associated articles\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On prendre seulement les entreprises avec au moins un articles associer\n",
    "\n",
    "values = list(dict_count.values())\n",
    "plt.hist(values,range =(0,20), bins=20) #bins=20\n",
    "plt.xlabel('associated articles')\n",
    "plt.ylabel('count')\n",
    "plt.title('Number of articles per company')\n",
    "\n",
    "number = 10\n",
    "print(stats.describe(values))\n",
    "print (\"There are\",round(values.count(1)/len(values)*100,2), \"% of companies with ONLY ONE associated article\")\n",
    "under_n = [1 for i in values if i < number]\n",
    "print (\"There are\",round(len(under_n)/len(values)*100,2), \"% of companies with LESS THAN\",number,\"associated articles\")\n",
    "print (\"There are\",len(values)-len(under_n), \"companies with more than\",number,\"associated articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize et suppression de stop words du corpus (Méthode 1) 3:30 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words 157\n",
      "Ex: ['au', 'aux', 'avec', 'ce', 'ces']\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of stop words\",len(stop_words ))\n",
    "print (\"Ex:\",stop_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57538/57538 [04:01<00:00, 238.22it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_cleaned = deepcopy(corpus_list)\n",
    "for document in tqdm(corpus_cleaned):\n",
    "    plain_text = document[\"corpus\"]\n",
    "    plain_text = plain_text.lower()\n",
    "    plain_text= re.sub(r'\\s+', ' ', plain_text)\n",
    "    #plain_text = re.sub(\"[^a-z0-9]\", ' ', plain_text)\n",
    "    plain_text = re.sub(\"[^a-z]\", ' ', plain_text)\n",
    "    plain_text = re.sub(r'\\s+', ' ', plain_text)\n",
    "    #remove one letter words?\n",
    "    #remove numbers?\n",
    "    pt_words = word_tokenize(plain_text)\n",
    "    cleaned_words =list()\n",
    "    for word in pt_words:\n",
    "        if len(word)>1:\n",
    "            if word not in stop_words:\n",
    "                cleaned_words.append(word)\n",
    "    document[\"corpus\"] = cleaned_words\n",
    "# 100%|██████████| 57540/57540 [03:30<00:00, 273.74it/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of number of words in articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5709 articles with LESS than 20 nouns\n",
      "There are 3919 articles with MORE than 500 nouns\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_count = list()\n",
    "for document in corpus_cleaned:\n",
    "    word_count.append(len(document[\"corpus\"]))\n",
    "max_count = 100\n",
    "plt.hist(word_count,range =(0,max_count), bins=20) #bins=20\n",
    "plt.xlabel('NOUN count')\n",
    "plt.ylabel('number of articles')\n",
    "plt.title('number of NOUNS per article')\n",
    "under = 20\n",
    "over = 500\n",
    "n_under = len([count for count in word_count if count<under])\n",
    "n_over = len([count for count in word_count if count>over])\n",
    "print (\"There are\",n_under,\"articles with LESS than\",under,\"nouns\")\n",
    "print (\"There are\",n_over,\"articles with MORE than\",over,\"nouns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize en gardant que les Noms (Méthode 2) 20h"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#pip install stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp # Lemma doesn't work\n",
    "#stanfordnlp.download('fr')   # This downloads the French models for the neural pipeline\n",
    "#nlp = stanfordnlp.Pipeline(lang=\"fr\",processors = \"tokenize,mwt,lemma,pos\") # This sets up a default neural pipeline in French\n",
    "nlp = stanfordnlp.Pipeline(lang=\"fr\",processors = \"tokenize,pos\")\n",
    "#Documentation:\n",
    "#https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple\n",
    "text = 'Les victoires de Joe Biden à la présidentielle américaine à peine proclamée par les principaux médias américains.'\n",
    "doc = nlp(text)  \n",
    "#extract_pos(doc)\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(word.text,\":\", word.upos, word.pos)\n",
    "doc.sentences[0].words[0] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Building corpus_nouns\n",
    "keep = [\"NOUN\",\"PROPN\"]\n",
    "corpus_nouns = deepcopy(corpus_list)\n",
    "for document in tqdm(corpus_nouns):\n",
    "\n",
    "    #document = corpus_nouns[i]\n",
    "    plain_text = document[\"corpus\"]\n",
    "    doc = nlp(plain_text)\n",
    "    cleaned_words =list()\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            pos_tag = word.upos\n",
    "            if pos_tag in keep:\n",
    "                cleaned_words.append(word.text.lower())\n",
    "    document[\"corpus\"] = cleaned_words \n",
    "#20hours of computation to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57538"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_nouns)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save corpus_nouns\n",
    "PATH = \"./data/ArticleCompany_2020-11-17/\"\n",
    "file = \"corpus_nouns\"\n",
    "a_file = open(PATH + file + \".json\", \"w\")\n",
    "json.dump(corpus_nouns, a_file)\n",
    "a_file.close()\n",
    "print (file,\"is saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus_nouns \n",
    "PATH = \"./data/ArticleCompany_2020-11-17/\"\n",
    "file = \"corpus_nouns\"\n",
    "with open(PATH + file +\".json\") as json_file: \n",
    "    corpus_nouns = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The siren list is: <class 'list'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print (\"The siren list is:\",type(corpus_nouns[0][\"siren\"]), type(corpus_nouns[0][\"corpus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction d'entreprises avec n ou plus articles asocciés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLE FRANCE\n",
      "322120916 APPLE FRANCE\n",
      "APPLE FRANCE a 7 articles dans le corpus\n"
     ]
    }
   ],
   "source": [
    "#Exemple de count pour une entreprise donnée\n",
    "#print(list(dict_names.keys())[0:5])\n",
    "print (dict_names['322120916'])\n",
    "name_search = \"APPLE FRANCE\"\n",
    "for siren, name in dict_names.items():  #fetch siren of company name\n",
    "    if name_search in name:\n",
    "        print(siren, name)\n",
    "print(name_search,\"a\",dict_count[\"322120916\"],\"articles dans le corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2084 companies with MORE than 10 associated articles out of the 30178 initial companies\n"
     ]
    }
   ],
   "source": [
    "n_associated_articles = 6 # Number of articles a company must have to be kept in the list\n",
    "siren_filtered =[key for key in dict_count if dict_count[key] >= n_associated_articles]\n",
    "print (\"There are\",len(siren_filtered),\"companies with MORE than\",number,\"associated articles out of the\",len(dict_names.keys()),\"initial companies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation de Train et Test set pour l'entrainement de Tf.Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing unwanted articles\n",
      "We removed: 29189 articles and we have 28349 left\n",
      "Splitting data\n",
      "We have 19844 documents in the training corpus\n",
      "We have 8505 documents in the testing corpus\n"
     ]
    }
   ],
   "source": [
    "# Remove all of the articles that dont talk about our selected companies (in siren filtered)\n",
    "# Split corpus train/test\n",
    "#corpus = corpus_cleaned\n",
    "corpus = corpus_nouns\n",
    "test_size = 0.3\n",
    "X_train_corpus = list()\n",
    "X_test_corpus = list()\n",
    "\n",
    "#Removing unwanted articles\n",
    "print(\"Removing unwanted articles\")\n",
    "corpus_temp = list()\n",
    "for document in corpus:\n",
    "    keep = False\n",
    "    for document_sirens in document[\"siren\"]:\n",
    "        for sirens in siren_filtered:\n",
    "            if document_sirens == sirens:\n",
    "                keep = True\n",
    "    if keep:\n",
    "        corpus_temp.append(document)\n",
    "print (\"We removed:\",len(corpus)-len(corpus_temp),\"articles and we have\",len(corpus_temp),\"left\")\n",
    "corpus = corpus_temp\n",
    " \n",
    "#Splitting data\n",
    "print(\"Splitting data\") \n",
    "#for document in corpus:\n",
    "#    if (random.uniform(0, 1)<test_size):\n",
    "#        X_test_corpus.append(document)\n",
    "#    else:\n",
    "#        X_train_corpus.append(document)\n",
    "X_train_corpus, X_test_corpus = train_test_split(corpus, test_size=test_size, random_state=0)\n",
    "\n",
    "print (\"We have\",len(X_train_corpus),\"documents in the training corpus\")\n",
    "print (\"We have\",len(X_test_corpus),\"documents in the testing corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf.Idf pour une liste d'entreprise sur le training data (1h20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2084/2084 [1:55:05<00:00,  3.31s/it]   \n"
     ]
    }
   ],
   "source": [
    "# Tf.Idf on Companies that have Associated Articles \n",
    "\n",
    "relevant_words_tfidf = {}\n",
    "corpus = X_train_corpus # corpus\n",
    "\n",
    "list_siren = siren_filtered\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "for siren in tqdm(list_siren):\n",
    "    #siren = \"322120916\" #APPLE FRANCE\n",
    "    plain_text_list = list()\n",
    "    company_article = list()\n",
    "    #binary = True\n",
    "    #sublinear_tf=False\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, ngram_range = (1,1), lowercase=False, sublinear_tf=True)\n",
    "    #tfidf_vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, ngram_range = (1,1), lowercase=False, sublinear_tf=False)\n",
    "    for document in corpus:\n",
    "        if siren in document[\"siren\"]:\n",
    "            company_article = company_article+document[\"corpus\"]  # add article to company BIG article\n",
    "        else:\n",
    "            plain_text_list.append(document[\"corpus\"]) # otherwise add to corpus\n",
    "\n",
    "    plain_text_list.insert(0,company_article) # add company article to begging of corpus\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(plain_text_list)\n",
    "\n",
    "    #Get the tf-idf scores for the words in the company article complication.\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] # discard tf.idf scores for the other texts\n",
    "\n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df = df.sort_values(by=[\"tfidf\"],ascending=False).head(40) # Take top 40 words\n",
    "\n",
    "    relevant_words_tfidf[siren] = list(zip(list(df.index),list(df[\"tfidf\"])))\n",
    "    #print (relevant_words_tfidf[company])\n",
    "\n",
    "\n",
    "#100%|██████████| 2084/2084 [2:28:45<00:00,  4.28s/it] # tokenized tf\n",
    "#100%|██████████| 2084/2084 [2:03:31<00:00,  3.56s/it] # tokenized binary\n",
    "#100%|██████████| 2084/2084 [3:06:54<00:00,  5.38s/it]   # tokenized sublinear_tf\n",
    "#100%|██████████| 2084/2084 [1:21:23<00:00,  2.34s/it] # nouns sublinear_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_words_tfidf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save dictionary\n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_tfidf_nouns_sublinear_tf\"\n",
    "a_file = open(PATH + file + \".json\", \"w\")\n",
    "json.dump(relevant_words_tfidf, a_file)\n",
    "a_file.close()\n",
    "print (file,\"is saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_words_tfidf_tokenize_sublinear_tf is loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# load dictionary \n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_tfidf_tokenize_sublinear_tf\"\n",
    "a_file = open(PATH + file + \".json\", \"r\")\n",
    "relevant_words_tfidf = json.load(a_file)\n",
    "#relevant_words_tfidf = dict(relevant_words_tfidf)\n",
    "# check if well loaded\n",
    "print (file,\"is loaded successfully\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ipsen', 0.13919225922627243],\n",
       " ['meek', 0.11910951051014622],\n",
       " ['onivyde', 0.10173082172319803],\n",
       " ['lebeaut', 0.09891742419373598],\n",
       " ['merrimack', 0.0896765377510199],\n",
       " ['probi', 0.0896765377510199],\n",
       " ['medecine', 0.08433141519724636],\n",
       " ['somatuline', 0.08382002820896443],\n",
       " ['oncologie', 0.07668327571700699],\n",
       " ['hennion', 0.07629149171357667]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_words_tfidf[list(relevant_words_tfidf.keys())[0]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant_words_tfidf.keys()\n",
    "#relevant_words_tfidf['419838529']\n",
    "#type(relevant_words_tfidf)\n",
    "#len(relevant_words_tfidf.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Relevant Words from ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_words_2 is loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# load dictionary \n",
    "PATH = \"./relevant_words/francais/\"\n",
    "#file = \"relevant_words_train\"\n",
    "file = \"relevant_words_2\"\n",
    "a_file = open(PATH + file + \".json\", \"r\")\n",
    "relevant_words_es = json.load(a_file)\n",
    "\n",
    "# check if well loaded\n",
    "print (file,\"is loaded successfully\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 companies in the ES RWords out of the 2084 words in the article labels\n"
     ]
    }
   ],
   "source": [
    "# Collect only the relevant words for the sirens that we want to consider\n",
    "count = 0\n",
    "for siren in siren_filtered:\n",
    "    if siren in relevant_words_es.keys():\n",
    "        count +=1\n",
    "print (\"There are\", count, \"companies in the ES RWords out of the\",len(siren_filtered), \"words in the article labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 non empty companies in ES RWords out of the 2084 words in the article labels\n"
     ]
    }
   ],
   "source": [
    "# Removing all the empty significant words and keeping only siren_filtered\n",
    "relevant_words_es_clean = dict()\n",
    "for siren in siren_filtered:\n",
    "    if siren in relevant_words_es.keys():\n",
    "        if len(relevant_words_es[siren])>0:\n",
    "            relevant_words_es_clean[siren] = relevant_words_es[siren]\n",
    "len(relevant_words_es_clean.keys())\n",
    "print (\"There are\", len(relevant_words_es_clean.keys()), \"non empty companies in ES RWords out of the\",len(siren_filtered), \"words in the article labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building baseline model - To FIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labeling article if it has the company name in it\n",
    "relevant_words_baseline = dict()\n",
    "for key in dict_names.keys():\n",
    "    #print (key)\n",
    "    #print([dict_names[key].lower()])\n",
    "    relevant_words_baseline[key] = [[dict_names[key].lower(),1]]\n",
    "#relevant_words_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports ADD ALL the NECESSARY imports\n",
    "import operator\n",
    "import string \n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleClassifier:\n",
    "\n",
    "    def __init__(self,n_sig_words=3,min_score=0.5,t=250):\n",
    "        # HyperParameters\n",
    "        self.n_sig_words=n_sig_words\n",
    "        self.min_score = min_score\n",
    "        self.t = t\n",
    "        self.epsilon = 0.0001\n",
    "        self.rounding = 3 # Number of significant numbers\n",
    "        self.related_words = {}\n",
    "        self.article_label_set = list()\n",
    "        # Predition attributes\n",
    "        self.pred_eval = list() # Tag each prediction 1:correct, 0:wrong for each article\n",
    "        self.pred_labels = list() # Siren predicted for each article\n",
    "        self.article_eval = list() # Tag each label if 1:predicted, 0:not predicted for each article\n",
    "        self.article_labels = list() # Siren labels for each article\n",
    "        self.pred_labels_flat = list() #list all predicted sirens flattened\n",
    "        self.article_labels_flat = list() # list of all siren labels flattened\n",
    "        # Evaluation attributes\n",
    "        self.score1 = 0       \n",
    "        self.score2 = 0\n",
    "        self.score3 = 0       \n",
    "        self.score4 = 0         \n",
    "        self.avg_n_pred = 0       \n",
    "        self.avg_n_labels = 0        \n",
    "        self.most_commun_label = 0\n",
    "        # Company evaluation\n",
    "        self.company_positive = list()\n",
    "        self.company_accuracy_list = list()\n",
    "        self.company_precision_list = list()\n",
    "        self.company_recall_list = list()\n",
    "        self.company_F1score_list = list()\n",
    "        # Article evaluation\n",
    "        self.alpha_eval_list = list()\n",
    "        self.article_recall_list = list()\n",
    "        self.article_precision_list = list()\n",
    "        self.alpha = 1   # penalizes errors if >1 hides errors if <1\n",
    "        self.beta = 0.25 # weight for the missed labels (False Negative)\n",
    "        self.gamma = 1   # weight for the wrongly predicted (False positives)\n",
    "        \n",
    "    #################################################################\n",
    "    # Adds scored relevant words to the model\n",
    "    #Input  : relevant word dictionary\n",
    "    #Output : Text removing all punctuation and lowercased\n",
    "    #################################################################\n",
    "    def fit(self, relevant_word_dict):\n",
    "        self.related_words = relevant_word_dict\n",
    "        self.article_label_set = list(relevant_word_dict.keys())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #################### PREDICTION FUNCTIONS #################################################################\n",
    "    \n",
    "    #################################################################\n",
    "    # CLEANING PLAIN TEXT (use on un tockenize, uncleaned text)\n",
    "    # Input  : Plain text - String\n",
    "    # Output : Text removing all punctuation and lowercased\n",
    "    #################################################################\n",
    "    def clean_plain_text(self,text):\n",
    "        text = text.lower() # lower\n",
    "        text = text.translate(str.maketrans(\"\",\"\", string.punctuation)) # removing punctuation\n",
    "        text = re.sub(r'»|«|–|…', '', text)  # suprime guillmets \n",
    "        text = re.sub(r'è|é|ê|ë|ē|ė|ę', 'e', text)  # suprime accents sur le e\n",
    "        text = re.sub(r'à|á|â|ä|æ|ã|å|ā', 'a', text)  # suprime accents sur le a\n",
    "        text = re.sub(r'\\s+', ' ', text) # remove everything else\n",
    "        text = word_tokenize(text)\n",
    "        cleaned_text = list()\n",
    "        for word in text:\n",
    "            if len(word)>1:\n",
    "                if word not in stop_words:\n",
    "                    cleaned_text.append(word)\n",
    "        text = cleaned_text\n",
    "        return text\n",
    "\n",
    "    \n",
    "    \n",
    "    #################################################################\n",
    "    # Gives a companies \"related score\" wrt an article (using it's significant words)\n",
    "    #INPUT :plain_text- String/ word_list - list of significant words\n",
    "    #OUTPUT: Score the chances the company is related to the article\n",
    "    #################################################################\n",
    "    def company_relevance_score(self,plain_text,sig_words_list): \n",
    "        sig_words = np.array(sig_words_list)[:,0]\n",
    "        sig_words_score = np.array(sig_words_list)[:,1]\n",
    "        sum_exp = np.sum([np.exp(float(score)) for score in sig_words_score]) # denominator for computing the soft max\n",
    "        n_words = len(plain_text)\n",
    "\n",
    "        words_in_text = 0\n",
    "        for i in range(len(sig_words_list)):\n",
    "            word_soft_max = np.exp(float(sig_words_score[i]))/sum_exp\n",
    "            words_in_text += word_soft_max*plain_text.count(sig_words[i])\n",
    "        \n",
    "        return words_in_text/n_words+self.epsilon # relevance score for the company\n",
    "    \n",
    "    \n",
    "    \n",
    "    #################################################################\n",
    "    # For an Article, gives the \"related scores\"(likeness of being a label) for all companies\n",
    "    #INPUT :plain_text- String/company related words - dict/ params\n",
    "    #OUTPUT: dict of companies and their \"related scores\"\n",
    "    #################################################################\n",
    "    def text_label_scores(self,plain_text):\n",
    "        label_dict = {}\n",
    "        label_dict_res = {}\n",
    "        for siren in self.related_words.keys():\n",
    "            sig_words_list = np.array(self.related_words[siren])[:self.n_sig_words] # Build significant word list (with no scores)\n",
    "            score = self.company_relevance_score(plain_text, sig_words_list)\n",
    "            score = 1 - 1/(1 + self.t*score) # smooth relevant scores\n",
    "            label_dict[siren]= score\n",
    "\n",
    "        label_dict = {k: v for k, v in sorted(label_dict.items(), key=lambda item: -item[1])} # sort all companies wrt score\n",
    "\n",
    "        for label in label_dict.keys():\n",
    "            if label_dict[label]>=self.min_score:\n",
    "                label_dict_res[label] = label_dict[label]\n",
    "        if label_dict_res == {}:\n",
    "            label_dict_res[list(label_dict.keys())[0]] = label_dict[list(label_dict.keys())[0]]\n",
    "\n",
    "        return label_dict_res #relevance score for each company\n",
    "    \n",
    "    \n",
    "    \n",
    "    #################################################################\n",
    "    # For an Article, predicts the labels (sirens)\n",
    "    #INPUT : plain_text- String/company related words - dict/ params\n",
    "    #OUTPUT: dict of companies and their \"related scores\"\n",
    "    #################################################################\n",
    "    def label_text(self,plain_text):\n",
    "        label_dict = self.text_label_scores(plain_text)\n",
    "        sirens = list(label_dict.keys())\n",
    "        return sirens # No limitation of the number of labels \n",
    "    \n",
    "    \n",
    "    #################################################################\n",
    "    # Predict le labels of a given corpus wrt. the given hyper parameters\n",
    "    #INPUT : Corpus, hyper parameters\n",
    "    #OUTPUT: Predicted companies for each article\n",
    "    #################################################################\n",
    "    def predict(self,corpus):\n",
    "        for document in tqdm(corpus):\n",
    "            plain_text = document[\"corpus\"]\n",
    "            \n",
    "            #self.pred_labels\n",
    "            pred_sirens = self.label_text(plain_text)\n",
    "            self.pred_labels.append(pred_sirens)\n",
    "            #self.pred_labels_flat\n",
    "            self.pred_labels_flat += pred_sirens\n",
    "\n",
    "            #self.article_labels\n",
    "            true_sirens =document[\"siren\"]\n",
    "            self.article_labels.append(true_sirens)\n",
    "            #self.article_labels_flat\n",
    "            self.article_labels_flat +=true_sirens\n",
    "\n",
    "            #self.pred_eval \n",
    "            is_labeled = [0]*len(pred_sirens)\n",
    "            for i in range(len(pred_sirens)):  # For each predicted company\n",
    "                for label in true_sirens: # For each labeled company\n",
    "                    if pred_sirens[i]==label:  # Tag if it is a good or bad predictions\n",
    "                        is_labeled[i]=1\n",
    "            self.pred_eval.append(is_labeled) \n",
    "\n",
    "            #self.article_eval\n",
    "            is_predicted = [0]*len(true_sirens)\n",
    "            for i in range(len(true_sirens)):  # For each label list\n",
    "                for pred in pred_sirens:       # For each prediction on the articel\n",
    "                    if true_sirens[i]==pred:    # Tag the labels that have been predicted\n",
    "                        is_predicted[i]=1\n",
    "            self.article_eval.append(is_predicted)\n",
    "        return self.pred_labels\n",
    "    \n",
    "    #################### MODEL EVALUATION FUNCTIONS ###########################################################\n",
    "    \n",
    "    #################################################################\n",
    "    # Generate scores for evaluating the model\n",
    "    # INPUT: \n",
    "    # OUTPUT: \n",
    "    #################################################################\n",
    "    def evaluate(self):\n",
    "        ########## How many times (at least) one of the companies is predicted ##########\n",
    "        acc1 = list()\n",
    "        for preds in self.pred_eval:\n",
    "            acc1.append(any(preds))\n",
    "        self.score1 = round(np.sum(acc1)/len(self.pred_eval),self.rounding)\n",
    "\n",
    "        ########## How many times ALL the labels are present in the prediction. ##########\n",
    "        acc2 = list()\n",
    "        for labels in self.article_eval:\n",
    "            acc2.append(labels.count(1)== len (labels)) \n",
    "        self.score2 = round(np.sum(acc2)/len(self.pred_eval),self.rounding)\n",
    "\n",
    "        ########## How many times ALL labels are predicted in the FIRST predictions. ##########\n",
    "        acc3 = list()\n",
    "        for i in range(len(self.pred_eval)):\n",
    "            labels = self.article_eval[i]\n",
    "            preds = self.pred_eval[i]\n",
    "            acc3.append(preds[:len(labels)].count(1)== len(labels))\n",
    "        self.score3 =  round(np.sum(acc3)/len(self.pred_eval),self.rounding)\n",
    "\n",
    "        ########## How many predictions are wrong wrt. how many are right (TRUE, FALSE) ##########\n",
    "        true_pred = 0\n",
    "        pred = 0\n",
    "        for preds in self.pred_eval:\n",
    "            true_pred += np.sum(preds)\n",
    "            pred += len(preds)\n",
    "        self.score4 = round(true_pred/pred,self.rounding)\n",
    "\n",
    "        ########## Average number of predictions vs average number of labels ##########\n",
    "        len_label = list()\n",
    "        len_pred = list()\n",
    "        for i in range(len(self.pred_eval)):\n",
    "            len_label.append(len(self.article_labels[i]))\n",
    "            len_pred.append(len(self.pred_eval[i]))\n",
    "        self.avg_n_pred = round(np.mean(len_pred),self.rounding)\n",
    "        self.avg_n_labels = round(np.mean(len_label),self.rounding)\n",
    "\n",
    "        ########## Most commun labels predicted ##########\n",
    "        count_pred = dict()\n",
    "        for siren in self.article_labels_flat:\n",
    "            if siren in count_pred.keys():\n",
    "                count_pred[siren] +=1\n",
    "            else:\n",
    "                count_pred[siren] = 1\n",
    "        key_max = list(filter(lambda t: t[1]==max(count_pred.values()), count_pred.items()))[0][0] \n",
    "        self.most_commun_label = [key_max,np.max(list(count_pred.values()))]\n",
    "        \n",
    "        ########## Precision & RECALL ########## per siren(Company)\n",
    "\n",
    "        for siren in self.article_label_set: # For each company compute it's TP,FP,TN,FN\n",
    "            true_pos = 0.0  # Siren IS a label and is predicted\n",
    "            false_pos = 0.0 # Siren is NOT a label and is predicted (false prediction)\n",
    "            true_neg = 0.0  # Siren is NOT a label and is not predicted (don't care)\n",
    "            false_neg = 0.0 # Siren IS a label and is NOT predicted\n",
    "            positive = 0.0  # Siren is label\n",
    "\n",
    "            # true_pos, false_neg\n",
    "            for i in range(len(self.article_labels)):\n",
    "                for j in range(len(self.article_labels[i])):\n",
    "                    if siren==self.article_labels[i][j]: # If company in the list of labels -> Check if was predicted\n",
    "                        positive +=1\n",
    "                        if self.article_eval[i][j]==1:\n",
    "                            true_pos +=1\n",
    "                        else:\n",
    "                            false_neg +=1\n",
    "\n",
    "            # false_pos\n",
    "            for i in range(len(self.pred_labels)):\n",
    "                for j in range(len(self.pred_labels[i])):\n",
    "                    if siren==self.pred_labels[i][j]:  # If company in the list of predictions -> Check if was a label (correct prediction)\n",
    "                        if self.pred_eval[i][j]==0: \n",
    "                            false_pos += 1 \n",
    "\n",
    "            if siren in list(set(self.article_labels_flat)): # Add to stats only if the company was part of the labels to predict\n",
    "                if true_pos ==0:\n",
    "                    precision = 0\n",
    "                    recall =0\n",
    "                    accuracy = 0\n",
    "                    F1score = 0\n",
    "                else:\n",
    "                    accuracy = true_pos/positive\n",
    "                    precision = true_pos/(true_pos+false_pos)\n",
    "                    recall = true_pos/(true_pos+false_neg)\n",
    "                    F1score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "                self.company_positive.append(positive)\n",
    "                self.company_accuracy_list.append(accuracy)\n",
    "                self.company_precision_list.append(precision)\n",
    "                self.company_recall_list.append(recall)\n",
    "                self.company_F1score_list.append(F1score)\n",
    "\n",
    "        ########## Precision & RECALL ########## per Article      \n",
    "        #alpha_eval_list\n",
    "        for i in range(len(self.article_labels)):\n",
    "            alpha_eval = pow((1-((self.beta*self.article_eval[i].count(0) + self.gamma*self.pred_eval[i].count(0))/(len(set(self.pred_labels[i]+self.article_labels[i]))))),self.alpha) \n",
    "            self.alpha_eval_list.append(alpha_eval)\n",
    "        #article_recall_list\n",
    "        for label in self.article_eval:\n",
    "            self.article_recall_list.append(label.count(1)/(len(label)+self.epsilon))\n",
    "        #article_precision_list\n",
    "        for pred in pred_eval:\n",
    "             self.article_precision_list.append(pred.count(1)/(len(pred)+self.epsilon))\n",
    "    \n",
    "    def print_eval(self):\n",
    "        \n",
    "        print(\"Score 1:\", self.score1,\"(with at least ONE label predicted)\")\n",
    "        print(\"Score 2:\", self.score2,\"(with ALL labels predicted)\")\n",
    "        print(\"Score 3:\", self.score3,\"(with ALL labels predicted in the FIRST predictions)\")\n",
    "        print(\"Score 4:\",self.score4,\"(Number of correct predictions over total number of predictions overall)\")\n",
    "        print(\"Average number of predictions\",self.avg_n_pred,\"vs average number of labels :\", self.avg_n_labels)\n",
    "        print(\"The siren that is predicted the most is:\",self.most_commun_label[0],\"(\",self.most_commun_label[1],\"times)\")\n",
    "        print()\n",
    "        print(\"######################### For Each company (Labeled at least Once) #########################\")\n",
    "        print(\"AVG ACCURACY :\",round(np.average(self.company_accuracy_list),self.rounding),\"True_pos/Pos -> average for each siren\")\n",
    "        print(\"AVG PRECISION:\",round(np.average(self.company_precision_list),self.rounding),\"True_pos/(True_Pos + False_Pos) -> average for each siren\")\n",
    "        print(\"AVG RECALL   :\",round(np.average(self.company_recall_list),self.rounding),\"True_pos/(True_Pos + False_Neg) -> average for each siren\")\n",
    "        print(\"AVG F1 score :\",round(np.average(self.company_F1score_list),self.rounding),\"combination of precision and recall -> average for each siren\")\n",
    "        print()\n",
    "        print(\"######################### For Each article #########################\")\n",
    "        print(\"AVG PRECISION:\",round(np.average(self.article_precision_list) ,self.rounding),\"#correct_predictions/#predictions-> average for each article\")\n",
    "        print(\"AVG RECALL   :\",round(np.average(self.article_recall_list),self.rounding),\"#predicted_labels/#labels -> average for each article\")\n",
    "        print(\"AVG alpha eval:\",round(np.average(self.alpha_eval_list),self.rounding),\"prediction score of an article -> average for each article\")\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Créé en 2008 par deux anciens salariés d’une agence de publicité, Konbini est un site d’infotainement (infodivertissement),\n",
      "mêlant informations et divertissement. Accessibles gratuitement, ces contenus sont financés par de la publicité apparente\n",
      "(les fameuses bannières cliquables et parfaitement identifiables) et de la publicité plus discrète, appelée \n",
      "publi-rédactionnel et « native advertising ». En raison des systèmes de blocage de publicité, comme Adblocks, \n",
      "les sites ont de plus en plus recours à ces publicités discrètes. \n",
      "\n",
      "\n",
      "['cree', '2008', 'deux', 'anciens', 'salaries', 'agence', 'publicite', 'konbini', 'site', 'infotainement', 'infodivertissement', 'melant', 'informations', 'divertissement', 'accessibles', 'gratuitement', 'contenus', 'finances', 'publicite', 'apparente', 'fameuses', 'bannieres', 'cliquables', 'parfaitement', 'identifiables', 'publicite', 'plus', 'discrete', 'appelee', 'publiredactionnel', 'native', 'advertising', 'raison', 'systemes', 'blocage', 'publicite', 'comme', 'adblocks', 'sites', 'plus', 'plus', 'recours', 'publicites', 'discretes']\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE of clean_plain_text of ArticleClassifier\n",
    "plain_text = \"\"\"\n",
    "Créé en 2008 par deux anciens salariés d’une agence de publicité, Konbini est un site d’infotainement (infodivertissement),\n",
    "mêlant informations et divertissement. Accessibles gratuitement, ces contenus sont financés par de la publicité apparente\n",
    "(les fameuses bannières cliquables et parfaitement identifiables) et de la publicité plus discrète, appelée \n",
    "publi-rédactionnel et « native advertising ». En raison des systèmes de blocage de publicité, comme Adblocks, \n",
    "les sites ont de plus en plus recours à ces publicités discrètes. \n",
    "\"\"\"\n",
    "#plain_text = \". ? ! , ; : … ( ) [ ] « » – / {} ...\"\n",
    "\n",
    "#index = 3\n",
    "#plain_text = corpus_list[index][\"corpus\"]\n",
    "#print (dict_names[corpus_list[index][\"siren\"][0]])\n",
    "print(plain_text)\n",
    "# Building model\n",
    "ac_model = ArticleClassifier() # Init Article Classifier \n",
    "plain_text = ac_model.clean_plain_text(plain_text)\n",
    "print()\n",
    "print(plain_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to compute a \"Related Scores\" for a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plain_text ['cree', '2008', 'deux', 'anciens', 'salaries', 'agence', 'publicite', 'konbini', 'site', 'infotainement', 'infodivertissement', 'melant', 'informations', 'divertissement', 'accessibles', 'gratuitement', 'contenus', 'finances', 'publicite', 'apparente', 'fameuses', 'bannieres', 'cliquables', 'parfaitement', 'identifiables', 'publicite', 'plus', 'discrete', 'appelee', 'publiredactionnel', 'native', 'advertising', 'raison', 'systemes', 'blocage', 'publicite', 'comme', 'adblocks', 'sites', 'plus', 'plus', 'recours', 'publicites', 'discretes']\n",
      "KONBINI\n",
      "n_significant_words: 1\n",
      "related_words:\n",
      " [['konbini' '0.1586134258507446']\n",
      " ['creuzot' '0.11478969771678144']]\n",
      "related score 0.011712594976069761\n",
      "smoothed related score 0.7454271553430869\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE OF : company_relevance_score \n",
    "plain_text = \"\"\"\n",
    "Créé en 2008 par deux anciens salariés d’une agence de publicité, Konbini est un site d’infotainement (infodivertissement),\n",
    "mêlant informations et divertissement. Accessibles gratuitement, ces contenus sont financés par de la publicité apparente\n",
    "(les fameuses bannières cliquables et parfaitement identifiables) et de la publicité plus discrète, appelée \n",
    "publi-rédactionnel et « native advertising ». En raison des systèmes de blocage de publicité, comme Adblocks, \n",
    "les sites ont de plus en plus recours à ces publicités discrètes. \n",
    "\"\"\"\n",
    "min_score = 0.7\n",
    "n_sig_words = 2\n",
    "t = 250\n",
    "ac_model = ArticleClassifier(n_sig_words,min_score ,t) # Init Article Classifier \n",
    "ac_model.fit(relevant_words_tfidf)    # fit related words\n",
    "\n",
    "plain_text = ac_model.clean_plain_text(plain_text)\n",
    "print (\"plain_text\",plain_text)\n",
    "n_sig_words = 1\n",
    "related_words = np.array(ac_model.related_words['502220056'])[:ac_model.n_sig_words] # filter significant words\n",
    "print (dict_names['502220056'])\n",
    "print(\"n_significant_words:\",n_significant_words)\n",
    "print (\"related_words:\\n\",related_words)\n",
    "score = ac_model.company_relevance_score(plain_text, related_words) \n",
    "print (\"related score\",score)\n",
    "score = 1 - 1/(1 + ac_model.t*score) \n",
    "print (\"smoothed related score\",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to compute and compare \"Related Scores\" for each company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing text_label_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plain_text ['cree', '2008', 'deux', 'anciens', 'salaries', 'agence', 'publicite', 'konbini', 'site', 'infotainement', 'infodivertissement', 'melant', 'informations', 'divertissement', 'accessibles', 'gratuitement', 'contenus', 'finances', 'publicite', 'apparente', 'fameuses', 'bannieres', 'cliquables', 'parfaitement', 'identifiables', 'publicite', 'plus', 'discrete', 'appelee', 'publiredactionnel', 'native', 'advertising', 'raison', 'systemes', 'blocage', 'publicite', 'comme', 'adblocks', 'sites', 'plus', 'plus', 'recours', 'publicites', 'discretes']\n",
      "params:\n",
      "n_sig_words: 2\n",
      "min_score  : 0.5\n",
      "t          : 250\n",
      "companies kept {'502220056': 0.7454271553430869, '537450140': 0.733964333270321}\n",
      "The predicted companies are:\n",
      "502220056 KONBINI\n",
      "537450140 INVIBES ADVERTISING\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE of : text_label_scores\n",
    "plain_text = \"\"\"\n",
    "La victoire de Joe Biden à la présidentielle américaine à peine proclamée par les principaux \n",
    "médias américains, les messages de félicitations des dirigeants occidentaux affluent. Sur Twitter,\n",
    "une courte séquence vidéo fait le buzz entre Londres et Dublin. Ce 7 novembre, on y voit le \n",
    "candidat démocrate entouré de journalistes.\n",
    "\"\"\"\n",
    "plain_text2 = \"\"\"\n",
    "Créé en 2008 par deux anciens salariés d’une agence de publicité, Konbini est un site d’infotainement (infodivertissement),\n",
    "mêlant informations et divertissement. Accessibles gratuitement, ces contenus sont financés par de la publicité apparente\n",
    "(les fameuses bannières cliquables et parfaitement identifiables) et de la publicité plus discrète, appelée \n",
    "publi-rédactionnel et « native advertising ». En raison des systèmes de blocage de publicité, comme Adblocks, \n",
    "les sites ont de plus en plus recours à ces publicités discrètes. \n",
    "\"\"\"\n",
    "plain_text3 = \"Bourse en ligne : Information boursiere, Economie, Finance, Bourse de paris - Cerclefinance\"\n",
    "\n",
    "related_words = relevant_words_tfidf\n",
    "\n",
    "n_sig_words= 2\n",
    "min_score = 0.5 # nbr of sig words in text\n",
    "t =250 # score smoothing facter: 0+: monte doucement vers 1, +inf: monte rapidement vers 1\n",
    "\n",
    "ac_model = ArticleClassifier(n_sig_words,min_score ,t) # Init Article Classifier \n",
    "ac_model.fit(related_words)    # fit related words\n",
    "\n",
    "plain_text = ac_model.clean_plain_text(plain_text2)\n",
    "print(\"plain_text\",plain_text)\n",
    "print (\"params:\")\n",
    "\n",
    "print (\"n_sig_words:\",ac_model.n_sig_words)\n",
    "print (\"min_score  :\",ac_model.min_score)\n",
    "print (\"t          :\",ac_model.t)\n",
    "\n",
    "label_dict = ac_model.text_label_scores(plain_text)\n",
    "print(\"companies kept\",label_dict)\n",
    "\n",
    "for key in label_dict.keys(): # Should not trigger\n",
    "    if len(key)>10:\n",
    "        print (key, \"error! this should not be triggered\")\n",
    "\n",
    "print (\"The predicted companies are:\")\n",
    "for key in label_dict.keys():\n",
    "    print(key, dict_names[key])\n",
    "#relevant_words_tfidf['537450140']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING the MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2801/8505 [3:15:59<35:58,  2.64it/s]      /opt/anaconda3/envs/P-SAT/lib/python3.7/site-packages/ipykernel_launcher.py:93: RuntimeWarning: invalid value encountered in double_scalars\n",
      "100%|██████████| 8505/8505 [4:14:14<00:00,  1.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1847 distinct labels predicted out of the 2084 total filtered labels\n",
      "There are 3755 distinct labels TO predicted out of the 2084 total filtered labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE -> Evaluating of a Test corpus\n",
    "\n",
    "related_words = relevant_words_tfidf #model related words\n",
    "#related_words = relevant_words_es_clean #relevant_words_es\n",
    "#related_words = relevant_words_baseline\n",
    "\n",
    "#corpus = X_train_corpus[:1000] # pour verifier que on peut sur entrainer\n",
    "corpus = X_test_corpus[:] # pour tester sur de nouveaux articles\n",
    "min_score = 0.7\n",
    "n_sig_words = 2\n",
    "t = 250\n",
    "\n",
    "ac_model = ArticleClassifier(n_sig_words,min_score ,t) # Init Article Classifier \n",
    "ac_model.fit(related_words)    # fit related words\n",
    "predictions = ac_model.predict(corpus) # evaluate corpus\n",
    "print (\"There are\",len(set(ac_model.pred_labels_flat)),\"distinct labels predicted out of the\",len(related_words.keys()),\"total filtered labels\")\n",
    "print (\"There are\",len(set(ac_model.article_labels_flat)),\"distinct labels TO predicted out of the\",len(related_words.keys()),\"total filtered labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2801/8505 runtime warning invalid value encounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 1: 0.769 (with at least ONE label predicted)\n",
      "Score 2: 0.658 (with ALL labels predicted)\n",
      "Score 3: 0.597 (with ALL labels predicted in the FIRST predictions)\n",
      "Score 4: 0.488 (Number of correct predictions over total number of predictions overall)\n",
      "Average number of predictions 1.638 vs average number of labels : 1.304\n",
      "The siren that is predicted the most is: 542107651 ( 54 times)\n",
      "\n",
      "######################### For Each company (Labeled at least Once) #########################\n",
      "AVG ACCURACY : 0.72 True_pos/Pos -> average for each siren\n",
      "AVG PRECISION: 0.668 True_pos/(True_Pos + False_Pos) -> average for each siren\n",
      "AVG RECALL   : 0.72 True_pos/(True_Pos + False_Neg) -> average for each siren\n",
      "AVG F1 score : 0.647 combination of precision and recall -> average for each siren\n",
      "\n",
      "######################### For Each article #########################\n",
      "AVG PRECISION: 0.494 #correct_predictions/#predictions-> average for each article\n",
      "AVG RECALL   : 0.71 #predicted_labels/#labels -> average for each article\n",
      "AVG alpha eval: 0.696 prediction score of an article -> average for each article\n"
     ]
    }
   ],
   "source": [
    "ac_model.evaluate()\n",
    "ac_model.print_eval()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(1,figsize=(15,10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(ac_model.company_precision_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('count')\n",
    "plt.title('Company Precision hist')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(ac_model.company_recall_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('count')\n",
    "plt.title('Company Recall hist')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(ac_model.company_accuracy_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('accuracy')\n",
    "plt.ylabel('count')\n",
    "plt.title('Company Accuracy hist')\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(ac_model.company_F1score_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('F1score')\n",
    "plt.ylabel('count')\n",
    "plt.title('Company F1score hist')\n",
    "plt.figure(2,figsize=(15,10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(ac_model.alpha_eval_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('alpha_eval')\n",
    "plt.ylabel('count')\n",
    "plt.title('alpha_eval hist (on each article)')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(ac_model.article_precision_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('article precision')\n",
    "plt.ylabel('count')\n",
    "plt.title('precision hist (on each article)')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(ac_model.article_recall_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('article recall')\n",
    "plt.ylabel('count')\n",
    "plt.title('recall hist (on each article)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['582041943'], [1]),\n",
       " (['424264281'], [0]),\n",
       " (['447800475'], [0]),\n",
       " (['384964508', '722045622'], [0, 1]),\n",
       " (['518706890'], [1]),\n",
       " (['347951238'], [1]),\n",
       " (['485182448'], [1]),\n",
       " (['180020026', '400456513', '483279923', '780129987'], [0, 0, 1, 1]),\n",
       " (['367801404'], [0]),\n",
       " (['315387688'], [0])]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start,end = 0,10\n",
    "label_evaluation= list(zip(ac_model.article_labels[start:end],ac_model.article_eval[start:end]))\n",
    "label_evaluation\n",
    "#REMARQUE: On ne peut pas predire des siren qui n'ont pas plus de 5 articles associé \n",
    "#          car leur relevant words n'ont pas été calculé par le TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['582041943'], [1]),\n",
       " (['419838529'], [0]),\n",
       " (['419838529'], [0]),\n",
       " (['722045622'], [1]),\n",
       " (['518706890'], [1]),\n",
       " (['347951238', '389191982', '399315613'], [1, 0, 0]),\n",
       " (['485182448'], [1]),\n",
       " (['780129987', '441639465', '954506077', '483279923', '520157579'],\n",
       "  [1, 0, 0, 1, 0]),\n",
       " (['380656439', '562123513'], [0, 0]),\n",
       " (['349694893'], [0])]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start,end = 0,10\n",
    "prediction_evaluation = list(zip(ac_model.pred_labels[start:end],ac_model.pred_eval[start:end]))\n",
    "prediction_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will do some hyper parameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNEXE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#EXAMPLE -> evaluating TO DELETE ONCE EVALUTATE WORKS\n",
    "\n",
    "related_words = relevant_words_tfidf #model related words\n",
    "#related_words = relevant_words_es_clean #relevant_words_es\n",
    "#related_words = relevant_words_baseline\n",
    "\n",
    "#corpus = X_train_corpus[:1000] # pour verifier que on peut sur entrainer\n",
    "corpus = X_test_corpus[:100] # pour tester sur de nouveaux articles\n",
    "min_score = 0.7\n",
    "n_sig_words = 2\n",
    "t = 250\n",
    "\n",
    "pred_eval = list() # Tag each prediction 1:correct, 0:wrong for each article\n",
    "pred_labels = list() # Siren predicted for each article\n",
    "article_eval = list() # Tag each label if 1:predicted, 0:not predicted for each article\n",
    "article_labels = list() # Siren labels for each article\n",
    "pred_labels_flat = list() #list all predicted sirens flattened\n",
    "article_labels_flat = list() # list of all siren labels flattened\n",
    "\n",
    "for document in tqdm(corpus):\n",
    "    plain_text = document[\"corpus\"]\n",
    "    \n",
    "    #pred_labels\n",
    "    pred_sirens = label_text(plain_text,related_words, n_sig_words, min_score,t)\n",
    "    pred_labels.append(pred_sirens)\n",
    "    #pred_labels_flat\n",
    "    pred_labels_flat += pred_sirens\n",
    "    \n",
    "    #article_labels\n",
    "    true_sirens =document[\"siren\"]\n",
    "    article_labels.append(true_sirens)\n",
    "    #article_labels_flat\n",
    "    article_labels_flat +=true_sirens\n",
    "    \n",
    "    #pred_eval \n",
    "    is_labeled = [0]*len(pred_sirens)\n",
    "    for i in range(len(pred_sirens)):  # For each predicted company\n",
    "        for label in true_sirens: # For each labeled company\n",
    "            if pred_sirens[i]==label:  # Tag if it is a good or bad predictions\n",
    "                is_labeled[i]=1\n",
    "    pred_eval.append(is_labeled) \n",
    "    \n",
    "    #article_eval\n",
    "    is_predicted = [0]*len(true_sirens)\n",
    "    for i in range(len(true_sirens)):  # For each label list\n",
    "        for pred in pred_sirens:       # For each prediction on the articel\n",
    "            if true_sirens[i]==pred:    # Tag the labels that have been predicted\n",
    "                is_predicted[i]=1\n",
    "    article_eval.append(is_predicted)\n",
    "\n",
    "#100%|██████████| 300/300 [02:23<00:00,  2.08it/s] # avec split train test\n",
    "#100%|██████████| 300/300 [01:37<00:00,  3.08it/s] # avec random split\n",
    "#07:46"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred_lim = 10\n",
    "\n",
    "print (\"There are\",len(pred_eval),\"texts evaluated\")\n",
    "########## How many times (at least) one of the companies is predicted ##########\n",
    "acc1 = list()\n",
    "for preds in pred_eval:\n",
    "    acc1.append(any(preds))\n",
    "print(\"Score 1:\", round(np.sum(acc1)/len(pred_eval),3),\"(with at least ONE label predicted)\")\n",
    "\n",
    "########## How many times ALL the labels are present in the prediction. ##########\n",
    "acc2 = list()\n",
    "for labels in article_eval:\n",
    "    acc2.append(labels.count(1)== len (labels)) \n",
    "print(\"Score 2:\", round(np.sum(acc2)/len(pred_eval),3),\"(with ALL labels predicted)\")\n",
    "\n",
    "########## How many times ALL labels are predicted in the FIRST predictions. ##########\n",
    "acc3 = list()\n",
    "for i in range(len(pred_eval)):\n",
    "    labels = article_eval[i]\n",
    "    preds = pred_eval[i]\n",
    "    acc3.append(preds[:len(labels)].count(1)== len(labels))\n",
    "print(\"Score 3:\", round(np.sum(acc3)/len(pred_eval),3),\"(with ALL labels predicted in the FIRST predictions)\")\n",
    "\n",
    "########## How many predictions are wrong wrt. how many are right (TRUE, FALSE) ##########\n",
    "true_pred = 0\n",
    "pred = 0\n",
    "for preds in pred_eval:\n",
    "    true_pred += np.sum(preds)\n",
    "    pred += len(preds)\n",
    "print(\"Score 4:\",round(true_pred/pred,3),\"(Number of correct predictions over total number of predictions overall)\")\n",
    "\n",
    "########## Average number of predictions vs average number of labels ##########\n",
    "len_label = list()\n",
    "len_pred = list()\n",
    "for i in range(len(pred_eval)):\n",
    "    len_label.append(len(article_labels[i]))\n",
    "    len_pred.append(len(pred_eval[i]))\n",
    "print(\"Average number of predictions\",round(np.mean(len_pred),3),\"vs average number of labels :\", round(np.mean(len_label),2))\n",
    "\n",
    "########## Most commun labels predicted ##########\n",
    "count_pred = dict()\n",
    "for siren in article_labels_flat:\n",
    "    if siren in count_pred.keys():\n",
    "        count_pred[siren] +=1\n",
    "    else:\n",
    "        count_pred[siren] = 1\n",
    "key_max = list(filter(lambda t: t[1]==max(count_pred.values()), count_pred.items()))[0][0] \n",
    "print(\"The siren that is predicted the most is:\",key_max,\"(\",np.max(list(count_pred.values())),\"times)\")\n",
    "\n",
    "########## Precision & RECALL ########## per siren(Company)\n",
    "company_accuracy_list = list()\n",
    "company_precision_list = list()\n",
    "company_recall_list = list()\n",
    "company_F1score_list = list()\n",
    "article_labels_set = siren_filtered  \n",
    "#article_labels_set = set(article_labels_flat) \n",
    "\n",
    "for siren in article_labels_set: # For each company compute it's TP,FP,TN,FN\n",
    "    true_pos = 0.0  # Siren IS a label and is predicted\n",
    "    false_pos = 0.0 # Siren is NOT a label and is predicted (false prediction)\n",
    "    true_neg = 0.0  # Siren is NOT a label and is not predicted (don't care)\n",
    "    false_neg = 0.0 # Siren IS a label and is NOT predicted\n",
    "    positive = 0.0  # Siren is label\n",
    "    \n",
    "    # true_pos, false_neg\n",
    "    for i in range(len(article_labels)):\n",
    "        for j in range(len(article_labels[i])):\n",
    "            if siren==article_labels[i][j]: # If company in the list of labels -> Check if was predicted\n",
    "                positive +=1\n",
    "                if article_eval[i][j]==1:\n",
    "                    true_pos +=1\n",
    "                else:\n",
    "                    false_neg +=1\n",
    "\n",
    "    # false_pos\n",
    "    for i in range(len(pred_labels)):\n",
    "        for j in range(len(pred_labels[i][:pred_lim])):\n",
    "            if siren==pred_labels[i][j]:  # If company in the list of predictions -> Check if was a label (correct prediction)\n",
    "                if pred_eval[i][j]==0: \n",
    "                    false_pos += 1 \n",
    "\n",
    "    if siren in list(set(article_labels_flat)): # Add to stats only if the company was part of the labels to predict\n",
    "        if true_pos ==0:\n",
    "            precision = 0\n",
    "            recall =0\n",
    "            accuracy = 0\n",
    "            F1score = 0\n",
    "        else:\n",
    "            accuracy = true_pos/positive\n",
    "            precision = true_pos/(true_pos+false_pos)\n",
    "            recall = true_pos/(true_pos+false_neg)\n",
    "            F1score = 2*(precision*recall)/(precision+recall)\n",
    "            \n",
    "        company_accuracy_list.append(accuracy)\n",
    "        company_precision_list.append(precision)\n",
    "        company_recall_list.append(recall)\n",
    "        company_F1score_list.append(F1score)\n",
    "          \n",
    "accuracy = np.average(company_accuracy_list)     \n",
    "precison = np.average(company_precision_list)\n",
    "recall = np.average(company_recall_list)\n",
    "F1score = np.average(company_F1score_list)\n",
    "print(\"######################### For Each company #########################\")\n",
    "print(\"AVG ACCURACY :\",round(accuracy,3),\"True_pos/Pos -> average for each siren\")\n",
    "print(\"AVG PRECISION:\",round(precison,3),\"True_pos/(True_Pos + False_Pos) -> average for each siren\")\n",
    "print(\"AVG RECALL   :\",round(recall,3),\"True_pos/(True_Pos + False_Neg) -> average for each siren\")\n",
    "print(\"AVG F1 score :\",round(F1score,3),\"combination of precision and recall -> average for each siren\")\n",
    "print(\"AVG ACCURACY :\",round(accuracy,3))\n",
    "print(\"AVG PRECISION:\",round(precison,3))\n",
    "print(\"AVG RECALL   :\",round(recall,3))\n",
    "print(\"AVG F1 score :\",round(F1score,3))\n",
    "print()\n",
    "########## Precision & RECALL ########## per Article      \n",
    "alpha_eval_list = list()\n",
    "article_recall_list = list()\n",
    "article_precision_list = list()\n",
    "alpha = 1   # penalizes errors if >1 hides errors if <1\n",
    "beta = 0.25 # weight for the missed labels (False Negative)\n",
    "gamma = 1   # weight for the wrongly predicted (False positives)\n",
    "#article_labels_set = list(set(article_labels_flat))\n",
    "article_labels_set = siren_filtered\n",
    "\n",
    "#alpha_eval_list\n",
    "for i in range(len(article_labels)):\n",
    "    alpha_eval = pow((1-((beta*article_eval[i].count(0) + gamma*pred_eval[i].count(0))/(len(set(pred_labels[i][:pred_lim]+article_labels[i]))))),alpha) \n",
    "    alpha_eval_list.append(alpha_eval)\n",
    "#recall_list\n",
    "for label in article_eval:\n",
    "    article_recall_list.append(label.count(1)/(len(label)+0.0001))\n",
    "#precision_list\n",
    "for pred in pred_eval:\n",
    "     article_precision_list.append(pred[:pred_lim].count(1)/(len(pred[:pred_lim])+0.0001))\n",
    "precison = np.average(article_precision_list)     \n",
    "recall = np.average(article_recall_list)\n",
    "alpha_eval = np.average(alpha_eval_list)\n",
    "\n",
    "print(\"######################### For Each article #########################\")\n",
    "print(\"AVG PRECISION:\",round(precison,3),\"#correct_predictions/#predictions-> average for each article\")\n",
    "print(\"AVG RECALL   :\",round(recall,3),\"#predicted_labels/#labels -> average for each article\")\n",
    "print(\"AVG alpha eval:\",round(alpha_eval,3),\"prediction score of an article -> average for each article\")\n",
    "print(\"AVG PRECISION:\",round(precison,3))\n",
    "print(\"AVG RECALL   :\",round(recall,3))\n",
    "print(\"AVG alpha eval:\",round(alpha_eval,3))\n",
    "print(\"AVG pred:\",round(np.mean(len_pred),3))\n",
    "\n",
    "plt.figure(1,figsize=(15,10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(company_precision_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('count')\n",
    "plt.title('Company Precision hist')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(company_recall_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('count')\n",
    "plt.title('Company Recall hist')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(company_accuracy_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('accuracy')\n",
    "plt.ylabel('count')\n",
    "plt.title('Company Accuracy hist')\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(company_F1score_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('F1score')\n",
    "plt.ylabel('count')\n",
    "plt.title('Company F1score hist')\n",
    "plt.figure(2,figsize=(15,10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(alpha_eval_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('alpha_eval')\n",
    "plt.ylabel('count')\n",
    "plt.title('alpha_eval hist (on each article)')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(article_precision_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('article precision')\n",
    "plt.ylabel('count')\n",
    "plt.title('precision hist (on each article)')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(article_recall_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('article recall')\n",
    "plt.ylabel('count')\n",
    "plt.title('recall hist (on each article)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
