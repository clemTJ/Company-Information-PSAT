{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de textes Francais par entreprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation de donn√©e avec label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import ast\n",
    "from scipy import stats\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching data\n",
    "PATH = \"./data/ArticleCompany_2020-11-17/\"\n",
    "coprus = \"corpus_check_long_SIREN_UPDATED2\"\n",
    "names = \"siren_name_map_clean\"\n",
    "\n",
    "with open(PATH + names +\".json\") as json_file: \n",
    "    dict_names = json.load(json_file) \n",
    "\n",
    "with open(PATH + coprus +\".json\") as json_file: \n",
    "    corpus_list = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The siren list is: <class 'str'>\n",
      "NOW the type of the siren list is: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Convert string to list of labels\n",
    "print (\"The siren list is:\",type(corpus_list[0][\"siren\"]))\n",
    "for document in corpus_list:\n",
    "    document[\"siren\"] = ast.literal_eval(document[\"siren\"]) # convert list in string format to list\n",
    "    for i in range(len(document[\"siren\"])): # Convert each int siren to string \n",
    "        document[\"siren\"][i] = str(document[\"siren\"][i])\n",
    "print (\"NOW the type of the siren list is:\",type(corpus_list[0][\"siren\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove articles with no text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of removed article: 29805   \n",
      "Index of removed article: 30158     \n"
     ]
    }
   ],
   "source": [
    "corpus_list_inter = list()\n",
    "for i in range(len(corpus_list)):\n",
    "    if len(corpus_list[i][\"corpus\"])<100: # small enough\n",
    "        text = corpus_list[i][\"corpus\"]\n",
    "        text = re.sub(\"^(\\s+)\", '', text)\n",
    "        if (len(text)>0):\n",
    "            corpus_list_inter.append(corpus_list[i])\n",
    "        else:\n",
    "            print (\"Index of removed article:\",i,corpus_list[i][\"corpus\"])\n",
    "    else:\n",
    "        corpus_list_inter.append(corpus_list[i])\n",
    "corpus_list = corpus_list_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57538 articles in the corpus\n",
      "There are 30178 companies in the list\n"
     ]
    }
   ],
   "source": [
    "print (\"There are\", len(corpus_list), \"articles in the corpus\")\n",
    "print (\"There are\", len(dict_names), \"companies in the list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compter Nombre d'Entreprises sans Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28690 companies with labels out of the 30178 companies\n",
      "there are 1488 companies with no articles\n",
      "95.07 % of the companies have articles\n",
      "Each article of the corpus has: dict_keys(['id', 'siren', 'corpus', 'url_article'])\n"
     ]
    }
   ],
   "source": [
    "dict_count = dict()\n",
    "#for company in dict_names.keys(): dict_count[company] = 0\n",
    "for document in corpus_list:\n",
    "    sir_list = document[\"siren\"]\n",
    "    for siren in sir_list:\n",
    "        if len(siren)>10: # Should not be triggered\n",
    "            print (\"ALERT:\",siren)\n",
    "        if siren in dict_count.keys():\n",
    "            dict_count[siren] +=1\n",
    "        else:\n",
    "            dict_count[siren] = 1\n",
    "print (\"There are\",len(dict_count.keys()),\"companies with labels out of the\", len(dict_names.keys()), \"companies\")\n",
    "print (\"there are\",len(dict_names.keys())-len(dict_count.keys()),\"companies with no articles\")\n",
    "print (round(len(dict_count)/(len(dict_names))*100,2),\"% of the companies have articles\")\n",
    "print (\"Each article of the corpus has:\",corpus_list[0].keys())\n",
    "#corpus_list[0][\"corpus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quels sont les entreprises sans articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_no_acticle_companies = dict()\n",
    "for company in dict_names.keys():\n",
    "    if company not in dict_count.keys():\n",
    "        dict_no_acticle_companies[company] = dict_names[company] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_count.keys():\n",
    "    if len(key)>10:\n",
    "        print (key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etude du nombre d'articles par entreprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=57538, minmax=(1, 29), mean=1.2476276547672842, variance=0.6042338178020358, skewness=8.133981486117998, kurtosis=136.56851777466036)\n",
      "There are 9362 arcticles with more that one tag out of the 57538 articles\n"
     ]
    }
   ],
   "source": [
    "multiple_siren = 0\n",
    "multiple_siren_list = list()\n",
    "for document in corpus_list:\n",
    "    if len(document[\"siren\"])==0:\n",
    "        print (\"ALERT article sans tag, id:\",document[\"id\"])\n",
    "    if len(document[\"siren\"])>1:\n",
    "        multiple_siren +=1\n",
    "    multiple_siren_list.append(len(document[\"siren\"]))\n",
    "    \n",
    "print(stats.describe(multiple_siren_list))   \n",
    "print (\"There are\",multiple_siren,\"arcticles with more that one tag out of the\",len(corpus_list),\"articles\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etudes du nombre d'articles associer a chaque entreprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=28690, minmax=(1, 175), mean=2.5021261763680727, variance=28.017371476985655, skewness=11.347634082651822, kurtosis=211.56847613512954)\n",
      "There are 63.58 % articles with one associated article\n",
      "There are 90.42 % articles with less than 5 associated article\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On prendre seulement les entreprises avec au moins un articles associer\n",
    "#sns.set(rc={'figure.figsize':(40,5)})\n",
    "values = list(dict_count.values())\n",
    "sns.displot(values, binwidth=1) #bins=20\n",
    "plt.xlim(0, 10)\n",
    "\n",
    "number = 5\n",
    "print(stats.describe(values))\n",
    "print (\"There are\",round(values.count(1)/len(values)*100,2), \"% articles with one associated article\")\n",
    "under_n = [1 for i in values if i < number]\n",
    "print (\"There are\",round(len(under_n)/len(values)*100,2), \"% articles with less than\",number,\"associated article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize et suppression de stop words du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words 157\n",
      "Ex: ['au', 'aux', 'avec', 'ce', 'ces']\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of stop words\",len(stop_words ))\n",
    "print (\"Ex:\",stop_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_cleaned = deepcopy(corpus_list)\n",
    "for document in tqdm(corpus_cleaned):\n",
    "    plain_text = document[\"corpus\"]\n",
    "    plain_text = plain_text.lower()\n",
    "    plain_text= re.sub(r'\\s+', ' ', plain_text)\n",
    "    #plain_text = re.sub(\"[^a-z0-9]\", ' ', plain_text)\n",
    "    plain_text = re.sub(\"[^a-z]\", ' ', plain_text)\n",
    "    plain_text = re.sub(r'\\s+', ' ', plain_text)\n",
    "    #remove one letter words?\n",
    "    #remove numbers?\n",
    "    pt_words = word_tokenize(plain_text)\n",
    "    cleaned_words =list()\n",
    "    for word in pt_words:\n",
    "        if len(word)>1:\n",
    "            if word not in stop_words:\n",
    "                cleaned_words.append(word)\n",
    "    document[\"corpus\"] = cleaned_words\n",
    "# 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57540/57540 [03:30<00:00, 273.74it/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize en gardant que les Noms "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#pip install stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp # Lemma doesn't work\n",
    "#stanfordnlp.download('fr')   # This downloads the French models for the neural pipeline\n",
    "#nlp = stanfordnlp.Pipeline(lang=\"fr\",processors = \"tokenize,mwt,lemma,pos\") # This sets up a default neural pipeline in French\n",
    "nlp = stanfordnlp.Pipeline(lang=\"fr\",processors = \"tokenize,pos\")\n",
    "#Documentation:\n",
    "#https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple\n",
    "text = 'Les victoires de Joe Biden √† la pr√©sidentielle am√©ricaine √† peine proclam√©e par les principaux m√©dias am√©ricains.'\n",
    "doc = nlp(text)  \n",
    "#extract_pos(doc)\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(word.text,\":\", word.upos, word.pos)\n",
    "doc.sentences[0].words[0] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keep = [\"NOUN\",\"PROPN\"]\n",
    "corpus_nouns = deepcopy(corpus_list)\n",
    "for document in tqdm(corpus_nouns):\n",
    "\n",
    "    #document = corpus_nouns[i]\n",
    "    plain_text = document[\"corpus\"]\n",
    "    doc = nlp(plain_text)\n",
    "    cleaned_words =list()\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            pos_tag = word.upos\n",
    "            if pos_tag in keep:\n",
    "                cleaned_words.append(word.text.lower())\n",
    "    document[\"corpus\"] = cleaned_words \n",
    "#20hours of computation to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57538"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_nouns)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save corpus_nouns\n",
    "PATH = \"./data/ArticleCompany_2020-11-17/\"\n",
    "file = \"corpus_nouns\"\n",
    "a_file = open(PATH + file + \".json\", \"w\")\n",
    "json.dump(corpus_nouns, a_file)\n",
    "a_file.close()\n",
    "print (file,\"is saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus_nouns \n",
    "PATH = \"./data/ArticleCompany_2020-11-17/\"\n",
    "file = \"corpus_nouns\"\n",
    "with open(PATH + file +\".json\") as json_file: \n",
    "    corpus_nouns = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The siren list is: <class 'list'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print (\"The siren list is:\",type(corpus_nouns[0][\"siren\"]), type(corpus_nouns[0][\"corpus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction d'entreprise avec plus de n articles sur elles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['419838529', '813883964', '572060333', '542104245', '399258755']\n",
      "SPIE OPERATIONS\n",
      "322120916 APPLE FRANCE\n",
      "APPLE FRANCE a 7 articles dans le corpus\n"
     ]
    }
   ],
   "source": [
    "print(list(dict_names.keys())[0:5])\n",
    "print (dict_names['399258755'])\n",
    "name_search = \"APPLE FRANCE\"\n",
    "for siren, name in dict_names.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "    if name_search in name:\n",
    "        print(siren, name)\n",
    "print(\"APPLE FRANCE a\",dict_count[\"322120916\"],\"articles dans le corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2084 companies with MORE than 5 associated articles\n"
     ]
    }
   ],
   "source": [
    "number = 5 # Number of articles a company must have to be kept in the list\n",
    "siren_filtered =[key for key in dict_count if dict_count[key] > number]\n",
    "print (\"There are\",len(siren_filtered),\"companies with MORE than\",number,\"associated articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in siren_filtered:\n",
    "    if len(key)>10:\n",
    "        print (key)\n",
    "#find out why label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation de Train et Test set pour l'entrainement de Tf.Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing unwanted articles\n",
      "We removed: 29189 articles and we have 28349 left\n",
      "Splitting data\n",
      "We have 19844 documents in the training corpus\n",
      "We have 8505 documents in the testing corpus\n"
     ]
    }
   ],
   "source": [
    "# Remove all of the articles that dont talk about our selected companies (in siren filtered)\n",
    "# Split corpus train/test\n",
    "#corpus = corpus_cleaned\n",
    "corpus = corpus_nouns\n",
    "test_size = 0.3\n",
    "X_train_corpus = list()\n",
    "X_test_corpus = list()\n",
    "\n",
    "#Removing unwanted articles\n",
    "print(\"Removing unwanted articles\")\n",
    "corpus_temp = list()\n",
    "for document in corpus:\n",
    "    keep = False\n",
    "    for document_sirens in document[\"siren\"]:\n",
    "        for sirens in siren_filtered:\n",
    "            if document_sirens == sirens:\n",
    "                keep = True\n",
    "    if keep:\n",
    "        corpus_temp.append(document)\n",
    "print (\"We removed:\",len(corpus)-len(corpus_temp),\"articles and we have\",len(corpus_temp),\"left\")\n",
    "corpus = corpus_temp\n",
    " \n",
    "#Splitting data\n",
    "print(\"Splitting data\") \n",
    "#for document in corpus:\n",
    "#    if (random.uniform(0, 1)<test_size):\n",
    "#        X_test_corpus.append(document)\n",
    "#    else:\n",
    "#        X_train_corpus.append(document)\n",
    "X_train_corpus, X_test_corpus = train_test_split(corpus, test_size=test_size, random_state=0)\n",
    "\n",
    "print (\"We have\",len(X_train_corpus),\"documents in the training corpus\")\n",
    "print (\"We have\",len(X_test_corpus),\"documents in the testing corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf.Idf pour une liste d'entreprise sur le training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf.Idf on Companies that have Associated Articles \n",
    "\n",
    "relevant_words_tfidf = {}\n",
    "corpus = X_train_corpus # corpus\n",
    "\n",
    "list_siren = siren_filtered\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "for siren in tqdm(list_siren):\n",
    "    #siren = \"322120916\" #APPLE FRANCE\n",
    "    plain_text_list = list()\n",
    "    company_article = list()\n",
    "    #binary = True\n",
    "    #sublinear_tf=False\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, ngram_range = (1,1), lowercase=False, sublinear_tf=True)\n",
    "    for document in corpus:\n",
    "        if siren in document[\"siren\"]:\n",
    "            company_article = company_article+document[\"corpus\"]  # add article to company BIG article\n",
    "        else:\n",
    "            plain_text_list.append(document[\"corpus\"]) # otherwise add to corpus\n",
    "\n",
    "    plain_text_list.insert(0,company_article) # add company article to begging of corpus\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(plain_text_list)\n",
    "\n",
    "    #Get the tf-idf scores for the words in the company article complication.\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] # discard tf.idf scores for the other texts\n",
    "\n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df = df.sort_values(by=[\"tfidf\"],ascending=False).head(40) # Take top 40 words\n",
    "\n",
    "    relevant_words_tfidf[siren] = list(zip(list(df.index),list(df[\"tfidf\"])))\n",
    "    #print (relevant_words_tfidf[company])\n",
    "\n",
    "\n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [2:19:42<00:00,  4.02s/it] # tokenized tf\n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [2:03:31<00:00,  3.56s/it] # tokenized binary\n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [1:50:00<00:00,  3.17s/it] # tokenized sublinear_tf\n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [1:21:23<00:00,  2.34s/it] # nouns sublinear_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_words_tfidf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save dictionary\n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_tfidf_nouns_sublinear_tf-tmp\"\n",
    "a_file = open(PATH + file + \".json\", \"w\")\n",
    "json.dump(relevant_words_tfidf, a_file)\n",
    "a_file.close()\n",
    "print (file,\"is saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_words_tfidf_nouns_sublinear_tf is loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# load dictionary \n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_tfidf_nouns_sublinear_tf\"\n",
    "a_file = open(PATH + file + \".json\", \"r\")\n",
    "relevant_words_tfidf = json.load(a_file)\n",
    "#relevant_words_tfidf = dict(relevant_words_tfidf)\n",
    "# check if well loaded\n",
    "print (file,\"is loaded successfully\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant_words_tfidf.keys()\n",
    "#relevant_words_tfidf['419838529']\n",
    "#type(relevant_words_tfidf)\n",
    "#len(relevant_words_tfidf.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Relevant Words from ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_words_train is loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# load dictionary \n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_train\"\n",
    "a_file = open(PATH + file + \".json\", \"r\")\n",
    "relevant_words_es = json.load(a_file)\n",
    "\n",
    "# check if well loaded\n",
    "print (file,\"is loaded successfully\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 2070 companies in the ES RWords out of the 2084 words in the labels\n"
     ]
    }
   ],
   "source": [
    "# Collect only the relevant words for the sirens that we want to consider\n",
    "count = 0\n",
    "for siren in siren_filtered:\n",
    "    if siren in relevant_words_es.keys():\n",
    "        count +=1\n",
    "print (\"There are\", count, \"companies in the ES RWords out of the\",len(siren_filtered), \"words in the article labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1970 non empty companies in ES RWords out of the 2084 words in the article labels\n"
     ]
    }
   ],
   "source": [
    "# Removing all the empty significant words and keeping only siren_filtered\n",
    "relevant_words_es_clean = dict()\n",
    "for siren in siren_filtered:\n",
    "    if siren in relevant_words_es.keys():\n",
    "        if len(relevant_words_es[siren])>0:\n",
    "            relevant_words_es_clean[siren] = relevant_words_es[siren]\n",
    "len(relevant_words_es_clean.keys())\n",
    "print (\"There are\", len(relevant_words_es_clean.keys()), \"non empty companies in ES RWords out of the\",len(siren_filtered), \"words in the article labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building baseline model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labeling article if it has the company name in it\n",
    "relevant_words_baseline = dict()\n",
    "for key in dict_names.keys():\n",
    "    #print (key)\n",
    "    #print([dict_names[key].lower()])\n",
    "    relevant_words_baseline[key] = [[dict_names[key].lower(),1]]\n",
    "#relevant_words_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# CLEANING PLAIN TEXT\n",
    "#Input  : Plain text - String\n",
    "#Output : Text removing all punctuation and lowercased\n",
    "#################################################################\n",
    "def clean_plain_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(\"[^a-z0-9]\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to compute a \"Related Scores\" for a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Gives a companies \"related score\" wrt an article (using it's significant words)\n",
    "#INPUT :plain_text- String/ word_list - list of significant words\n",
    "#OUTPUT: Score the chances the company is related to the article\n",
    "#################################################################\n",
    "def score_company(plain_text, word_list): \n",
    "    epsilon = 0.0001\n",
    "    avg_word_length =6+1 #+1 counting the spaces\n",
    "    n_words = len(word_list)\n",
    "    words_in_text = 0\n",
    "    #print (word_list)\n",
    "    for word in word_list:\n",
    "        words_in_text +=plain_text.count(word)\n",
    "    #return words_in_text\n",
    "    return words_in_text/(len(plain_text)/avg_word_length + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to compute and compare \"Related Scores\" for each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# For an Article, gives the \"related scores\"(likeness of being a label) for all companies\n",
    "#INPUT :plain_text- String/company related words - dict/ params\n",
    "#OUTPUT: dict of companies and their \"related scores\"\n",
    "#################################################################\n",
    "def text_label_scores(plain_text,related_words,n_sig_words=10, min_score = 0.01):\n",
    "    label_dict = {}\n",
    "    #print (sig_words_list)\n",
    "    for siren in related_words.keys():\n",
    "        #print(related_words[siren])\n",
    "        sig_words_list = np.array(related_words[siren])[:n_sig_words,0] # Build significant word list (with no scores)\n",
    "        #print (\"sig_words_list\")\n",
    "        score = score_company(plain_text, sig_words_list)\n",
    "        #print (score)\n",
    "        if score>=min_score:\n",
    "            label_dict[siren]= score\n",
    "    ### Soft_max ###\n",
    "    #sum_exp = sum([np.exp(v) for v in label_dict.values()])\n",
    "    #label_dict = {k: np.exp(v)/sum_exp for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    ### normalizing score ###\n",
    "    #max_val = max(label_dict.values())\n",
    "    #label_dict = {k: v/max_val for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    ### Plain score ###\n",
    "    label_dict = {k: v for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing text_label_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# testing text_label_scores\n",
    "plain_text = \"\"\"\n",
    "La victoire de Joe Biden √† la pr√©sidentielle am√©ricaine √† peine proclam√©e par les principaux \n",
    "m√©dias am√©ricains, les messages de f√©licitations des dirigeants occidentaux affluent. Sur Twitter,\n",
    "une courte s√©quence vid√©o fait le buzz entre Londres et Dublin. Ce 7 novembre, on y voit le \n",
    "candidat d√©mocrate entour√© de journalistes.\n",
    "\"\"\"\n",
    "plain_text = clean_plain_text(plain_text)\n",
    "\n",
    "related_words = relevant_words_tfidf\n",
    "n_sig_words= 10\n",
    "min_score = 0.1 # nbr of sig words in text\n",
    "#print (plain_text)\n",
    "label_dict = text_label_scores(plain_text,related_words, n_sig_words, min_score)\n",
    "print(label_dict)\n",
    "for key in label_dict.keys(): # Should not trigger\n",
    "    if len(key)>10:\n",
    "        print (key)\n",
    "#find out why label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function predict text labels of plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# For an Article, predicts the labels (sirens)\n",
    "#INPUT : plain_text- String/company related words - dict/ params\n",
    "#OUTPUT: dict of companies and their \"related scores\"\n",
    "#################################################################\n",
    "def label_text(plain_text,related_words, n_sig_words= 10, min_score = 0.1):\n",
    "    label_dict = text_label_scores(plain_text,related_words, n_sig_words, min_score)\n",
    "    #print(\"best score\",label_dict[list(label_dict.keys())[0]])\n",
    "    sirens = list(label_dict.keys())\n",
    "    return sirens[:8] # limiting the number of predictions to 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [36:49<00:00,  2.26it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Convert the list in the sirens to actual list!\n",
    "\n",
    "related_words = relevant_words_tfidf #model related words\n",
    "#related_words = relevant_words_es_clean #relevant_words_es\n",
    "#related_words = relevant_words_baseline\n",
    "corpus = X_train_corpus[:5000] # pour verifier que on peut sur entrainer\n",
    "#corpus = X_test_corpus[:] # pour tester sur de nouveaux articles\n",
    "\n",
    "pred_eval = list() # Tag each prediction 1:correct, 0:wrong for each article\n",
    "pred_labels = list() # Siren predicted for each article\n",
    "article_eval = list() # Tag each label if 1:predicted, 0:not predicted for each article\n",
    "article_labels = list() # Siren labels for each article\n",
    "pred_labels_flat = list() #list all predicted sirens\n",
    "article_labels_flat = list() # iist of all siren labels\n",
    "for document in tqdm(corpus):\n",
    "    plain_text = document[\"corpus\"]\n",
    "    \n",
    "    #pred_labels\n",
    "    pred_sirens = label_text(plain_text,related_words, n_sig_words= 10, min_score = 0.1)\n",
    "    pred_labels.append(pred_sirens)\n",
    "    #pred_labels_flat\n",
    "    pred_labels_flat += pred_sirens\n",
    "    \n",
    "    #article_labels\n",
    "    true_sirens =document[\"siren\"]\n",
    "    article_labels.append(true_sirens)\n",
    "    #article_labels_flat\n",
    "    article_labels_flat +=true_sirens\n",
    "    \n",
    "    #pred_eval \n",
    "    is_labeled = [0]*len(pred_sirens)\n",
    "    for i in range(len(pred_sirens)):  # For each prediction list of sirens\n",
    "        for label in true_sirens: # For each label of the article\n",
    "            if pred_sirens[i]==label:  # Tag if they are good (or bad) predictions\n",
    "                is_labeled[i]=1\n",
    "                \n",
    "    pred_eval.append(is_labeled) \n",
    "    \n",
    "    #article_eval\n",
    "    is_predicted = [0]*len(true_sirens)\n",
    "    for i in range(len(true_sirens)):  # For each label list\n",
    "        for pred in pred_sirens:       # For each prediction on the articel\n",
    "            if true_sirens[i]==pred:    # Tag the labels that have been predicted\n",
    "                is_predicted[i]=1\n",
    "    article_eval.append(is_predicted) \n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [02:23<00:00,  2.08it/s] # avec split train test\n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [01:37<00:00,  3.08it/s] # avec random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct pred_labels_flat 2051 of the 2084 total filtered labels\n",
      "distinct article_labels_flat 2731 of the 2084 total filtered labels\n"
     ]
    }
   ],
   "source": [
    "print (\"distinct pred_labels_flat\",len(set(pred_labels_flat)),\"of the\",len(siren_filtered),\"total filtered labels\")\n",
    "print (\"distinct article_labels_flat\",len(set(article_labels_flat)),\"of the\",len(siren_filtered),\"total filtered labels\")\n",
    "#set(article_labels_flat)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start,end = 13,20\n",
    "label_evaluation= list(zip(article_labels[start:end],article_eval[start:end]))\n",
    "label_evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prediction_evaluation= list(zip(pred_labels[start:end],pred_eval[start:end]))\n",
    "prediction_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5000 texts evaluated\n",
      "Accuracy1: 0.818 (with at least ONE label predicted)\n",
      "Accuracy2: 0.697 (with ALL labels predicted)\n",
      "Accuracy3: 0.483 (with ALL labels predicted in the FIRST predictions)\n",
      "Accuracy4: 0.117 (Number of correct predictions over total number of predictions overall)\n",
      "Average number of predictions 7.385 vs average number of labels : 1.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2084 [00:00<00:41, 49.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The siren that is predicted the most is: 542107651 ( 37 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [00:40<00:00, 52.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG ACCURACY: 0.802 True_pos/Pos -> average for each siren\n",
      "AVG PRECISION: 0.336 True_pos/(True_Pos + False_Pos) -> average for each siren\n",
      "AVG RECALL: 0.802 True_pos/(True_Pos + False_Neg) -> average for each siren\n",
      "AVG F1: 0.299 True_pos/(True_Pos + False_Neg) -> average for each siren\n",
      "\n",
      "PRECISION DescribeResult(nobs=1751, minmax=(0.0, 1.0), mean=0.3363983387967619, variance=0.09216380279669965, skewness=0.890266090103209, kurtosis=-0.2977353095518036)\n",
      "\n",
      "RECALL DescribeResult(nobs=1751, minmax=(0.0, 1.0), mean=0.801660499085178, variance=0.08838813095938677, skewness=-1.4829479234130587, kurtosis=1.2387133084625734)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Recall hist')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########## How many times (at least) one of the companies are predicted ##########\n",
    "print (\"There are\",len(pred_eval),\"texts evaluated\")\n",
    "acc1 = list()\n",
    "for preds in pred_eval:\n",
    "    acc1.append(any(preds))\n",
    "print(\"Accuracy1:\", round(np.sum(acc1)/len(pred_eval),3),\"(with at least ONE label predicted)\")\n",
    "\n",
    "########## How many times ALL the labels are present in the prediction. ##########\n",
    "acc2 = list()\n",
    "for i in range(len(pred_eval)):\n",
    "    len_label = len(corpus[i][\"siren\"])\n",
    "    n_correct_pred = np.sum(pred_eval[i])\n",
    "    if len_label==n_correct_pred:\n",
    "        acc2.append(True)\n",
    "    else:\n",
    "        acc2.append(False)\n",
    "    if len(corpus[i][\"siren\"])<np.sum(pred_eval[i]): # Should never trigger\n",
    "        print(\"Error to many good predictions\")   \n",
    "print(\"Accuracy2:\", round(np.sum(acc2)/len(pred_eval),3),\"(with ALL labels predicted)\")\n",
    "\n",
    "########## How many times ALL the labels are present in the prediction and are . ##########\n",
    "acc3 = list()\n",
    "for i in range(len(pred_eval)):\n",
    "    len_label = len(corpus[i][\"siren\"])\n",
    "    n_first_correct_pred = np.sum(pred_eval[i][:len_label])# Keeping only the len_label first predictions\n",
    "    if len_label==n_first_correct_pred:\n",
    "        acc3.append(True)\n",
    "    else:\n",
    "        acc3.append(False)\n",
    "    if len(corpus[i][\"siren\"])<np.sum(pred_eval[i]): # Should never trigger\n",
    "        print(\"Error to many good predictions\") \n",
    "print(\"Accuracy3:\", round(np.sum(acc3)/len(pred_eval),3),\"(with ALL labels predicted in the FIRST predictions)\")\n",
    "\n",
    "########## How many predictions are wrong wrt. how many are right (TRUE, FALSE) ##########\n",
    "true_pred = 0\n",
    "pred = 0\n",
    "for preds in pred_eval:\n",
    "    true_pred += np.sum(preds)\n",
    "    pred += len(preds)\n",
    "print(\"Accuracy4:\",round(true_pred/pred,3),\"(Number of correct predictions over total number of predictions overall)\")\n",
    "\n",
    "########## Average number of predictions vs average number of labels ##########\n",
    "len_label = list()\n",
    "len_pred = list()\n",
    "for i in range(len(pred_eval)):\n",
    "    len_label.append(len(corpus[i][\"siren\"]))\n",
    "    len_pred.append(len(pred_eval[i]))\n",
    "print(\"Average number of predictions\",round(np.mean(len_pred),3),\"vs average number of labels :\", round(np.mean(len_label),2))\n",
    "\n",
    "########## Most commun labels predicted ##########\n",
    "count_pred = dict()\n",
    "for siren in article_labels_flat:\n",
    "    if siren in count_pred.keys():\n",
    "        count_pred[siren] +=1\n",
    "    else:\n",
    "        count_pred[siren] = 1\n",
    "key_max = list(filter(lambda t: t[1]==max(count_pred.values()), count_pred.items()))[0][0] \n",
    "print(\"The siren that is predicted the most is:\",key_max,\"(\",np.max(list(count_pred.values())),\"times)\")\n",
    "#sns.catplot(x=\"deck\", kind=\"count\", palette=\"ch:.25\", data=pred_labels_flat)\n",
    "\n",
    "########## Precision & RECALL ##########\n",
    "accuracy_list = list()\n",
    "precision_list = list()\n",
    "recall_list = list()\n",
    "\n",
    "#article_labels_set = list(set(article_labels_flat))\n",
    "article_labels_set = siren_filtered\n",
    "for siren in tqdm(article_labels_set): # For each company compute it's TP,FP,TN,FN\n",
    "    \n",
    "    true_pos = 0.0  # Siren is a label and is predicted\n",
    "    false_pos = 0.0 # Siren is NOT a label and is predicted (false prediction)\n",
    "    true_neg = 0.0  # Siren is NOT a label and is not predicted (don't care)\n",
    "    false_neg = 0.0 # Siren is a label and is NOT predicted\n",
    "    positive = 0.0       # Siren is label\n",
    "    \n",
    "    # true_pos, false_neg\n",
    "    for i in range(len(article_labels)):\n",
    "        for j in range(len(article_labels[i])):\n",
    "            if siren==article_labels[i][j]: # If company in the list of labels -> Check if was predicted\n",
    "                positive +=1\n",
    "                if article_eval[i][j]==1:\n",
    "                    true_pos +=1\n",
    "                else:\n",
    "                    false_neg +=1\n",
    "\n",
    "    # false_pos\n",
    "    for i in range(len(pred_labels)):\n",
    "        for j in range(len(pred_labels[i])):\n",
    "            if siren==pred_labels[i][j]:  # If company in the list of predictions -> Check if was a label (correct prediction)\n",
    "                if pred_eval[i][j]==0: \n",
    "                    false_pos += 1 \n",
    "\n",
    "    if siren in list(set(article_labels_flat)):\n",
    "        if true_pos ==0:\n",
    "            precision = 0\n",
    "            recall =0\n",
    "            accuracy = 0\n",
    "        else:\n",
    "            accuracy = true_pos/positive\n",
    "            precision = true_pos/(true_pos+false_pos)\n",
    "            recall = true_pos/(true_pos+false_neg)\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "accuracy = np.average(accuracy_list)     \n",
    "precison = np.average(precision_list)\n",
    "recall = np.average(recall_list)\n",
    "\n",
    "print(\"AVG ACCURACY:\",round(accuracy,3),\"True_pos/Pos -> average for each siren\")\n",
    "print(\"AVG PRECISION:\",round(precison,3),\"True_pos/(True_Pos + False_Pos) -> average for each siren\")\n",
    "print(\"AVG RECALL:\",round(recall,3),\"True_pos/(True_Pos + False_Neg) -> average for each siren\")\n",
    "print(\"AVG F1:\",round(2*precison*recall/(precision+recall),3),\"True_pos/(True_Pos + False_Neg) -> average for each siren\")\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"PRECISION\",stats.describe(precision_list))\n",
    "print ()\n",
    "print(\"RECALL\",stats.describe(recall_list))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(precision_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('count')\n",
    "plt.title('Precision hist')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(recall_list, range = (0, 1), bins = 20, color = 'yellow',edgecolor = 'red')\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('count')\n",
    "plt.title('Recall hist')\n",
    "\n",
    "\n",
    "#La pr√©cision est le nombre de documents pertinents retrouv√©s rapport√© au nombre de documents total propos√© pour une requ√™te donn√©e.\n",
    "\n",
    "\n",
    "#Le rappel est d√©fini par le nombre de documents pertinents retrouv√©s au regard du nombre de documents pertinents que poss√®de la base de donn√©es. \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#La pr√©cision est le nombre de documents pertinents retrouv√©s rapport√© au nombre de documents total propos√© pour une requ√™te donn√©e.\n",
    "CAD:\"donne le niveau de certitude que un article classifi√© d'une classe est bien de cette classe\"\n",
    "#Le rappel est d√©fini par le nombre de documents pertinents retrouv√©s au regard du nombre de documents pertinents que poss√®de la base de donn√©es.\n",
    "CAD:\"donne le niveau de certitude que tout les acticles de la classe ont √©t√© trouv√©\"\n",
    "\n",
    "### 200 articles\n",
    "AVG PRECISION: 0.718 True_pos/(True_Pos + False_Pos) -> average for each siren\n",
    "AVG RECALL: 0.865 True_pos/(True_Pos + False_Neg) -> average for each siren\n",
    "AVG F1: 0.666 True_pos/(True_Pos + False_Neg) -> average for each siren\n",
    "### 500 articles\n",
    "AVG PRECISION: 0.717 True_pos/(True_Pos + False_Pos) -> average for each siren\n",
    "AVG RECALL: 0.897 True_pos/(True_Pos + False_Neg) -> average for each siren\n",
    "AVG F1: 0.678 True_pos/(True_Pos + False_Neg) -> average for each siren\n",
    "# 1000 articles\n",
    "AVG PRECISION: 0.471 True_pos/(True_Pos + False_Pos) -> average for each siren\n",
    "AVG RECALL: 0.714 True_pos/(True_Pos + False_Neg) -> average for each siren\n",
    "AVG F1: 0.554 True_pos/(True_Pos + False_Neg) -> average for each siren\n",
    "# 5000 articles - avg on the 2700 labels\n",
    "AVG PRECISION: 0.216 True_pos/(True_Pos + False_Pos) -> average for each siren\n",
    "AVG RECALL: 0.514 True_pos/(True_Pos + False_Neg) -> average for each siren\n",
    "AVG F1: 0.146 True_pos/(True_Pos + False_Neg) -> average for each siren\n",
    "# 5000 articles - avg on the 280 labels..\n",
    "AVG ACCURACY: 0.802 True_pos/Pos -> average for each siren\n",
    "AVG PRECISION: 0.336 True_pos/(True_Pos + False_Pos) -> average for each siren\n",
    "AVG RECALL: 0.802 True_pos/(True_Pos + False_Neg) -> average for each siren\n",
    "AVG F1: 0.299 True_pos/(True_Pos + False_Neg) -> average for each siren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['534318415'], [1]),\n",
       " (['394149496'], [0]),\n",
       " (['382510816'], [1]),\n",
       " (['71501803'], [1]),\n",
       " (['350422622'], [1]),\n",
       " (['414967984', '439780339', '521724336'], [0, 0, 0]),\n",
       " (['652014051'], [1]),\n",
       " (['435361209'], [0]),\n",
       " (['414520254',\n",
       "   '483015749',\n",
       "   '528203755',\n",
       "   '539528190',\n",
       "   '793845090',\n",
       "   '809642176',\n",
       "   '876580077'],\n",
       "  [1, 0, 0, 0, 0, 0, 1]),\n",
       " (['378775746'], [1]),\n",
       " (['480470152', '500569405'], [0, 0]),\n",
       " (['552096281'], [1]),\n",
       " (['480333269', '503616666'], [1, 0]),\n",
       " (['562082909'], [1]),\n",
       " (['384093563'], [1]),\n",
       " (['343059564'], [1]),\n",
       " (['802698746'], [1]),\n",
       " (['444606750'], [1]),\n",
       " (['532112315'], [1]),\n",
       " (['389522152'], [0])]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start,end = 0,20\n",
    "label_evaluation= list(zip(article_labels[start:end],article_eval[start:end]))\n",
    "label_evaluation\n",
    "#REMARQUE: On ne peut pas predire des siren qui n'ont pas plus de 5 articles associ√© \n",
    "#          car leur relevant words n'ont pas √©t√© calcul√© par le TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['419838529', '833663172'], [1, 0]),\n",
       " (['419838529', '833663172', '430425314'], [1, 0, 0]),\n",
       " (['419838529',\n",
       "   '390265734',\n",
       "   '500005624',\n",
       "   '331408336',\n",
       "   '433624707',\n",
       "   '833663172',\n",
       "   '344497011',\n",
       "   '332822485'],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " (['408790608', '722045622', '542168463'], [0, 0, 0]),\n",
       " (['419838529',\n",
       "   '349501676',\n",
       "   '433428018',\n",
       "   '833663172',\n",
       "   '344497011',\n",
       "   '430425314'],\n",
       "  [1, 0, 0, 0, 0, 0]),\n",
       " (['408790608', '722045622', '542168463'], [0, 0, 0]),\n",
       " (['438085557',\n",
       "   '810885251',\n",
       "   '833663172',\n",
       "   '379954886',\n",
       "   '419838529',\n",
       "   '441417110',\n",
       "   '813346350',\n",
       "   '752360180'],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " (['419838529',\n",
       "   '414565341',\n",
       "   '390265734',\n",
       "   '344497011',\n",
       "   '622035749',\n",
       "   '833663172',\n",
       "   '349501676',\n",
       "   '433428018'],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " (['833663172', '793920521', '419838529'], [0, 0, 1]),\n",
       " (['419838529', '833663172', '430425314'], [1, 0, 0]),\n",
       " (['419838529',\n",
       "   '392527404',\n",
       "   '552120222',\n",
       "   '86380730',\n",
       "   '383699048',\n",
       "   '833663172',\n",
       "   '380373753',\n",
       "   '430425314'],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " (['419838529',\n",
       "   '833663172',\n",
       "   '430425314',\n",
       "   '431373471',\n",
       "   '444427298',\n",
       "   '488629783',\n",
       "   '305729352',\n",
       "   '808426662'],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " (['419838529', '833663172', '379668809'], [1, 0, 0]),\n",
       " (['419838529',\n",
       "   '318251600',\n",
       "   '395030844',\n",
       "   '833663172',\n",
       "   '493634711',\n",
       "   '344497011',\n",
       "   '430425314'],\n",
       "  [1, 0, 1, 0, 0, 0, 0]),\n",
       " (['833663172',\n",
       "   '419838529',\n",
       "   '331408336',\n",
       "   '500005624',\n",
       "   '622035749',\n",
       "   '702023508',\n",
       "   '433624707',\n",
       "   '330265323'],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0]),\n",
       " (['419838529',\n",
       "   '833663172',\n",
       "   '344497011',\n",
       "   '502220056',\n",
       "   '440055861',\n",
       "   '331408336',\n",
       "   '433624707',\n",
       "   '491048575'],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " (['833663172',\n",
       "   '419838529',\n",
       "   '793920521',\n",
       "   '791889777',\n",
       "   '407535517',\n",
       "   '316580869',\n",
       "   '438479941',\n",
       "   '751554866'],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0]),\n",
       " (['419838529',\n",
       "   '500005624',\n",
       "   '622035749',\n",
       "   '662049840',\n",
       "   '433624707',\n",
       "   '833663172',\n",
       "   '344497011',\n",
       "   '332822485'],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " (['419838529',\n",
       "   '833663172',\n",
       "   '349501676',\n",
       "   '433428018',\n",
       "   '431373471',\n",
       "   '414565341',\n",
       "   '444427298',\n",
       "   '305729352'],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " (['419838529', '384518114', '552051302', '833663172'], [1, 0, 0, 0])]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_evaluation= list(zip(pred_labels[start:end],pred_eval[start:end]))\n",
    "prediction_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
