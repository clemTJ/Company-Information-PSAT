{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de textes Francais par entreprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation de donn√©e avec label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import ast\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching data\n",
    "PATH = \"./data/ArticleCompany_2020-11-17/\"\n",
    "coprus = \"corpus_check_long_SIREN_UPDATED2\"\n",
    "names = \"siren_name_map_clean\"\n",
    "\n",
    "with open(PATH + names +\".json\") as json_file: \n",
    "    dict_names = json.load(json_file) \n",
    "\n",
    "with open(PATH + coprus +\".json\") as json_file: \n",
    "    corpus_list = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The siren list is: <class 'str'>\n",
      "NOW the type of the siren list is: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Convert string to list of labels\n",
    "print (\"The siren list is:\",type(corpus_list[0][\"siren\"]))\n",
    "for document in corpus_list:\n",
    "    document[\"siren\"] = ast.literal_eval(document[\"siren\"]) # convert list in string format to list\n",
    "    for i in range(len(document[\"siren\"])): # Convert each int siren to string \n",
    "        document[\"siren\"][i] = str(document[\"siren\"][i])\n",
    "print (\"NOW the type of the siren list is:\",type(corpus_list[0][\"siren\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in corpus_list:\n",
    "    for siren in document['siren']:\n",
    "        if len(siren)>10:\n",
    "            print(siren)\n",
    "            print (document['siren'])\n",
    "            print (\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57540 articles in the corpus\n",
      "There are 30178 companies in the list\n"
     ]
    }
   ],
   "source": [
    "print (\"There are\", len(corpus_list), \"articles in the corpus\")\n",
    "print (\"There are\", len(dict_names), \"companies in the list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compter Nombre d'Entreprises sans Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28690 companies with labels out of the 30178 companies\n",
      "there are 1488 companies with no articles\n",
      "95.07 % of the companies have articles\n",
      "Each article of the corpus has: dict_keys(['id', 'siren', 'corpus', 'url_article'])\n"
     ]
    }
   ],
   "source": [
    "dict_count = dict()\n",
    "#for company in dict_names.keys(): dict_count[company] = 0\n",
    "for document in corpus_list:\n",
    "    sir_list = document[\"siren\"]\n",
    "    for siren in sir_list:\n",
    "        if len(siren)>10: # Should not be triggered\n",
    "            print (\"ALERT:\",siren)\n",
    "        if siren in dict_count.keys():\n",
    "            dict_count[siren] +=1\n",
    "        else:\n",
    "            dict_count[siren] = 1\n",
    "print (\"There are\",len(dict_count.keys()),\"companies with labels out of the\", len(dict_names.keys()), \"companies\")\n",
    "print (\"there are\",len(dict_names.keys())-len(dict_count.keys()),\"companies with no articles\")\n",
    "print (round(len(dict_count)/(len(dict_names))*100,2),\"% of the companies have articles\")\n",
    "print (\"Each article of the corpus has:\",corpus_list[0].keys())\n",
    "#corpus_list[0][\"corpus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quels sont les entreprises sans articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_no_acticle_companies = dict()\n",
    "for company in dict_names.keys():\n",
    "    if company not in dict_count.keys():\n",
    "        dict_no_acticle_companies[company] = dict_names[company] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_count.keys():\n",
    "    if len(key)>10:\n",
    "        print (key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quels sont les entreprise avec 2 tags ou plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=57540, minmax=(1, 29), mean=1.2476190476190476, variance=0.6042149465497106, skewness=8.134111908500053, kurtosis=136.5727450895893)\n",
      "There are 9362 arcticles with more that one tag out of the 57540 articles\n"
     ]
    }
   ],
   "source": [
    "multiple_siren = 0\n",
    "multiple_siren_list = list()\n",
    "for document in corpus_list:\n",
    "    if len(document[\"siren\"])==0:\n",
    "        print (\"ALERT article sans tag, id:\",document[\"id\"])\n",
    "    if len(document[\"siren\"])>1:\n",
    "        multiple_siren +=1\n",
    "    multiple_siren_list.append(len(document[\"siren\"]))\n",
    "    \n",
    "print(stats.describe(multiple_siren_list))   \n",
    "print (\"There are\",multiple_siren,\"arcticles with more that one tag out of the\",len(corpus_list),\"articles\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etudes du nombre d'articles associer a chaque entreprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=28690, minmax=(1, 175), mean=2.502195887068665, variance=28.018556298899046, skewness=11.346985559301654, kurtosis=211.55000716780165)\n",
      "There are 63.58 % articles with one associated article\n",
      "There are 90.42 % articles with less than 5 associated article\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On prendre seulement les entreprises avec au moins un articles associer\n",
    "#sns.set(rc={'figure.figsize':(40,5)})\n",
    "values = list(dict_count.values())\n",
    "sns.displot(values, binwidth=1) #bins=20\n",
    "plt.xlim(0, 10)\n",
    "\n",
    "number = 5\n",
    "print(stats.describe(values))\n",
    "print (\"There are\",round(values.count(1)/len(values)*100,2), \"% articles with one associated article\")\n",
    "under_n = [1 for i in values if i < number]\n",
    "print (\"There are\",round(len(under_n)/len(values)*100,2), \"% articles with less than\",number,\"associated article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize et suppression de stop words du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words 157\n",
      "Ex: ['au', 'aux', 'avec', 'ce', 'ces']\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of stop words\",len(stop_words ))\n",
    "print (\"Ex:\",stop_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57540/57540 [03:13<00:00, 298.07it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_cleaned = deepcopy(corpus_list)\n",
    "for document in tqdm(corpus_cleaned):\n",
    "    plain_text = document[\"corpus\"]\n",
    "    plain_text = plain_text.lower()\n",
    "    plain_text= re.sub(r'\\s+', ' ', plain_text)\n",
    "    #plain_text = re.sub(\"[^a-z0-9]\", ' ', plain_text)\n",
    "    plain_text = re.sub(\"[^a-z]\", ' ', plain_text)\n",
    "    plain_text = re.sub(r'\\s+', ' ', plain_text)\n",
    "    #remove one letter words?\n",
    "    #remove numbers?\n",
    "    pt_words = word_tokenize(plain_text)\n",
    "    cleaned_words =list()\n",
    "    for word in pt_words:\n",
    "        if len(word)>1:\n",
    "            if word not in stop_words:\n",
    "                cleaned_words.append(word)\n",
    "    document[\"corpus\"] = cleaned_words\n",
    "# 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57540/57540 [03:30<00:00, 273.74it/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize en gardant que les Noms "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# \n",
    "corpus_nouns = list()\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "root_path=\"/data/StanfordPostagger/\"\n",
    "pos_tagger = StanfordPOSTagger(root_path + \"models/french.tagger\", path_to_jar=root_path + \"stanford-postagger.jar\",encoding='utf8') #instance de la classe StanfordPOSTagger en UTF-8\n",
    "def pos_tag(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence) #je transforme la phrase en tokens => si vous avez un texte avec plusieurs phrases, passez d'abord par nltk pour r√©cup√©rer les phrases\n",
    "    tags = pos_tagger.tag(tokens) #lance le tagging\n",
    "    return tags\n",
    "sentence = \"Bonjour, je m'appelle Pierre Petrella.\"\n",
    "pos_tag(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction d'entreprise avec plus de n articles sur elles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['419838529', '813883964', '572060333', '542104245', '399258755']\n",
      "SPIE OPERATIONS\n",
      "322120916 APPLE FRANCE\n",
      "APPLE FRANCE a 7 articles dans le corpus\n"
     ]
    }
   ],
   "source": [
    "print(list(dict_names.keys())[0:5])\n",
    "print (dict_names['399258755'])\n",
    "name_search = \"APPLE FRANCE\"\n",
    "for siren, name in dict_names.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "    if name_search in name:\n",
    "        print(siren, name)\n",
    "print(\"APPLE FRANCE a\",dict_count[\"322120916\"],\"articles dans le corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2084 companies with MORE than 5 associated articles\n"
     ]
    }
   ],
   "source": [
    "number = 5 # Number of articles a company must have to be kept in the list\n",
    "siren_filtered =[key for key in dict_count if dict_count[key] > number]\n",
    "print (\"There are\",len(siren_filtered),\"companies with MORE than\",number,\"associated articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in siren_filtered:\n",
    "    if len(key)>10:\n",
    "        print (key)\n",
    "#find out why label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation de Train et Test set pour l'entrainement de Tf.Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing unwanted articles\n",
      "We removed: 29189 articles and we have 28351 left\n",
      "Splitting data\n",
      "We have 19845 documents in the training corpus\n",
      "We have 8506 documents in the testing corpus\n"
     ]
    }
   ],
   "source": [
    "# Remove all of the articles that dont talk about our selected companies (in siren filtered)\n",
    "# Split corpus train/test\n",
    "corpus = corpus_cleaned\n",
    "train_size = 0.7\n",
    "X_train_corpus = list()\n",
    "X_test_corpus = list()\n",
    "\n",
    "#Removing unwanted articles\n",
    "print(\"Removing unwanted articles\")\n",
    "corpus_temp = list()\n",
    "for document in corpus:\n",
    "    keep = False\n",
    "    for document_sirens in document[\"siren\"]:\n",
    "        for sirens in siren_filtered:\n",
    "            if document_sirens == sirens:\n",
    "                keep = True\n",
    "    if keep:\n",
    "        corpus_temp.append(document)\n",
    "print (\"We removed:\",len(corpus)-len(corpus_temp),\"articles and we have\",len(corpus_temp),\"left\")\n",
    "corpus = corpus_temp\n",
    " \n",
    "#Splitting data\n",
    "print(\"Splitting data\") \n",
    "for document in corpus:\n",
    "    if (random.uniform(0, 1)<train_size):\n",
    "        X_train_corpus.append(document)\n",
    "    else:\n",
    "        X_test_corpus.append(document)\n",
    "print (\"We have\",len(X_train_corpus),\"documents in the training corpus\")\n",
    "print (\"We have\",len(X_test_corpus),\"documents in the testing corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf.Idf pour une liste d'entreprise sur le training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [1:50:00<00:00,  3.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# Tf.Idf on Companies that have Associated Articles \n",
    "\n",
    "relevant_words_tfidf = {}\n",
    "corpus = X_train_corpus # corpus\n",
    "\n",
    "list_siren = siren_filtered\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "for siren in tqdm(list_siren):\n",
    "    #siren = \"322120916\" #APPLE FRANCE\n",
    "    plain_text_list = list()\n",
    "    company_article = list()\n",
    "    #binary = True\n",
    "    #sublinear_tf=False\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, ngram_range = (1,1), lowercase=False, sublinear_tf=True)\n",
    "    for document in corpus:\n",
    "        if siren in document[\"siren\"]:\n",
    "            company_article = company_article+document[\"corpus\"]  # add article to company BIG article\n",
    "        else:\n",
    "            plain_text_list.append(document[\"corpus\"]) # otherwise add to corpus\n",
    "\n",
    "    plain_text_list.insert(0,company_article) # add company article to begging of corpus\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(plain_text_list)\n",
    "\n",
    "    #Get the tf-idf scores for the words in the company article complication.\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] # discard tf.idf scores for the other texts\n",
    "\n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df = df.sort_values(by=[\"tfidf\"],ascending=False).head(40) # Take top 40 words\n",
    "\n",
    "    relevant_words_tfidf[siren] = list(zip(list(df.index),list(df[\"tfidf\"])))\n",
    "    #print (relevant_words_tfidf[company])\n",
    "\n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [22:35<00:00, 13.55s/it]\n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [2:19:42<00:00,  4.02s/it] \n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [2:03:31<00:00,  3.56s/it] # binary\n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [1:50:00<00:00,  3.17s/it] # sublinear_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_words_tfidf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save dictionary\n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_tfidf_5articles_\"\n",
    "a_file = open(PATH + file + \".json\", \"w\")\n",
    "json.dump(relevant_words_tfidf, a_file)\n",
    "a_file.close()\n",
    "print (file,\"is saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_words_tfidf_5articles_sublinear_tf is loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# load dictionary \n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_tfidf_5articles_sublinear_tf\"\n",
    "a_file = open(PATH + file + \".json\", \"r\")\n",
    "relevant_words_tfidf = json.load(a_file)\n",
    "#relevant_words_tfidf = dict(relevant_words_tfidf)\n",
    "# check if well loaded\n",
    "print (file,\"is loaded successfully\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant_words_tfidf.keys()\n",
    "#relevant_words_tfidf['419838529']\n",
    "#type(relevant_words_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Relevant Words from ES"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load dictionary \n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_ES\"\n",
    "a_file = open(PATH + file + \".json\", \"r\")\n",
    "relevant_words_es = json.load(a_file)\n",
    "#relevant_words_tfidf = dict(relevant_words_tfidf)\n",
    "\n",
    "\n",
    "# check if well loaded\n",
    "print (file,\"is loaded successfully\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# CLEANING PLAIN TEXT\n",
    "#Input  : Plain text - String\n",
    "#Output : Text removing all punctuation and lowercased\n",
    "#################################################################\n",
    "def clean_plain_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(\"[^a-z0-9]\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Gives a companies \"related score\" wrt an article (using it's significant words)\n",
    "#INPUT :plain_text- String/ word_list - list of significant words\n",
    "#OUTPUT: Score the chances the company is related to the article\n",
    "#################################################################\n",
    "def score_company(plain_text, word_list): \n",
    "    epsilon = 0.0001\n",
    "    avg_word_length =6+1 #+1 counting the spaces\n",
    "    n_words = len(word_list)\n",
    "    words_in_text = 0\n",
    "    #print (word_list)\n",
    "    for word in word_list:\n",
    "        words_in_text +=plain_text.count(word)\n",
    "    #return words_in_text\n",
    "    return words_in_text/(len(plain_text)/avg_word_length + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# For an Article, gives the \"related scores\" for all companies\n",
    "#INPUT :plain_text- String/company related words - dict/ params\n",
    "#OUTPUT: dict of companies and their \"related scores\"\n",
    "#################################################################\n",
    "def text_label_scores(plain_text,related_words,n_sig_words=10, min_score = 0.01):\n",
    "    label_dict = {}\n",
    "    #print (sig_words_list)\n",
    "    for siren in related_words.keys():\n",
    "        #print(\"Company\", company)\n",
    "        sig_words_list = np.array(relevant_words_tfidf[siren])[:n_sig_words,0] # Build significant word list (with no scores)\n",
    "        #print (\"sig_words_list\")\n",
    "        score = score_company(plain_text, sig_words_list)\n",
    "        #print (score)\n",
    "        if score>=min_score:\n",
    "            label_dict[siren]= score\n",
    "    ### Soft_max ###\n",
    "    #sum_exp = sum([np.exp(v) for v in label_dict.values()])\n",
    "    #label_dict = {k: np.exp(v)/sum_exp for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    ### normalizing score ###\n",
    "    #max_val = max(label_dict.values())\n",
    "    #label_dict = {k: v/max_val for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    ### Plain score ###\n",
    "    label_dict = {k: v for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'510302953': 0.13433858807402332, '500034574': 0.1199451679232351, '802798975': 0.1199451679232351, '515151447': 0.1199451679232351, '412460354': 0.1151473612063057, '502240625': 0.1151473612063057, '450804364': 0.1151473612063057, '323159715': 0.1151473612063057, '489869321': 0.1151473612063057, '532501848': 0.10555174777244689, '320050859': 0.10555174777244689}\n"
     ]
    }
   ],
   "source": [
    "# testing text_label_scores\n",
    "plain_text = \"\"\"\n",
    "The New York Times said on Monday that it was exiting its partnership with Apple News, as news organizations struggle to compete with large tech companies for readers‚Äô attention and dollars.\n",
    "\n",
    "Starting on Monday, Times articles were no longer appearing alongside those from other publications in the curated Apple News feed available on Apple devices.\n",
    "\n",
    "The Times is one of the first media organizations to pull out of Apple News. The Times, which has made adding new subscribers a key business goal, said Apple had given it little in the way of direct relationships with readers and little control over the business. It said it hoped to instead drive readers directly to its own website and mobile app so that it could ‚Äúfund quality journalism.‚Äù\n",
    "\n",
    "‚ÄúCore to a healthy model between The Times and the platforms is a direct path for sending those readers back into our environments, where we control the presentation of our report, the relationships with our readers and the nature of our business rules,‚Äù Meredith Kopit Levien, chief operating officer, wrote in a memo to employees. ‚ÄúOur relationship with Apple News does not fit within these parameters.‚Äù\n",
    "\n",
    "An Apple spokesman said that The Times ‚Äúonly offered Apple News a few stories a day,‚Äù and that the company would continue to provide readers with trusted information from thousands of publishers.\n",
    "\n",
    "‚ÄúWe are also committed to supporting quality journalism through the proven business models of advertising, subscriptions and commerce,‚Äù he said.\"\n",
    "\"\"\"\n",
    "plain_text = clean_plain_text(plain_text)\n",
    "\n",
    "related_words = relevant_words_tfidf\n",
    "n_sig_words= 10\n",
    "min_score = 0.1 # nbr of sig words in text\n",
    "#print (plain_text)\n",
    "label_dict = text_label_scores(plain_text,related_words, n_sig_words, min_score)\n",
    "print(label_dict)\n",
    "for key in label_dict.keys(): # Should not trigger\n",
    "    if len(key)>10:\n",
    "        print (key)\n",
    "#find out why label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to return predicted text labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# For an Article, predicts the labels (sirens)\n",
    "#INPUT : plain_text- String/company related words - dict/ params\n",
    "#OUTPUT: dict of companies and their \"related scores\"\n",
    "#################################################################\n",
    "def label_text(plain_text,related_words, n_sig_words= 10, min_score = 0.1):\n",
    "    label_dict = text_label_scores(plain_text,related_words, n_sig_words, min_score)\n",
    "    #print(\"best score\",label_dict[list(label_dict.keys())[0]])\n",
    "    sirens = list(label_dict.keys())\n",
    "    return sirens[:8] # limiting the number of predictions to 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert the list in the sirens to actual list!\n",
    "\n",
    "related_words = relevant_words_tfidf #model related words\n",
    "corpus = X_train_corpus[:100] # pour verifier que on peut sur entrainer\n",
    "#corpus = X_test_corpus[:] # pour tester sur de nouveaux articles\n",
    "model_eval = list()\n",
    "pred_stats = list()\n",
    "for document in tqdm(corpus):\n",
    "    plain_text = document[\"corpus\"]\n",
    "    pred_siren = label_text(plain_text,related_words, n_sig_words= 10, min_score = 0.1)\n",
    "    pred_stats += pred_siren\n",
    "    #print (plain_text)\n",
    "    true_label =document[\"siren\"]\n",
    "    #print (\"true_label\",true_label)\n",
    "    #print(\"pred_siren\",pred_siren)\n",
    "    is_labeled = [0]*len(pred_siren)\n",
    "    for i in range(len(pred_siren)):  # For each prediction list tag the good and bad predictions\n",
    "        for label in true_label:\n",
    "            if pred_siren[i]==label:\n",
    "                is_labeled[i]=1\n",
    "    model_eval.append(is_labeled) \n",
    "    #print (is_labeled)\n",
    "#print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 texts evaluated\n",
      "Accuracy1: 1.0 (with at least ONE label predicted)\n",
      "Accuracy2: 0.9 (with ALL labels predicted)\n",
      "Accuracy3: 0.9 (with ALL labels predicted in the FIRST predictions)\n",
      "Accuracy4: 0.35714285714285715 (Number of correct predictions over total number of predictions overall)\n",
      "Average number of predictions 2.8 vs average number of labels : 1.1\n",
      "The siren that is predicted the most is: 419838529 ( 10 times)\n"
     ]
    }
   ],
   "source": [
    "# How many times (at least) one of the companies are predicted\n",
    "print (\"There are\",len(model_eval),\"texts evaluated\")\n",
    "acc1 = list()\n",
    "for preds in model_eval:\n",
    "    acc1.append(any(preds))\n",
    "print(\"Accuracy1:\", np.sum(acc1)/len(model_eval),\"(with at least ONE label predicted)\")\n",
    "\n",
    "# How many times ALL the labels are present in the prediction. \n",
    "acc2 = list()\n",
    "for i in range(len(model_eval)):\n",
    "    len_label = len(corpus[i][\"siren\"])\n",
    "    n_correct_pred = np.sum(model_eval[i])\n",
    "    if len_label==n_correct_pred:\n",
    "        acc2.append(True)\n",
    "    else:\n",
    "        acc2.append(False)\n",
    "    if len(corpus[i][\"siren\"])<np.sum(model_eval[i]): # Should never trigger\n",
    "        print(\"Error to many good predictions\")   \n",
    "print(\"Accuracy2:\", np.sum(acc2)/len(model_eval),\"(with ALL labels predicted)\")\n",
    "\n",
    "# How many times ALL the labels are present in the prediction and are . \n",
    "acc3 = list()\n",
    "for i in range(len(model_eval)):\n",
    "    len_label = len(corpus[i][\"siren\"])\n",
    "    n_first_correct_pred = np.sum(model_eval[i][:len_label])# Keeping only the len_label first predictions\n",
    "    if len_label==n_first_correct_pred:\n",
    "        acc3.append(True)\n",
    "    else:\n",
    "        acc3.append(False)\n",
    "    if len(corpus[i][\"siren\"])<np.sum(model_eval[i]): # Should never trigger\n",
    "        print(\"Error to many good predictions\") \n",
    "print(\"Accuracy3:\", np.sum(acc3)/len(model_eval),\"(with ALL labels predicted in the FIRST predictions)\")\n",
    "\n",
    "# How many predictions are wrong wrt. how many are right (TRUE, FALSE)\n",
    "true_pred = 0\n",
    "pred = 0\n",
    "for preds in model_eval:\n",
    "    true_pred += np.sum(preds)\n",
    "    pred += len(preds)\n",
    "print(\"Accuracy4:\",true_pred/pred,\"(Number of correct predictions over total number of predictions overall)\")\n",
    "\n",
    "# Average number of predictions vs average number of labels\n",
    "len_label = list()\n",
    "len_pred = list()\n",
    "for i in range(len(model_eval)):\n",
    "    len_label.append(len(corpus[i][\"siren\"]))\n",
    "    len_pred.append(len(model_eval[i]))\n",
    "print(\"Average number of predictions\",np.mean(len_pred),\"vs average number of labels :\", np.mean(len_label))\n",
    "\n",
    "# Most commun labels predicted\n",
    "count_pred = dict()\n",
    "for siren in pred_stats:\n",
    "    if siren in count_pred.keys():\n",
    "        count_pred[siren] +=1\n",
    "    else:\n",
    "        count_pred[siren] = 1\n",
    "key_max = list(filter(lambda t: t[1]==max(count_pred.values()), count_pred.items()))[0][0] \n",
    "print(\"The siren that is predicted the most is:\",key_max,\"(\",np.max(list(count_pred.values())),\"times)\")\n",
    "#sns.catplot(x=\"deck\", kind=\"count\", palette=\"ch:.25\", data=pred_stats)\n",
    "# Influence of the min_score/ n_sig_words on the prediction.\n",
    "\n",
    "# Influence on the prediction protocol influence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
