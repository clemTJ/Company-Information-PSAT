{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading french texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching data\n",
    "PATH = \"./data/ArticleCompany_2020-11-17/\"\n",
    "coprus = \"corpus_check_long_SIREN_UPDATED2\"\n",
    "names = \"siren_name_map_clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_name = pd.read_json(PATH + names +\".json\")\n",
    "with open(PATH + names +\".json\") as json_file: \n",
    "    dict_names = json.load(json_file) \n",
    "\n",
    "with open(PATH + coprus +\".json\") as json_file: \n",
    "    corpus_list = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57540 articles in the corpus\n",
      "There are 30178 companies in the list\n"
     ]
    }
   ],
   "source": [
    "print (\"There are\", len(corpus_list), \"articles in the corpus\")\n",
    "print (\"There are\", len(dict_names), \"companies in the list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compter Nombre d'Entreprises sans Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28690 companies with labels out of the 30178 companies\n",
      "there are 1488 companies with no articles\n",
      "95.07 % of the companies have articles\n"
     ]
    }
   ],
   "source": [
    "dict_count = dict()\n",
    "#for company in dict_names.keys(): dict_count[company] = 0\n",
    "for document in corpus_list:\n",
    "    #print (document[\"siren\"])\n",
    "    sir_list = document[\"siren\"][1:-1].split(\", \")\n",
    "    #print (sir_list)\n",
    "    for siren in sir_list:\n",
    "        #print (siren)\n",
    "        if siren in dict_count.keys():\n",
    "            dict_count[siren] +=1\n",
    "        else:\n",
    "            dict_count[siren] = 1\n",
    "print (\"There are\",len(dict_count.keys()),\"companies with labels out of the\", len(dict_names.keys()), \"companies\")\n",
    "print (\"there are\",len(dict_names.keys())-len(dict_count.keys()),\"companies with no articles\")\n",
    "print (round(len(dict_count)/(len(dict_names))*100,2),\"% of the companies have articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quels sont les entreprises sans articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_no_acticle_companies = dict()\n",
    "for company in dict_names.keys():\n",
    "    if company not in dict_count.keys():\n",
    "        dict_no_acticle_companies[company] = dict_names[company] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etudes du nombre d'articles associer a chaque entreprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=28690, minmax=(1, 175), mean=2.502195887068665, variance=28.018556298899046, skewness=11.346985559301654, kurtosis=211.55000716780165)\n",
      "There are 63.58 % articles with one associated article\n",
      "There are 90.42 % articles with less than 5 associated article\n"
     ]
    }
   ],
   "source": [
    "# On prendre seulement les entreprises avec au moins un articles associer\n",
    "#sns.set(rc={'figure.figsize':(40,5)})\n",
    "values = list(dict_count.values())\n",
    "#sns.displot(values, binwidth=3) #bins=20\n",
    "\n",
    "number = 5\n",
    "print(stats.describe(values))\n",
    "print (\"There are\",round(values.count(1)/len(values)*100,2), \"% articles with one associated article\")\n",
    "under_n = [1 for i in values if i < number]\n",
    "print (\"There are\",round(len(under_n)/len(values)*100,2), \"% articles with less than\",number,\"associated article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'552100554'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(dict_count, key=dict_count.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and remove stop words of Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words 157\n",
      "Ex: ['au', 'aux', 'avec', 'ce', 'ces']\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of stop words\",len(stop_words ))\n",
    "print (\"Ex:\",stop_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57540/57540 [04:03<00:00, 235.94it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_cleaned = list(corpus_list)\n",
    "for document in tqdm(corpus_cleaned):\n",
    "    plain_text = document[\"corpus\"]\n",
    "    plain_text = plain_text.lower()\n",
    "    plain_text= re.sub(r'\\s+', ' ', plain_text)\n",
    "    #plain_text = re.sub(\"[^a-z0-9]\", ' ', plain_text)\n",
    "    plain_text = re.sub(\"[^a-z]\", ' ', plain_text)\n",
    "    plain_text = re.sub(r'\\s+', ' ', plain_text)\n",
    "    #remove one letter words?\n",
    "    #remove numbers?\n",
    "    pt_words = word_tokenize(plain_text)\n",
    "    cleaned_words =list()\n",
    "    for word in pt_words:\n",
    "        if len(word)>1:\n",
    "            if word not in stop_words:\n",
    "                cleaned_words.append(word)\n",
    "    document[\"corpus\"] = cleaned_words\n",
    "# 100%|██████████| 57540/57540 [03:30<00:00, 273.74it/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf.Idf pour une entreprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['419838529', '813883964', '572060333', '542104245', '399258755']\n",
      "SPIE OPERATIONS\n",
      "322120916 APPLE FRANCE\n",
      "APPLE FRANCE a 7 articles dans le corpus\n"
     ]
    }
   ],
   "source": [
    "corpus_cleaned[0]\n",
    "print(list(dict_names.keys())[0:5])\n",
    "print (dict_names['399258755'])\n",
    "name_search = \"APPLE FRANCE\"\n",
    "for siren, name in dict_names.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "    if name_search in name:\n",
    "        print(siren, name)\n",
    "print(\"APPLE FRANCE a\",dict_count[\"322120916\"],\"articles dans le corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2084"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number = 5\n",
    "siren_filtered =[key for key in dict_count if dict_count[key] > number]\n",
    "len(siren_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [22:35<00:00, 13.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# Tf.Idf on Companies that have Associated Articles \n",
    "relevant_words_tfidf = {}\n",
    "#list_siren = list(dict_count.keys())\n",
    "list_siren = siren_filtered[:100]\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "for siren in tqdm(list_siren):\n",
    "    #siren = \"322120916\" #APPLE FRANCE\n",
    "    plain_text_list = list()\n",
    "    company_article = list()\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, ngram_range = (1,1), lowercase=False)\n",
    "\n",
    "    for document in corpus_cleaned:\n",
    "        if siren in document[\"siren\"]:\n",
    "            company_article = company_article+document[\"corpus\"]  # add article to company BIG article\n",
    "        else:\n",
    "            plain_text_list.append(document[\"corpus\"]) # otherwise add to corpus\n",
    "\n",
    "    plain_text_list.insert(0,company_article) # add company article to begging of corpus\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(plain_text_list)\n",
    "\n",
    "    #Get the tf-idf scores for the words in the company article complication.\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] # discard tf.idf scores for the other texts\n",
    "\n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df = df.sort_values(by=[\"tfidf\"],ascending=False).head(40) # Take top 40 words\n",
    "\n",
    "    relevant_words_tfidf[siren] = list(zip(list(df.index),list(df[\"tfidf\"])))\n",
    "    #print (relevant_words_tfidf[company])\n",
    "\n",
    "#100%|██████████| 100/100 [22:35<00:00, 13.55s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_words_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_tfidf_5articles_small\"\n",
    "a_file = open(PATH + file + \".json\", \"w\")\n",
    "json.dump(relevant_words_tfidf, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary \n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_tfidf_5articles_small\"\n",
    "a_file = open(PATH + file + \".json\", \"r\")\n",
    "relevant_words_tfidf = json.load(a_file)\n",
    "#relevant_words_tfidf = dict(relevant_words_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant_words_tfidf.keys()\n",
    "#relevant_words_tfidf['419838529']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
