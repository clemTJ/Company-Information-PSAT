{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de textes Francais par entreprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation de donn√©e avec label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import ast\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching data\n",
    "PATH = \"./data/ArticleCompany_2020-11-17/\"\n",
    "coprus = \"corpus_check_long_SIREN_UPDATED2\"\n",
    "names = \"siren_name_map_clean\"\n",
    "\n",
    "with open(PATH + names +\".json\") as json_file: \n",
    "    dict_names = json.load(json_file) \n",
    "\n",
    "with open(PATH + coprus +\".json\") as json_file: \n",
    "    corpus_list = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The siren list is: <class 'str'>\n",
      "NOW the type of the siren list is: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Convert string to list of labels\n",
    "print (\"The siren list is:\",type(corpus_list[0][\"siren\"]))\n",
    "for document in corpus_list:\n",
    "    document[\"siren\"] = ast.literal_eval(document[\"siren\"]) # convert list in string format to list\n",
    "    for i in range(len(document[\"siren\"])): # Convert each int siren to string \n",
    "        document[\"siren\"][i] = str(document[\"siren\"][i])\n",
    "print (\"NOW the type of the siren list is:\",type(corpus_list[0][\"siren\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove articles with no text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of removed article: 29805   \n",
      "Index of removed article: 30158     \n"
     ]
    }
   ],
   "source": [
    "corpus_list_inter = list()\n",
    "for i in range(len(corpus_list)):\n",
    "    if len(corpus_list[i][\"corpus\"])<100: # small enough\n",
    "        text = corpus_list[i][\"corpus\"]\n",
    "        text = re.sub(\"^(\\s+)\", '', text)\n",
    "        if (len(text)>0):\n",
    "            corpus_list_inter.append(corpus_list[i])\n",
    "        else:\n",
    "            print (\"Index of removed article:\",i,corpus_list[i][\"corpus\"])\n",
    "    else:\n",
    "        corpus_list_inter.append(corpus_list[i])\n",
    "corpus_list = corpus_list_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57538 articles in the corpus\n",
      "There are 30178 companies in the list\n"
     ]
    }
   ],
   "source": [
    "print (\"There are\", len(corpus_list), \"articles in the corpus\")\n",
    "print (\"There are\", len(dict_names), \"companies in the list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compter Nombre d'Entreprises sans Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28690 companies with labels out of the 30178 companies\n",
      "there are 1488 companies with no articles\n",
      "95.07 % of the companies have articles\n",
      "Each article of the corpus has: dict_keys(['id', 'siren', 'corpus', 'url_article'])\n"
     ]
    }
   ],
   "source": [
    "dict_count = dict()\n",
    "#for company in dict_names.keys(): dict_count[company] = 0\n",
    "for document in corpus_list:\n",
    "    sir_list = document[\"siren\"]\n",
    "    for siren in sir_list:\n",
    "        if len(siren)>10: # Should not be triggered\n",
    "            print (\"ALERT:\",siren)\n",
    "        if siren in dict_count.keys():\n",
    "            dict_count[siren] +=1\n",
    "        else:\n",
    "            dict_count[siren] = 1\n",
    "print (\"There are\",len(dict_count.keys()),\"companies with labels out of the\", len(dict_names.keys()), \"companies\")\n",
    "print (\"there are\",len(dict_names.keys())-len(dict_count.keys()),\"companies with no articles\")\n",
    "print (round(len(dict_count)/(len(dict_names))*100,2),\"% of the companies have articles\")\n",
    "print (\"Each article of the corpus has:\",corpus_list[0].keys())\n",
    "#corpus_list[0][\"corpus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quels sont les entreprises sans articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_no_acticle_companies = dict()\n",
    "for company in dict_names.keys():\n",
    "    if company not in dict_count.keys():\n",
    "        dict_no_acticle_companies[company] = dict_names[company] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_count.keys():\n",
    "    if len(key)>10:\n",
    "        print (key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etude du nombre d'articles par entreprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=57538, minmax=(1, 29), mean=1.2476276547672842, variance=0.6042338178020358, skewness=8.133981486117998, kurtosis=136.56851777466036)\n",
      "There are 9362 arcticles with more that one tag out of the 57538 articles\n"
     ]
    }
   ],
   "source": [
    "multiple_siren = 0\n",
    "multiple_siren_list = list()\n",
    "for document in corpus_list:\n",
    "    if len(document[\"siren\"])==0:\n",
    "        print (\"ALERT article sans tag, id:\",document[\"id\"])\n",
    "    if len(document[\"siren\"])>1:\n",
    "        multiple_siren +=1\n",
    "    multiple_siren_list.append(len(document[\"siren\"]))\n",
    "    \n",
    "print(stats.describe(multiple_siren_list))   \n",
    "print (\"There are\",multiple_siren,\"arcticles with more that one tag out of the\",len(corpus_list),\"articles\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etudes du nombre d'articles associer a chaque entreprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=28690, minmax=(1, 175), mean=2.5021261763680727, variance=28.017371476985655, skewness=11.347634082651822, kurtosis=211.56847613512954)\n",
      "There are 63.58 % articles with one associated article\n",
      "There are 90.42 % articles with less than 5 associated article\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On prendre seulement les entreprises avec au moins un articles associer\n",
    "#sns.set(rc={'figure.figsize':(40,5)})\n",
    "values = list(dict_count.values())\n",
    "sns.displot(values, binwidth=1) #bins=20\n",
    "plt.xlim(0, 10)\n",
    "\n",
    "number = 5\n",
    "print(stats.describe(values))\n",
    "print (\"There are\",round(values.count(1)/len(values)*100,2), \"% articles with one associated article\")\n",
    "under_n = [1 for i in values if i < number]\n",
    "print (\"There are\",round(len(under_n)/len(values)*100,2), \"% articles with less than\",number,\"associated article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize et suppression de stop words du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words 157\n",
      "Ex: ['au', 'aux', 'avec', 'ce', 'ces']\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of stop words\",len(stop_words ))\n",
    "print (\"Ex:\",stop_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57540/57540 [03:55<00:00, 244.24it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_cleaned = deepcopy(corpus_list)\n",
    "for document in tqdm(corpus_cleaned):\n",
    "    plain_text = document[\"corpus\"]\n",
    "    plain_text = plain_text.lower()\n",
    "    plain_text= re.sub(r'\\s+', ' ', plain_text)\n",
    "    #plain_text = re.sub(\"[^a-z0-9]\", ' ', plain_text)\n",
    "    plain_text = re.sub(\"[^a-z]\", ' ', plain_text)\n",
    "    plain_text = re.sub(r'\\s+', ' ', plain_text)\n",
    "    #remove one letter words?\n",
    "    #remove numbers?\n",
    "    pt_words = word_tokenize(plain_text)\n",
    "    cleaned_words =list()\n",
    "    for word in pt_words:\n",
    "        if len(word)>1:\n",
    "            if word not in stop_words:\n",
    "                cleaned_words.append(word)\n",
    "    document[\"corpus\"] = cleaned_words\n",
    "# 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57540/57540 [03:30<00:00, 273.74it/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize en gardant que les Noms "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#pip install stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/pierre/stanfordnlp_resources/fr_gsd_models/fr_gsd_tokenizer.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/pierre/stanfordnlp_resources/fr_gsd_models/fr_gsd_tagger.pt', 'pretrain_path': '/Users/pierre/stanfordnlp_resources/fr_gsd_models/fr_gsd.pretrain.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "#stanfordnlp.download('fr')   # This downloads the French models for the neural pipeline\n",
    "#nlp = stanfordnlp.Pipeline(lang=\"fr\",processors = \"tokenize,mwt,lemma,pos\") # This sets up a default neural pipeline in French\n",
    "nlp = stanfordnlp.Pipeline(lang=\"fr\",processors = \"tokenize,pos\")\n",
    "#Documentation:\n",
    "#https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les : DET _\n",
      "victoires : NOUN _\n",
      "de : ADP _\n",
      "Joe : PROPN _\n",
      "Biden : PROPN _\n",
      "√† : ADP _\n",
      "la : DET _\n",
      "pr√©sidentielle : ADJ _\n",
      "am√©ricaine : ADJ _\n",
      "√† : ADP _\n",
      "peine : NOUN _\n",
      "proclam√©e : VERB _\n",
      "par : ADP _\n",
      "les : DET _\n",
      "principaux : ADJ _\n",
      "m√©dias : NOUN _\n",
      "am√©ricains : ADJ _\n",
      ". : PUNCT _\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Word index=1;text=Les;upos=DET;xpos=_;feats=Definite=Def|Gender=Fem|Number=Plur|PronType=Art>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exemple\n",
    "doc = nlp(\"Les victoires de Joe Biden √† la pr√©sidentielle am√©ricaine √† peine proclam√©e par les principaux m√©dias am√©ricains.\")  \n",
    "#extract_pos(doc)\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(word.text,\":\", word.upos, word.pos)\n",
    "doc.sentences[0].words[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = [\"NOUN\",\"PROPN\"]\n",
    "#corpus_nouns = deepcopy(corpus_list)\n",
    "#for document in tqdm(corpus_nouns):\n",
    "\n",
    "for i in range(29805,len(corpus_nouns)):\n",
    "    document = corpus_nouns[i]\n",
    "    plain_text = document[\"corpus\"]\n",
    "    doc = nlp(plain_text)\n",
    "    cleaned_words =list()\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            pos_tag = word.upos\n",
    "            if pos_tag in keep:\n",
    "                cleaned_words.append(word.text.lower())\n",
    "    document[\"corpus\"] = cleaned_words \n",
    "#20hours of computation to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57538"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_nouns is saved successfully\n"
     ]
    }
   ],
   "source": [
    "# save corpus_nouns\n",
    "PATH = \"./data/ArticleCompany_2020-11-17/\"\n",
    "file = \"corpus_nouns\"\n",
    "a_file = open(PATH + file + \".json\", \"w\")\n",
    "json.dump(corpus_nouns, a_file)\n",
    "a_file.close()\n",
    "print (file,\"is saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus_nouns \n",
    "PATH = \"./data/ArticleCompany_2020-11-17/\"\n",
    "file = \"corpus_nouns\"\n",
    "with open(PATH + file +\".json\") as json_file: \n",
    "    corpus_nouns = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The siren list is: <class 'list'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print (\"The siren list is:\",type(corpus_nouns[0][\"siren\"]), type(corpus_nouns[0][\"corpus\"]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Clean up the lists\n",
    "# Convert string to list of labels\n",
    "print (\"The siren list is:\",type(corpus_nouns[0][\"siren\"]), type(corpus_nouns[0][\"corpus\"]))\n",
    "for document in corpus_nouns:\n",
    "    #document[\"siren\"] = ast.literal_eval(document[\"siren\"]) # convert list in string format to list\n",
    "    document[\"corpus\"] = ast.literal_eval(document[\"corpus\"]) # convert list in string format to list\n",
    "    #for i in range(len(document[\"siren\"])): # Convert each int siren to string \n",
    "    #    document[\"siren\"][i] = str(document[\"siren\"][i])\n",
    "    for i in range(len(document[\"corpus\"])): # Convert each word to string # might not be necessary\n",
    "        document[\"corpus\"][i] = str(document[\"corpus\"][i])\n",
    "print (\"NOW the type of the siren list is:\",type(corpus_nouns[0][\"siren\"]), type(corpus_nouns[0][\"corpus\"]))\n",
    "\n",
    "# check if well loaded\n",
    "print (file,\"is loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction d'entreprise avec plus de n articles sur elles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['419838529', '813883964', '572060333', '542104245', '399258755']\n",
      "SPIE OPERATIONS\n",
      "322120916 APPLE FRANCE\n",
      "APPLE FRANCE a 7 articles dans le corpus\n"
     ]
    }
   ],
   "source": [
    "print(list(dict_names.keys())[0:5])\n",
    "print (dict_names['399258755'])\n",
    "name_search = \"APPLE FRANCE\"\n",
    "for siren, name in dict_names.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "    if name_search in name:\n",
    "        print(siren, name)\n",
    "print(\"APPLE FRANCE a\",dict_count[\"322120916\"],\"articles dans le corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2084 companies with MORE than 5 associated articles\n"
     ]
    }
   ],
   "source": [
    "number = 5 # Number of articles a company must have to be kept in the list\n",
    "siren_filtered =[key for key in dict_count if dict_count[key] > number]\n",
    "print (\"There are\",len(siren_filtered),\"companies with MORE than\",number,\"associated articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in siren_filtered:\n",
    "    if len(key)>10:\n",
    "        print (key)\n",
    "#find out why label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation de Train et Test set pour l'entrainement de Tf.Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing unwanted articles\n",
      "We removed: 29189 articles and we have 28349 left\n",
      "Splitting data\n",
      "We have 19846 documents in the training corpus\n",
      "We have 8503 documents in the testing corpus\n"
     ]
    }
   ],
   "source": [
    "# Remove all of the articles that dont talk about our selected companies (in siren filtered)\n",
    "# Split corpus train/test\n",
    "#corpus = corpus_cleaned\n",
    "corpus = corpus_nouns\n",
    "train_size = 0.7\n",
    "X_train_corpus = list()\n",
    "X_test_corpus = list()\n",
    "\n",
    "#Removing unwanted articles\n",
    "print(\"Removing unwanted articles\")\n",
    "corpus_temp = list()\n",
    "for document in corpus:\n",
    "    keep = False\n",
    "    for document_sirens in document[\"siren\"]:\n",
    "        for sirens in siren_filtered:\n",
    "            if document_sirens == sirens:\n",
    "                keep = True\n",
    "    if keep:\n",
    "        corpus_temp.append(document)\n",
    "print (\"We removed:\",len(corpus)-len(corpus_temp),\"articles and we have\",len(corpus_temp),\"left\")\n",
    "corpus = corpus_temp\n",
    " \n",
    "#Splitting data\n",
    "print(\"Splitting data\") \n",
    "for document in corpus:\n",
    "    if (random.uniform(0, 1)<train_size):\n",
    "        X_train_corpus.append(document)\n",
    "    else:\n",
    "        X_test_corpus.append(document)\n",
    "print (\"We have\",len(X_train_corpus),\"documents in the training corpus\")\n",
    "print (\"We have\",len(X_test_corpus),\"documents in the testing corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf.Idf pour une liste d'entreprise sur le training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [1:21:23<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# Tf.Idf on Companies that have Associated Articles \n",
    "\n",
    "relevant_words_tfidf = {}\n",
    "corpus = X_train_corpus # corpus\n",
    "\n",
    "list_siren = siren_filtered\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "for siren in tqdm(list_siren):\n",
    "    #siren = \"322120916\" #APPLE FRANCE\n",
    "    plain_text_list = list()\n",
    "    company_article = list()\n",
    "    #binary = True\n",
    "    #sublinear_tf=False\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, ngram_range = (1,1), lowercase=False, sublinear_tf=True)\n",
    "    for document in corpus:\n",
    "        if siren in document[\"siren\"]:\n",
    "            company_article = company_article+document[\"corpus\"]  # add article to company BIG article\n",
    "        else:\n",
    "            plain_text_list.append(document[\"corpus\"]) # otherwise add to corpus\n",
    "\n",
    "    plain_text_list.insert(0,company_article) # add company article to begging of corpus\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(plain_text_list)\n",
    "\n",
    "    #Get the tf-idf scores for the words in the company article complication.\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] # discard tf.idf scores for the other texts\n",
    "\n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df = df.sort_values(by=[\"tfidf\"],ascending=False).head(40) # Take top 40 words\n",
    "\n",
    "    relevant_words_tfidf[siren] = list(zip(list(df.index),list(df[\"tfidf\"])))\n",
    "    #print (relevant_words_tfidf[company])\n",
    "\n",
    "\n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [2:19:42<00:00,  4.02s/it] # tokenized tf\n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [2:03:31<00:00,  3.56s/it] # tokenized binary\n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [1:50:00<00:00,  3.17s/it] # tokenized sublinear_tf\n",
    "#100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2084/2084 [1:21:23<00:00,  2.34s/it] # nouns sublinear_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_words_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_words_tfidf_nouns_sublinear_tf is saved successfully\n"
     ]
    }
   ],
   "source": [
    "# save dictionary\n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_tfidf_nouns_sublinear_tf\"\n",
    "a_file = open(PATH + file + \".json\", \"w\")\n",
    "json.dump(relevant_words_tfidf, a_file)\n",
    "a_file.close()\n",
    "print (file,\"is saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_words_tfidf_nouns_sublinear_tf is loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# load dictionary \n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_tfidf_nouns_sublinear_tf\"\n",
    "a_file = open(PATH + file + \".json\", \"r\")\n",
    "relevant_words_tfidf = json.load(a_file)\n",
    "#relevant_words_tfidf = dict(relevant_words_tfidf)\n",
    "# check if well loaded\n",
    "print (file,\"is loaded successfully\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant_words_tfidf.keys()\n",
    "#relevant_words_tfidf['419838529']\n",
    "#type(relevant_words_tfidf)\n",
    "#len(relevant_words_tfidf.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Relevant Words from ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_words_train is loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# load dictionary \n",
    "PATH = \"./relevant_words/francais/\"\n",
    "file = \"relevant_words_train\"\n",
    "a_file = open(PATH + file + \".json\", \"r\")\n",
    "relevant_words_es = json.load(a_file)\n",
    "#relevant_words_tfidf = dict(relevant_words_tfidf)\n",
    "\n",
    "\n",
    "# check if well loaded\n",
    "print (file,\"is loaded successfully\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building baseline model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labeling article if it has the company name in it\n",
    "relevant_words_baseline = dict()\n",
    "for key in dict_names.keys():\n",
    "    #print (key)\n",
    "    #print([dict_names[key].lower()])\n",
    "    relevant_words_baseline[key] = [[dict_names[key].lower(),1]]\n",
    "#relevant_words_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# CLEANING PLAIN TEXT\n",
    "#Input  : Plain text - String\n",
    "#Output : Text removing all punctuation and lowercased\n",
    "#################################################################\n",
    "def clean_plain_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(\"[^a-z0-9]\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Gives a companies \"related score\" wrt an article (using it's significant words)\n",
    "#INPUT :plain_text- String/ word_list - list of significant words\n",
    "#OUTPUT: Score the chances the company is related to the article\n",
    "#################################################################\n",
    "def score_company(plain_text, word_list): \n",
    "    epsilon = 0.0001\n",
    "    avg_word_length =6+1 #+1 counting the spaces\n",
    "    n_words = len(word_list)\n",
    "    words_in_text = 0\n",
    "    #print (word_list)\n",
    "    for word in word_list:\n",
    "        words_in_text +=plain_text.count(word)\n",
    "    #return words_in_text\n",
    "    return words_in_text/(len(plain_text)/avg_word_length + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# For an Article, gives the \"related scores\" for all companies\n",
    "#INPUT :plain_text- String/company related words - dict/ params\n",
    "#OUTPUT: dict of companies and their \"related scores\"\n",
    "#################################################################\n",
    "def text_label_scores(plain_text,related_words,n_sig_words=10, min_score = 0.01):\n",
    "    label_dict = {}\n",
    "    #print (sig_words_list)\n",
    "    for siren in related_words.keys():\n",
    "        #print(related_words[siren])\n",
    "        sig_words_list = np.array(related_words[siren])[:n_sig_words,0] # Build significant word list (with no scores)\n",
    "        #print (\"sig_words_list\")\n",
    "        score = score_company(plain_text, sig_words_list)\n",
    "        #print (score)\n",
    "        if score>=min_score:\n",
    "            label_dict[siren]= score\n",
    "    ### Soft_max ###\n",
    "    #sum_exp = sum([np.exp(v) for v in label_dict.values()])\n",
    "    #label_dict = {k: np.exp(v)/sum_exp for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    ### normalizing score ###\n",
    "    #max_val = max(label_dict.values())\n",
    "    #label_dict = {k: v/max_val for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    ### Plain score ###\n",
    "    label_dict = {k: v for k, v in sorted(label_dict.items(), key=lambda item: -item[1])}\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'504914094': 0.37421301273865115, '315127944': 0.35220048257755404, '533727111': 0.33018795241645693, '805170032': 0.33018795241645693, '790003032': 0.30817542225535977, '818435240': 0.30817542225535977, '502220056': 0.28616289209426266, '444180434': 0.28616289209426266, '304602956': 0.2421378317720684, '810146563': 0.2421378317720684, '334181708': 0.15408771112767988, '433925229': 0.15408771112767988, '413967159': 0.11006265080548563, '444465736': 0.11006265080548563, '433691862': 0.11006265080548563, '352973622': 0.11006265080548563}\n"
     ]
    }
   ],
   "source": [
    "# testing text_label_scores\n",
    "plain_text = \"\"\"\n",
    "La victoire de Joe Biden √† la pr√©sidentielle am√©ricaine √† peine proclam√©e par les principaux \n",
    "m√©dias am√©ricains, les messages de f√©licitations des dirigeants occidentaux affluent. Sur Twitter,\n",
    "une courte s√©quence vid√©o fait le buzz entre Londres et Dublin. Ce 7 novembre, on y voit le \n",
    "candidat d√©mocrate entour√© de journalistes.\n",
    "\"\"\"\n",
    "plain_text = clean_plain_text(plain_text)\n",
    "\n",
    "related_words = relevant_words_tfidf\n",
    "n_sig_words= 10\n",
    "min_score = 0.1 # nbr of sig words in text\n",
    "#print (plain_text)\n",
    "label_dict = text_label_scores(plain_text,related_words, n_sig_words, min_score)\n",
    "print(label_dict)\n",
    "for key in label_dict.keys(): # Should not trigger\n",
    "    if len(key)>10:\n",
    "        print (key)\n",
    "#find out why label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to return predicted text labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# For an Article, predicts the labels (sirens)\n",
    "#INPUT : plain_text- String/company related words - dict/ params\n",
    "#OUTPUT: dict of companies and their \"related scores\"\n",
    "#################################################################\n",
    "def label_text(plain_text,related_words, n_sig_words= 10, min_score = 0.1):\n",
    "    label_dict = text_label_scores(plain_text,related_words, n_sig_words, min_score)\n",
    "    #print(\"best score\",label_dict[list(label_dict.keys())[0]])\n",
    "    sirens = list(label_dict.keys())\n",
    "    return sirens[:8] # limiting the number of predictions to 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [01:15<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert the list in the sirens to actual list!\n",
    "\n",
    "#related_words = relevant_words_tfidf #model related words\n",
    "#related_words = relevant_words_es\n",
    "related_words = relevant_words_baseline\n",
    "corpus = X_train_corpus[:300] # pour verifier que on peut sur entrainer\n",
    "#corpus = X_test_corpus[:300] # pour tester sur de nouveaux articles\n",
    "\n",
    "pred_eval = list() # Tag each prediction 1:correct, 0:wrong for each article\n",
    "pred_labels = list() # Siren predicted for each article\n",
    "article_eval = list() # Tag each label if 1:predicted, 0:not predicted for each article\n",
    "article_labels = list() # Siren labels for each article\n",
    "pred_stats = list() #list all predicted sirens\n",
    "label_stats = list() # iist of all siren labels\n",
    "for document in tqdm(corpus):\n",
    "    plain_text = document[\"corpus\"]\n",
    "    #pred_labels\n",
    "    pred_siren = label_text(plain_text,related_words, n_sig_words= 10, min_score = 0.1)\n",
    "    pred_labels.append(pred_siren)\n",
    "    \n",
    "    #pred_stats\n",
    "    pred_stats += pred_siren\n",
    "    \n",
    "    #article_labels\n",
    "    true_label =document[\"siren\"]\n",
    "    article_labels.append(true_label)\n",
    "    \n",
    "    #pred_eval\n",
    "    is_labeled = [0]*len(pred_siren)\n",
    "    for i in range(len(pred_siren)):  # For each prediction list tag the good and bad predictions\n",
    "        for label in true_label:\n",
    "            if pred_siren[i]==label:\n",
    "                is_labeled[i]=1\n",
    "    pred_eval.append(is_labeled) \n",
    "    \n",
    "    #article_eval\n",
    "    is_predicted = [0]*len(true_label)\n",
    "    for i in range(len(true_label)):  # For each prediction list tag the good and bad predictions\n",
    "        for pred in pred_siren:\n",
    "            if true_label[i]==pred:\n",
    "                is_predicted[i]=1\n",
    "    article_eval.append(is_predicted) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 300 texts evaluated\n",
      "Accuracy1: 0.6566666666666666 (with at least ONE label predicted)\n",
      "Accuracy2: 0.5866666666666667 (with ALL labels predicted)\n",
      "Accuracy3: 0.5733333333333334 (with ALL labels predicted in the FIRST predictions)\n",
      "Accuracy4: 0.6623376623376623 (Number of correct predictions over total number of predictions overall)\n",
      "Average number of predictions 1.0266666666666666 vs average number of labels : 1.1433333333333333\n",
      "The siren that is predicted the most is: 542107651 ( 96 times)\n",
      "Precision: 0.5661111111111112 AVG(L(a) correcte/Somme de L(a))\n",
      "Rappel: 0.621 AVG((nombre de label correctes/Nombre de label vrai ))\n"
     ]
    }
   ],
   "source": [
    "# How many times (at least) one of the companies are predicted\n",
    "print (\"There are\",len(pred_eval),\"texts evaluated\")\n",
    "acc1 = list()\n",
    "for preds in pred_eval:\n",
    "    acc1.append(any(preds))\n",
    "print(\"Accuracy1:\", np.sum(acc1)/len(pred_eval),\"(with at least ONE label predicted)\")\n",
    "\n",
    "# How many times ALL the labels are present in the prediction. \n",
    "acc2 = list()\n",
    "for i in range(len(pred_eval)):\n",
    "    len_label = len(corpus[i][\"siren\"])\n",
    "    n_correct_pred = np.sum(pred_eval[i])\n",
    "    if len_label==n_correct_pred:\n",
    "        acc2.append(True)\n",
    "    else:\n",
    "        acc2.append(False)\n",
    "    if len(corpus[i][\"siren\"])<np.sum(pred_eval[i]): # Should never trigger\n",
    "        print(\"Error to many good predictions\")   \n",
    "print(\"Accuracy2:\", np.sum(acc2)/len(pred_eval),\"(with ALL labels predicted)\")\n",
    "\n",
    "# How many times ALL the labels are present in the prediction and are . \n",
    "acc3 = list()\n",
    "for i in range(len(pred_eval)):\n",
    "    len_label = len(corpus[i][\"siren\"])\n",
    "    n_first_correct_pred = np.sum(pred_eval[i][:len_label])# Keeping only the len_label first predictions\n",
    "    if len_label==n_first_correct_pred:\n",
    "        acc3.append(True)\n",
    "    else:\n",
    "        acc3.append(False)\n",
    "    if len(corpus[i][\"siren\"])<np.sum(pred_eval[i]): # Should never trigger\n",
    "        print(\"Error to many good predictions\") \n",
    "print(\"Accuracy3:\", np.sum(acc3)/len(pred_eval),\"(with ALL labels predicted in the FIRST predictions)\")\n",
    "\n",
    "# How many predictions are wrong wrt. how many are right (TRUE, FALSE)\n",
    "true_pred = 0\n",
    "pred = 0\n",
    "for preds in pred_eval:\n",
    "    true_pred += np.sum(preds)\n",
    "    pred += len(preds)\n",
    "print(\"Accuracy4:\",true_pred/pred,\"(Number of correct predictions over total number of predictions overall)\")\n",
    "\n",
    "# Average number of predictions vs average number of labels\n",
    "len_label = list()\n",
    "len_pred = list()\n",
    "for i in range(len(pred_eval)):\n",
    "    len_label.append(len(corpus[i][\"siren\"]))\n",
    "    len_pred.append(len(pred_eval[i]))\n",
    "print(\"Average number of predictions\",np.mean(len_pred),\"vs average number of labels :\", np.mean(len_label))\n",
    "\n",
    "# Most commun labels predicted\n",
    "count_pred = dict()\n",
    "for siren in pred_stats:\n",
    "    if siren in count_pred.keys():\n",
    "        count_pred[siren] +=1\n",
    "    else:\n",
    "        count_pred[siren] = 1\n",
    "key_max = list(filter(lambda t: t[1]==max(count_pred.values()), count_pred.items()))[0][0] \n",
    "print(\"The siren that is predicted the most is:\",key_max,\"(\",np.max(list(count_pred.values())),\"times)\")\n",
    "#sns.catplot(x=\"deck\", kind=\"count\", palette=\"ch:.25\", data=pred_stats)\n",
    "\n",
    "#Precision : Somme :a dans A ((Somme de L(a) correcte) /( Somme de L(a)))/ |A|\n",
    "# True pos/(True Pos + False Pos) -> average for each siren\n",
    "avg_sum = 0.0\n",
    "for preds in pred_eval:\n",
    "    if len(preds)>0:\n",
    "        avg_sum += sum(preds)/len(preds)\n",
    "\n",
    "print(\"Precision:\", avg_sum/len(pred_eval),\"AVG(L(a) correcte/Somme de L(a))\")\n",
    "\n",
    "#Rappel : pour chaque article (nombre de label correctes/Nombre de label vrai )\n",
    "# True pos/(True Pos + False Neg) -> average for each siren\n",
    "avg_sum = 0.0\n",
    "for labels in article_eval:\n",
    "    if len(labels)>0:\n",
    "        avg_sum += sum(labels)/len(labels)\n",
    "print(\"Rappel:\",avg_sum/len(article_eval),\"AVG((nombre de label correctes/Nombre de label vrai ))\")\n",
    "\n",
    "# Influence of the min_score/ n_sig_words on the prediction.\n",
    "\n",
    "# Influence on the prediction protocol influence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [] True_pos/(True_Pos + False_Neg) -> average for each siren\n",
      "Rappel: [] True_pos/(True_Pos + False_Neg) -> average for each siren\n"
     ]
    }
   ],
   "source": [
    "#Precision : Somme :a dans A ((Somme de L(a) correcte) /( Somme de L(a)))/ |A|\n",
    "# True pos/(True Pos + False Pos) -> average for each siren\n",
    "precision_list = list()\n",
    "recall_list = list()\n",
    "for siren in siren_filtered:\n",
    "    true_pos = 0.0  # Siren is a label and is predicted\n",
    "    false_pos = 0.0 # Siren is NOT a label and is predicted (false prediction)\n",
    "    true_neg = 0.0  # Siren is NOT a label and is not predicted (don't care)\n",
    "    false_neg = 0.0 # Siren is a label and is NOT predicted\n",
    "    for i in range(len(article_eval)):\n",
    "        for j in range(len(article_labels[i])):\n",
    "            if siren==article_labels[i][j]:\n",
    "                if article_eval[i][j]==1:\n",
    "                    true_pos +=1\n",
    "                else:\n",
    "                    false_neg +=1\n",
    "    for i in range(len(pred_eval)):\n",
    "        for j in range(len(pred_labels[i])):\n",
    "            if siren==pred_labels[i][j]:\n",
    "                if pred_eval[i][j]==0:\n",
    "                    false_pos += 1\n",
    "\n",
    "    if siren in label_stats:\n",
    "        precision = true_pos/(true_pos+false_pos)\n",
    "        recall = true_pos/(true_pos+false_neg)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "print(\"Precision:\",precision_list,\"True_pos/(True_Pos + False_Neg) -> average for each siren\")\n",
    "\n",
    "#Rappel : pour chaque article (nombre de label correctes/Nombre de label vrai )\n",
    "# True pos/(True Pos + False Neg) -> average for each siren\n",
    "print(\"Rappel:\",recall_list,\"True_pos/(True_Pos + False_Neg) -> average for each siren\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0],\n",
       " [0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1], [1], [0], [0], [1], [0], [1], [1], [1, 1], [1]]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_eval[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
