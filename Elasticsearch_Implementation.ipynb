{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to test ElasticSearch on the corpus of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import requests, json\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Path to directory\n",
    "!pwd\n",
    "path = './data/extract_bulk.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Elasticsearch is ready for connection.\n",
      "[+] Elasticsearch successfully connected.\n"
     ]
    }
   ],
   "source": [
    "# Connect to ElasticSearch\n",
    "res = requests.get('http://localhost:9200')\n",
    "if res.content:\n",
    "    print('[+] Elasticsearch is ready for connection.')\n",
    "try:\n",
    "    es = Elasticsearch(hosts='http://localhost', PORT=9200, timeout=60, retry_on_timeout=True)\n",
    "    print('[+] Elasticsearch successfully connected.')\n",
    "except:\n",
    "    print('[-] An error occured during connection to Elasticsearch.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Read the json and load it into Elasticsearch\n",
    "i = 1\n",
    "with open(path) as f:\n",
    "    for line in tqdm(f):\n",
    "        # Send the data to es\n",
    "        es.index(index='index_articles', ignore=400, doc_type='articles', id=i, body=json.loads(line))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Index\n",
    "INDEX_NAME = 'articles_companies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match all the document\n",
    "query_all = {\n",
    "    'query' : {\n",
    "        'match_all' : { }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 324185 document in the index 'articles_companies'.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of documents in the index\n",
    "resp = es.count(index=INDEX_NAME, body=query_all)\n",
    "print(\"We have {} document in the index '{}'.\".format(resp['count'], INDEX_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return to query to get 10 most significant word reguarding a company name\n",
    "def query_significant_terms(company_name):\n",
    "    query = {\n",
    "        \"size\": 0, \n",
    "        \"query\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"query_string\": {\n",
    "                \"query\": company_name,\n",
    "                \"fields\": [\"title\", \"full-text\"]\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "      \"aggs\": {\n",
    "        \"sample\": {\n",
    "          \"sampler\": {\n",
    "            \"shard_size\": 150000\n",
    "          },\n",
    "          \"aggs\": {\n",
    "            \"keywords\": {\n",
    "              \"significant_text\": {\n",
    "                \"field\": \"full-text\",\n",
    "                \"include\": '.*' + company_name.lower() + '.*',\n",
    "                \"size\": 10\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      } \n",
    "    }\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 30731,\n",
       " 'timed_out': False,\n",
       " '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},\n",
       " 'hits': {'total': {'value': 7950, 'relation': 'eq'},\n",
       "  'max_score': None,\n",
       "  'hits': []},\n",
       " 'aggregations': {'sample': {'doc_count': 7950,\n",
       "   'keywords': {'doc_count': 7950,\n",
       "    'bg_count': 324185,\n",
       "    'buckets': [{'key': 'amzn.o',\n",
       "      'doc_count': 1817,\n",
       "      'score': 5.641238325053093,\n",
       "      'bg_count': 2885},\n",
       "     {'key': 'bezos',\n",
       "      'doc_count': 979,\n",
       "      'score': 3.970225654945654,\n",
       "      'bg_count': 1201},\n",
       "     {'key': 'rossignol',\n",
       "      'doc_count': 743,\n",
       "      'score': 3.1953080998381895,\n",
       "      'bg_count': 861},\n",
       "     {'key': 'lauwin',\n",
       "      'doc_count': 584,\n",
       "      'score': 2.9220559313318306,\n",
       "      'bg_count': 584},\n",
       "     {'key': 'planque',\n",
       "      'doc_count': 584,\n",
       "      'score': 2.9220559313318306,\n",
       "      'bg_count': 584},\n",
       "     {'key': 'warehouses',\n",
       "      'doc_count': 1021,\n",
       "      'score': 2.3961235522733912,\n",
       "      'bg_count': 2118},\n",
       "     {'key': 'microsoft',\n",
       "      'doc_count': 1241,\n",
       "      'score': 2.219703169089212,\n",
       "      'bg_count': 3325},\n",
       "     {'key': 'pascal',\n",
       "      'doc_count': 744,\n",
       "      'score': 2.0189566529352927,\n",
       "      'bg_count': 1344},\n",
       "     {'key': 'nasdaq:amzn',\n",
       "      'doc_count': 383,\n",
       "      'score': 1.7648669233478356,\n",
       "      'bg_count': 415},\n",
       "     {'key': 'rainforest',\n",
       "      'doc_count': 390,\n",
       "      'score': 1.5899508183479873,\n",
       "      'bg_count': 476}]}}}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with one company Apple\n",
    "my_company = 'Amazon'\n",
    "query = query_significant_terms(my_company)\n",
    "resp = es.search(index=INDEX_NAME, body=query)\n",
    "\n",
    "resp\n",
    "# Problem here, exlcude and inlcude seems to have revese operation : \n",
    "#   include works like an exlcude and exclude wroks like an include\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching companies names and related names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load companies lexical\n",
    "companies = pd.read_excel (r'./data/comapny_name-related_words.xlsx', header = None, names=['text'])\n",
    "# Lower\n",
    "#df['text'] = [str(row).lower() for index, row in df.iterrows()] \n",
    "companies[['companies', \"words\"]] = companies.text.str.split(\";\", expand=True)\n",
    "companies.drop(labels=['text'], axis = 1, inplace=True)\n",
    "companies = companies.groupby('companies')['words'].apply(list).reset_index(name='lexic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          21st Century Fox\n",
       "1       Activision Blizzard\n",
       "2                    Adobe \n",
       "3    Advanced Micro Devices\n",
       "4       Akamai Technologies\n",
       "Name: companies, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies.companies[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting significant words for a company\n",
    "def get_significant_terms(company):\n",
    "    significant_terms = []\n",
    "    query = query_significant_terms(company)\n",
    "    try:\n",
    "        response = es.search(index=INDEX_NAME, body=query)\n",
    "        bucket = response['aggregations']['sample']['keywords']['buckets']\n",
    "        #print('[+] Elasticsearch query successfully sent and received for {}.'.format(company))\n",
    "    except:\n",
    "        bucket = []\n",
    "        print('[-] An error occured during querying Elasticsearch for {}.'.format(company))\n",
    "    \n",
    "    if bucket:\n",
    "        for i in range(len(bucket)):\n",
    "            term = bucket[i]['key']\n",
    "            significant_terms.append(term)\n",
    "            \n",
    "    return significant_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Elasticsearch query successfully sent adn received for 21st Century Fox.\n",
      "[+] Elasticsearch query successfully sent adn received for Activision Blizzard.\n",
      "[+] Elasticsearch query successfully sent adn received for Adobe .\n",
      "[+] Elasticsearch query successfully sent adn received for Advanced Micro Devices.\n",
      "[+] Elasticsearch query successfully sent adn received for Akamai Technologies.\n",
      "[+] Elasticsearch query successfully sent adn received for Akamai Tecnologies.\n",
      "[+] Elasticsearch query successfully sent adn received for Alexion Pharmaceuticals.\n",
      "[+] Elasticsearch query successfully sent adn received for Alphabet.\n",
      "[+] Elasticsearch query successfully sent adn received for Amazon.\n",
      "[-] An error occured during querying Elasticsearch for American Airlines Group.\n",
      "[-] An error occured for American Airlines Group, the list of significant terms is empty.\n",
      "[+] Elasticsearch query successfully sent adn received for Amgen.\n",
      "[+] Elasticsearch query successfully sent adn received for Analog Devices.\n",
      "[+] Elasticsearch query successfully sent adn received for Apple.\n",
      "[+] Elasticsearch query successfully sent adn received for Autodesk.\n",
      "[-] An error occured during querying Elasticsearch for Automatic Data Processing.\n",
      "[-] An error occured for Automatic Data Processing, the list of significant terms is empty.\n",
      "[+] Elasticsearch query successfully sent adn received for Baidu.\n",
      "[+] Elasticsearch query successfully sent adn received for Bed Bath & Beyond.\n",
      "[+] Elasticsearch query successfully sent adn received for Biogen.\n",
      "[+] Elasticsearch query successfully sent adn received for CA Technologies.\n",
      "[+] Elasticsearch query successfully sent adn received for Celgene.\n",
      "[+] Elasticsearch query successfully sent adn received for Cerner.\n",
      "[+] Elasticsearch query successfully sent adn received for Cisco .\n",
      "[+] Elasticsearch query successfully sent adn received for Cognizant.\n",
      "[+] Elasticsearch query successfully sent adn received for Comcast.\n",
      "[+] Elasticsearch query successfully sent adn received for Discovery Communications.\n",
      "[+] Elasticsearch query successfully sent adn received for Dish Network.\n",
      "[+] Elasticsearch query successfully sent adn received for EBay.\n",
      "[+] Elasticsearch query successfully sent adn received for Electronic Arts.\n",
      "[+] Elasticsearch query successfully sent adn received for Equinix.\n",
      "[+] Elasticsearch query successfully sent adn received for Expeditors International.\n",
      "[+] Elasticsearch query successfully sent adn received for Facebook.\n",
      "[+] Elasticsearch query successfully sent adn received for Intel.\n",
      "[-] An error occured during querying Elasticsearch for Liberty Global.\n",
      "[-] An error occured for Liberty Global, the list of significant terms is empty.\n",
      "[+] Elasticsearch query successfully sent adn received for Liberty Interactive.\n",
      "[+] Elasticsearch query successfully sent adn received for Linear Technology.\n",
      "[+] Elasticsearch query successfully sent adn received for Marriott International.\n",
      "[+] Elasticsearch query successfully sent adn received for Mattel.\n",
      "[+] Elasticsearch query successfully sent adn received for Mattle.\n",
      "[-] An error occured for Mattle, the list of significant terms is empty.\n",
      "[+] Elasticsearch query successfully sent adn received for McKesson.\n",
      "[+] Elasticsearch query successfully sent adn received for McKesson .\n",
      "[+] Elasticsearch query successfully sent adn received for Microsoft.\n",
      "[+] Elasticsearch query successfully sent adn received for NVIDIA.\n",
      "[+] Elasticsearch query successfully sent adn received for Netflix.\n",
      "[+] Elasticsearch query successfully sent adn received for Paypal.\n",
      "[+] Elasticsearch query successfully sent adn received for Qualcomm.\n",
      "[+] Elasticsearch query successfully sent adn received for Starbucks.\n",
      "[+] Elasticsearch query successfully sent adn received for Stericycle.\n",
      "[-] An error occured for Stericycle, the list of significant terms is empty.\n",
      "[+] Elasticsearch query successfully sent adn received for Tesla Motors.\n",
      "[+] Elasticsearch query successfully sent adn received for Texas Instruments.\n",
      "[-] An error occured during querying Elasticsearch for The Priceline Group.\n",
      "[-] An error occured for The Priceline Group, the list of significant terms is empty.\n",
      "[+] Elasticsearch query successfully sent adn received for Universal Display.\n",
      "[+] Elasticsearch query successfully sent adn received for Universal Display .\n"
     ]
    }
   ],
   "source": [
    "# Create the dictionnary with all the significatn terms\n",
    "relevant_words = {company:get_significant_terms(company) for company in companies.companies}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'21st Century Fox': ['fox',\n",
       "  'century',\n",
       "  '21st',\n",
       "  '19th',\n",
       "  'white',\n",
       "  'news',\n",
       "  'trump',\n",
       "  'house',\n",
       "  'donald',\n",
       "  'interview'],\n",
       " 'Activision Blizzard': ['blizzard',\n",
       "  'activision',\n",
       "  'nasdaq:atvi',\n",
       "  'atvi.o',\n",
       "  'warcraft',\n",
       "  'videogame',\n",
       "  'warzone',\n",
       "  \"blizzard's\",\n",
       "  '4185',\n",
       "  'kotick'],\n",
       " 'Adobe ': ['adobe',\n",
       "  'photoshop',\n",
       "  'adbe.o',\n",
       "  'athletica',\n",
       "  'valueacts',\n",
       "  '96.08',\n",
       "  \"adobe's\",\n",
       "  '9,588.81',\n",
       "  '4185',\n",
       "  '3,041.31'],\n",
       " 'Advanced Micro Devices': ['advanced',\n",
       "  'devices',\n",
       "  'micro',\n",
       "  'barda',\n",
       "  'biomedical',\n",
       "  'technology',\n",
       "  'device',\n",
       "  'apple',\n",
       "  'research',\n",
       "  'bluetooth'],\n",
       " 'Akamai Technologies': ['technologies',\n",
       "  'huawei',\n",
       "  'uber',\n",
       "  'huaweis',\n",
       "  'uber.n',\n",
       "  'hailing',\n",
       "  'hwt.ul',\n",
       "  'technology',\n",
       "  'entity',\n",
       "  'wanzhou'],\n",
       " 'Akamai Tecnologies': ['akamai',\n",
       "  'akam',\n",
       "  'llnw',\n",
       "  'nasdaq:akam',\n",
       "  'cdn',\n",
       "  \"fastly's\",\n",
       "  'fastly',\n",
       "  'nyse:fsly'],\n",
       " 'Alexion Pharmaceuticals': ['pharmaceuticals',\n",
       "  'regeneron',\n",
       "  'inovio',\n",
       "  'sanofi',\n",
       "  'alexion',\n",
       "  'kevzara',\n",
       "  'regn.o',\n",
       "  'drugmakers',\n",
       "  'ino.o',\n",
       "  'yancopoulos'],\n",
       " 'Alphabet': ['google',\n",
       "  'googl.o',\n",
       "  'incs',\n",
       "  'googles',\n",
       "  'fb.o',\n",
       "  'januar',\n",
       "  'tech',\n",
       "  'twitters',\n",
       "  'cajole',\n",
       "  'balkin'],\n",
       " 'Amazon': ['amazons',\n",
       "  'amazon.com',\n",
       "  'amzn.o',\n",
       "  'bezos',\n",
       "  'rossignol',\n",
       "  'planque',\n",
       "  'lauwin',\n",
       "  \"amazon's\",\n",
       "  'warehouses',\n",
       "  'microsoft'],\n",
       " 'American Airlines Group': [],\n",
       " 'Amgen': ['otezla',\n",
       "  'repatha',\n",
       "  'amgn.o',\n",
       "  'adpt.o',\n",
       "  'celgenes',\n",
       "  'biotechnologies',\n",
       "  '14.85',\n",
       "  'nasdaq:amgn',\n",
       "  'amgens',\n",
       "  '15.60'],\n",
       " 'Analog Devices': ['devices',\n",
       "  'device',\n",
       "  'bluetooth',\n",
       "  'iphones',\n",
       "  'apple',\n",
       "  'iphone',\n",
       "  'technology',\n",
       "  'android',\n",
       "  'smartphone',\n",
       "  'apps'],\n",
       " 'Apple': ['aapl.o',\n",
       "  'iphone',\n",
       "  'iphones',\n",
       "  'google',\n",
       "  'bluetooth',\n",
       "  'smartphone',\n",
       "  'apples',\n",
       "  'tech',\n",
       "  'apps',\n",
       "  'app'],\n",
       " 'Autodesk': ['selldown',\n",
       "  'datacenter',\n",
       "  'skyy.o',\n",
       "  'hsnp',\n",
       "  'adsk.o',\n",
       "  'nasdaq:adsk',\n",
       "  'vmw.n',\n",
       "  'twitchy',\n",
       "  \"autodesk's\",\n",
       "  'funnelled'],\n",
       " 'Automatic Data Processing': [],\n",
       " 'Baidu': ['bidu.o',\n",
       "  'nasdaq:bidu',\n",
       "  \"baidu's\",\n",
       "  '67.13',\n",
       "  '961,729',\n",
       "  '109,664',\n",
       "  '183,700',\n",
       "  'hxc',\n",
       "  'tcom.o',\n",
       "  'iqiyi'],\n",
       " 'Bed Bath & Beyond': ['beyond',\n",
       "  'bed',\n",
       "  'bath',\n",
       "  'transcripts',\n",
       "  'contents',\n",
       "  \"i'll\",\n",
       "  \"i'd\",\n",
       "  'hi',\n",
       "  \"we'll\",\n",
       "  'differ'],\n",
       " 'Biogen': ['tecfidera',\n",
       "  'biib.o',\n",
       "  'aducanumab',\n",
       "  \"biogen's\",\n",
       "  'nasdaq:biib',\n",
       "  'spinraza',\n",
       "  'biib',\n",
       "  'biogens',\n",
       "  'sclerosis',\n",
       "  'eisai'],\n",
       " 'CA Technologies': ['technologies',\n",
       "  'ca',\n",
       "  'huawei',\n",
       "  'uber',\n",
       "  'huaweis',\n",
       "  'uber.n',\n",
       "  'hailing',\n",
       "  'hwt.ul',\n",
       "  'technology',\n",
       "  'entity'],\n",
       " 'Celgene': ['squibb',\n",
       "  'revlimid',\n",
       "  'otezla',\n",
       "  'bmy.n',\n",
       "  'celgenes',\n",
       "  'adpt.o',\n",
       "  'repatha',\n",
       "  'amgn.o',\n",
       "  'zeposia',\n",
       "  'liso'],\n",
       " 'Cerner': ['nasdaq:cern'],\n",
       " 'Cisco ': ['cisco',\n",
       "  'csco.o',\n",
       "  'toolchain',\n",
       "  'reprieves',\n",
       "  '6501',\n",
       "  'asml.as',\n",
       "  'chipmaking',\n",
       "  'asml',\n",
       "  'qualcomm',\n",
       "  'unreliable'],\n",
       " 'Cognizant': ['hadjikyriacos',\n",
       "  'malosh',\n",
       "  'haircutter',\n",
       "  'marios',\n",
       "  'mathema',\n",
       "  'barun',\n",
       "  'playdate',\n",
       "  'marybeth',\n",
       "  'cohorting',\n",
       "  'janavss'],\n",
       " 'Comcast': ['cmcsa.o',\n",
       "  'nbcuniversal',\n",
       "  'ccz.n',\n",
       "  'nasdaq:cmcsa',\n",
       "  'hettema',\n",
       "  'profun',\n",
       "  'damaro',\n",
       "  'rodstrom',\n",
       "  'arjuna',\n",
       "  'reimagines'],\n",
       " 'Discovery Communications': ['communications',\n",
       "  'discovery',\n",
       "  'fcc',\n",
       "  'decency',\n",
       "  'zm.o',\n",
       "  '230',\n",
       "  'censorship',\n",
       "  'pai',\n",
       "  'encrypted',\n",
       "  'unsubstantiated'],\n",
       " 'Dish Network': ['network',\n",
       "  'dish',\n",
       "  'networks',\n",
       "  '5g',\n",
       "  'today',\n",
       "  'usa',\n",
       "  'fox',\n",
       "  \"it's\",\n",
       "  'huawei',\n",
       "  'these'],\n",
       " 'EBay': ['ebay.o',\n",
       "  'ebays',\n",
       "  '3fculg5',\n",
       "  'harville',\n",
       "  'wenig',\n",
       "  'natick',\n",
       "  'cyberstalking',\n",
       "  'smucker',\n",
       "  'j.m',\n",
       "  'lanyards'],\n",
       " 'Electronic Arts': ['electronic',\n",
       "  'arts',\n",
       "  'lujiazui',\n",
       "  'overpass',\n",
       "  'martial',\n",
       "  'nikkei',\n",
       "  'mercantile',\n",
       "  'pedestrian',\n",
       "  'graphs',\n",
       "  'aly'],\n",
       " 'Equinix': ['nasdaq:eqix', 'eqix', \"equinix's\"],\n",
       " 'Expeditors International': ['international',\n",
       "  'airport',\n",
       "  'flights',\n",
       "  'travel',\n",
       "  'airlines',\n",
       "  'foreign',\n",
       "  'countries',\n",
       "  'flight',\n",
       "  'global',\n",
       "  'united'],\n",
       " 'Facebook': ['fb.o',\n",
       "  'google',\n",
       "  'zuckerberg',\n",
       "  'facebooks',\n",
       "  'posts',\n",
       "  'platforms',\n",
       "  'users',\n",
       "  'twitter',\n",
       "  'content',\n",
       "  'tech'],\n",
       " 'Intel': ['intc.o',\n",
       "  'chipmaker',\n",
       "  'tikva',\n",
       "  'petah',\n",
       "  'intels',\n",
       "  \"corp's\",\n",
       "  'microelectronics',\n",
       "  'qualcomm',\n",
       "  'semiconductor',\n",
       "  'chips'],\n",
       " 'Liberty Global': [],\n",
       " 'Liberty Interactive': ['interactive',\n",
       "  'liberty',\n",
       "  'browser',\n",
       "  '3airuz7',\n",
       "  'tmsnrt.rs',\n",
       "  'external',\n",
       "  'graphic',\n",
       "  'tracking',\n",
       "  'tracker',\n",
       "  '2w7hx9t'],\n",
       " 'Linear Technology': ['technology',\n",
       "  'tech',\n",
       "  'companies',\n",
       "  'software',\n",
       "  'technologies',\n",
       "  'company',\n",
       "  'apple',\n",
       "  'bluetooth',\n",
       "  'smartphone',\n",
       "  'inc'],\n",
       " 'Marriott International': ['international',\n",
       "  'airport',\n",
       "  'flights',\n",
       "  'travel',\n",
       "  'airlines',\n",
       "  'countries',\n",
       "  'foreign',\n",
       "  'flight',\n",
       "  'global',\n",
       "  'united'],\n",
       " 'Mattel': ['mat.o',\n",
       "  'has.o',\n",
       "  'toymaker',\n",
       "  'hasbro',\n",
       "  'kreiz',\n",
       "  'ynon',\n",
       "  'hasbros',\n",
       "  '594.1',\n",
       "  'mattels',\n",
       "  \"mattel's\"],\n",
       " 'Mattle': [],\n",
       " 'McKesson': ['mck',\n",
       "  '79.94',\n",
       "  'gorsky',\n",
       "  'nyse:mck',\n",
       "  '7.68',\n",
       "  'cah',\n",
       "  'deray',\n",
       "  '157.10',\n",
       "  'mohnot',\n",
       "  'andreessens'],\n",
       " 'McKesson ': ['mckesson',\n",
       "  'mck',\n",
       "  '79.94',\n",
       "  'gorsky',\n",
       "  'nyse:mck',\n",
       "  '7.68',\n",
       "  'cah',\n",
       "  'deray',\n",
       "  '157.10',\n",
       "  'andreessens'],\n",
       " 'Microsoft': ['msft.o',\n",
       "  'amzn.o',\n",
       "  'amazon',\n",
       "  'azure',\n",
       "  'cloud',\n",
       "  'microsofts',\n",
       "  'nasdaq:msft',\n",
       "  'amazon.com',\n",
       "  'apple',\n",
       "  'xbox'],\n",
       " 'NVIDIA': ['nvda.o',\n",
       "  'amd.o',\n",
       "  'nvidias',\n",
       "  'nasdaq:nvda',\n",
       "  'mellanox',\n",
       "  \"nvidia's\",\n",
       "  'neuffer',\n",
       "  '9.34',\n",
       "  '6.32',\n",
       "  'nanonmeter'],\n",
       " 'Netflix': ['nflx.o',\n",
       "  'streaming',\n",
       "  \"netflix's\",\n",
       "  'hulu',\n",
       "  'hbo',\n",
       "  'nasdaq:nflx',\n",
       "  'documentary',\n",
       "  'movies',\n",
       "  'film',\n",
       "  'disney'],\n",
       " 'Paypal': ['pypl.o',\n",
       "  'paypals',\n",
       "  'nasdaq:pypl',\n",
       "  'schulman',\n",
       "  '194.23',\n",
       "  '2shfpy2',\n",
       "  'dreilinden',\n",
       "  'europarc',\n",
       "  '87.4',\n",
       "  'kleinmachnow'],\n",
       " 'Qualcomm': ['qcom.o',\n",
       "  'klac.o',\n",
       "  'tsmc',\n",
       "  'lrcx.o',\n",
       "  'kla',\n",
       "  'foundries',\n",
       "  '2330',\n",
       "  'tw',\n",
       "  '8035',\n",
       "  'amat.o'],\n",
       " 'Starbucks': ['sbux.o',\n",
       "  'coffee',\n",
       "  'nasdaq:sbux',\n",
       "  'dunkin',\n",
       "  '310.77',\n",
       "  'jdep.as',\n",
       "  'khiem',\n",
       "  'iposcoop',\n",
       "  'arteriosclerotic',\n",
       "  'procters'],\n",
       " 'Stericycle': [],\n",
       " 'Tesla Motors': ['motors',\n",
       "  'tesla',\n",
       "  'musk',\n",
       "  'elon',\n",
       "  'tsla.o',\n",
       "  'gm.n',\n",
       "  'electric',\n",
       "  'teslas',\n",
       "  'automakers',\n",
       "  'vehicle'],\n",
       " 'Texas Instruments': ['texas',\n",
       "  'intermediate',\n",
       "  'wti',\n",
       "  'houston',\n",
       "  'oklahoma',\n",
       "  'florida',\n",
       "  'abbott',\n",
       "  'austin',\n",
       "  'barrel',\n",
       "  'barrels'],\n",
       " 'The Priceline Group': [],\n",
       " 'Universal Display': ['display',\n",
       "  'universal',\n",
       "  'suffrage',\n",
       "  'studios',\n",
       "  'theme',\n",
       "  'seaworld',\n",
       "  'rationales',\n",
       "  'symbolized',\n",
       "  'mensah',\n",
       "  'disney'],\n",
       " 'Universal Display ': ['display',\n",
       "  'universal',\n",
       "  'suffrage',\n",
       "  'studios',\n",
       "  'theme',\n",
       "  'seaworld',\n",
       "  'rationales',\n",
       "  'symbolized',\n",
       "  'mensah',\n",
       "  'disney']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output json file \n",
    "output_path = './data/relevant_words.json'\n",
    "with open(output_path, \"w\") as outfile:  \n",
    "    json.dump(relevant_words, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching data\n",
    "PATH = \"./data/ArticleCompany_2020-11-17/\"\n",
    "corpus = \"corpus_check_long_SIREN_UPDATED2\"\n",
    "names = \"siren_name_map_clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_name = pd.read_json(PATH + names +\".json\")\n",
    "with open(PATH + names +\".json\") as json_file: \n",
    "    dict_names = json.load(json_file) \n",
    "\n",
    "with open(PATH + corpus +\".json\") as json_file: \n",
    "    dict_corpus = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57540"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '8',\n",
       " 'siren': '[419838529, 813883964]',\n",
       " 'corpus': 'Ipsen lorgne les peptides de PeptiMimesis ',\n",
       " 'url_article': 'http://www.boursier.com/actions/actualites/news/ipsen-lorgne-les-peptides-de-peptimimesis-677907.html'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_name(siren):\n",
    "    try:\n",
    "        name = dict_names[siren]\n",
    "    except:\n",
    "        name = ''\n",
    "    return name\n",
    "    \n",
    "# Append Compay name to di\n",
    "for corpus in dict_corpus:\n",
    "    sirens = corpus['siren'][1:-1].split(',')\n",
    "    sirens = [element.strip() for element in sirens]\n",
    "    names = [get_company_name(element) for element in sirens] \n",
    "    corpus['companies'] = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '8',\n",
       " 'siren': '[419838529, 813883964]',\n",
       " 'corpus': 'Ipsen lorgne les peptides de PeptiMimesis ',\n",
       " 'url_article': 'http://www.boursier.com/actions/actualites/news/ipsen-lorgne-les-peptides-de-peptimimesis-677907.html',\n",
       " 'companies': ['IPSEN', 'PEPTIMIMESIS']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_corpus[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Output json corpus file \n",
    "output_path = './data/ArticleCompany_2020-11-17/french_articles.json'\n",
    "with open(output_path, \"w\", newline='\\r\\n') as outfile:  \n",
    "    for line in dict_corpus:\n",
    "        outfile.write(json.dumps(line))\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40278\n"
     ]
    }
   ],
   "source": [
    "size = len(dict_corpus)\n",
    "train_size = (int) (0.7 * size)\n",
    "print(train_size)\n",
    "\n",
    "train_data = dict_corpus[:train_size]\n",
    "test_data = dict_corpus[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output json corpus file \n",
    "output_train = './data/french_articles/train_fr_articles.json'\n",
    "output_test = './data/french_articles/test_fr_articles.json'\n",
    "\n",
    "with open(output_train, \"w\") as outfile:  \n",
    "    for line in train_data:\n",
    "        outfile.write(json.dumps(line))\n",
    "        outfile.write('\\n')\n",
    "\n",
    "with open(output_test, \"w\") as outfile:  \n",
    "    for line in test_data:\n",
    "        outfile.write(json.dumps(line))\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionnary for french companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My index\n",
    "INDEX_NAME_FR = 'french_articles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 30178 french companies.\n"
     ]
    }
   ],
   "source": [
    "# Listing all the companies\n",
    "french_companies = list(dict_names.values())\n",
    "print(\"We have {} french companies.\".format(len(french_companies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 57540 document in the index 'french_articles'.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of documents in the index\n",
    "resp = es.count(index=INDEX_NAME_FR, body=query_all)\n",
    "print(\"We have {} document in the index '{}'.\".format(resp['count'], INDEX_NAME_FR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return to query to get 10 most significant word reguarding a company name\n",
    "def query_significant_terms_fr(company_name):\n",
    "    query = {\n",
    "        \"size\": 0, \n",
    "        \"query\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"query_string\": {\n",
    "                \"query\": company_name,\n",
    "                \"fields\": [\"companies\"]\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "      \"aggs\": {\n",
    "        \"sample\": {\n",
    "          \"sampler\": {\n",
    "            \"shard_size\": 150000\n",
    "          },\n",
    "          \"aggs\": {\n",
    "            \"keywords\": {\n",
    "              \"significant_text\": {\n",
    "                \"field\": \"corpus\",\n",
    "                \"include\": '.*' + company_name.lower() + '.*',\n",
    "                \"size\": 20\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      } \n",
    "    }\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting significant words for a company\n",
    "def get_significant_terms_fr(company, index):\n",
    "    significant_terms = list()\n",
    "    query = query_significant_terms_fr(company)\n",
    "    try:\n",
    "        response = es.search(index=index, body=query)\n",
    "        bucket = response['aggregations']['sample']['keywords']['buckets']\n",
    "        #print('[+] Elasticsearch query successfully sent and received for {}.'.format(company))\n",
    "    except:\n",
    "        bucket = []\n",
    "        #print('[-] An error occured during querying Elasticsearch for {}.'.format(company))\n",
    "    \n",
    "    if bucket:\n",
    "        for i in range(len(bucket)):\n",
    "            term = list()\n",
    "            term.append(bucket[i]['key'])\n",
    "            term.append(bucket[i]['score'])\n",
    "            significant_terms.append(term)\n",
    "            \n",
    "    return significant_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionnary with all the significatn terms\n",
    "relevant_words_fr = {company:get_significant_terms_fr(company, INDEX_NAME_FR) for company in tqdm(french_companies)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionnary from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FR = 'train_fr_articles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.7 s, sys: 5.29 s, total: 32 s\n",
      "Wall time: 10h 20min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "relevant_words_train = {siren:get_significant_terms_fr(dict_names[siren], INDEX_FR) for siren in dict_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relevant_words_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8468c302286a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrelevant_words_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'relevant_words_train' is not defined"
     ]
    }
   ],
   "source": [
    "relevant_words_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output json file \n",
    "output_path = './relevant_words/francais/relevant_words_train.json'\n",
    "with open(output_path, \"w\") as outfile:  \n",
    "    json.dump(relevant_words_train, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we label the test set with those labels and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(text, relevant_words):\n",
    "    labels = []\n",
    "    for siren in relevant_words:\n",
    "        if dict_names[siren].lower() in text.lower():\n",
    "            labels.append(siren)\n",
    "        else:\n",
    "            for related_name in relevant_words[siren]:\n",
    "                if related_name.lower() in text.lower() and siren not in labels:\n",
    "                    labels.append(siren)\n",
    "                    break\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "output_test = './data/french_articles/test_fr_articles.json'\n",
    "y_test = list()\n",
    "y_corpuses = list()\n",
    "with open(output_test, \"r\") as outfile:  \n",
    "    for line in test_data:\n",
    "        tmp = line['siren'][1:-1].split(',')\n",
    "        y_test.append([element.strip() for element in tmp])\n",
    "        y_corpuses.append(line['corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-f0530cd5b19f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_words_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_corpuses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-136-f0530cd5b19f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_words_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_corpuses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-134-0b089e2ca6a4>\u001b[0m in \u001b[0;36mlabeling\u001b[0;34m(text, relevant_words)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrelated_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelevant_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msiren\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mrelated_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msiren\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = [labeling(text, relevant_words_train) for text in y_corpuses]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the relevant terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i]:\n",
    "            for label in predicted[i]:\n",
    "                if label in actual[i]:\n",
    "                    correct += 1\n",
    "                    break\n",
    "    return correct / float(len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The accuracy of the relevant terms list is : ')\n",
    "print(accuracy_metric(y_test, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
